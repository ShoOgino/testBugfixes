{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","commits":[{"id":"a8ad56fe9e068c425e8ac38f2263714e167490c2","date":1462704115,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc1 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(new Random(seed), dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc2 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(new Random(seed), dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a6d0e4c7ab9c0e1fc073fddd21f4784555be9cd","date":1463081111,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc1 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(new Random(seed), dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    IndexWriterConfig iwc2 = newIndexWriterConfig(new MockAnalyzer(random()));\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(new Random(seed), dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":0,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":0,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":0,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"/dev/null","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e065cebbdee472a02bde38052717a8cd7ee8ab3b","date":1479244971,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"sorted_set\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"sorted_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n    \n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n    \n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"acd9883560fd89e6448b2b447302fe543040cd4f","date":1488478696,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      String value = IntStream.range(0, id).mapToObj(k -> Integer.toString(id)).collect(Collectors.joining(\" \"));\n      TextField norms = new TextField(\"norms\", value, Store.NO);\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      TextField norms = new TextField(\"norms\", Integer.toString(id), Store.NO);\n      norms.setBoost(Float.intBitsToFloat(id));\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      String value = IntStream.range(0, id).mapToObj(k -> Integer.toString(id)).collect(Collectors.joining(\" \"));\n      TextField norms = new TextField(\"norms\", value, Store.NO);\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getMetaData().getSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      String value = IntStream.range(0, id).mapToObj(k -> Integer.toString(id)).collect(Collectors.joining(\" \"));\n      TextField norms = new TextField(\"norms\", value, Store.NO);\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexSorting#testRandom2().mjava","sourceNew":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      String value = IntStream.range(0, id).mapToObj(k -> Integer.toString(id)).collect(Collectors.joining(\" \"));\n      TextField norms = new TextField(\"norms\", value, Store.NO);\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getMetaData().getSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","sourceOld":"  public void testRandom2() throws Exception {\n    int numDocs = atLeast(100);\n\n    FieldType POSITIONS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    POSITIONS_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    POSITIONS_TYPE.freeze();\n\n    FieldType TERM_VECTORS_TYPE = new FieldType(TextField.TYPE_NOT_STORED);\n    TERM_VECTORS_TYPE.setStoreTermVectors(true);\n    TERM_VECTORS_TYPE.freeze();\n\n    Analyzer a = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName) {\n        Tokenizer tokenizer = new MockTokenizer();\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      }\n    };\n\n    List<Document> docs = new ArrayList<>();\n    for (int i=0;i<numDocs;i++) {\n      int id = i * 10;\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", Integer.toString(id), Store.YES));\n      doc.add(new StringField(\"docs\", \"#all#\", Store.NO));\n      PositionsTokenStream positions = new PositionsTokenStream();\n      positions.setId(id);\n      doc.add(new Field(\"positions\", positions, POSITIONS_TYPE));\n      doc.add(new NumericDocValuesField(\"numeric\", id));\n      String value = IntStream.range(0, id).mapToObj(k -> Integer.toString(id)).collect(Collectors.joining(\" \"));\n      TextField norms = new TextField(\"norms\", value, Store.NO);\n      doc.add(norms);\n      doc.add(new BinaryDocValuesField(\"binary\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedDocValuesField(\"sorted\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id))));\n      doc.add(new SortedSetDocValuesField(\"multi_valued_string\", new BytesRef(Integer.toString(id + 1))));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id));\n      doc.add(new SortedNumericDocValuesField(\"multi_valued_numeric\", id + 1));\n      doc.add(new Field(\"term_vectors\", Integer.toString(id), TERM_VECTORS_TYPE));\n      byte[] bytes = new byte[4];\n      NumericUtils.intToSortableBytes(id, bytes, 0);\n      doc.add(new BinaryPoint(\"points\", bytes));\n      docs.add(doc);\n    }\n\n    // Must use the same seed for both RandomIndexWriters so they behave identically\n    long seed = random().nextLong();\n\n    // We add document alread in ID order for the first writer:\n    Directory dir1 = newFSDirectory(createTempDir());\n\n    Random random1 = new Random(seed);\n    IndexWriterConfig iwc1 = newIndexWriterConfig(random1, a);\n    iwc1.setSimilarity(new NormsSimilarity(iwc1.getSimilarity())); // for testing norms field\n    // preserve docIDs\n    iwc1.setMergePolicy(newLogMergePolicy());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index pre-sorted\");\n    }\n    RandomIndexWriter w1 = new RandomIndexWriter(random1, dir1, iwc1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      w1.addDocument(doc);\n    }\n\n    // We shuffle documents, but set index sort, for the second writer:\n    Directory dir2 = newFSDirectory(createTempDir());\n\n    Random random2 = new Random(seed);\n    IndexWriterConfig iwc2 = newIndexWriterConfig(random2, a);\n    iwc2.setSimilarity(new NormsSimilarity(iwc2.getSimilarity())); // for testing norms field\n\n    Sort sort = new Sort(new SortField(\"numeric\", SortField.Type.INT));\n    iwc2.setIndexSort(sort);\n\n    Collections.shuffle(docs, random());\n    if (VERBOSE) {\n      System.out.println(\"TEST: now index with index-time sorting\");\n    }\n    RandomIndexWriter w2 = new RandomIndexWriter(random2, dir2, iwc2);\n    int count = 0;\n    int commitAtCount = TestUtil.nextInt(random(), 1, numDocs-1);\n    for(Document doc : docs) {\n      ((PositionsTokenStream) ((Field) doc.getField(\"positions\")).tokenStreamValue()).setId(Integer.parseInt(doc.get(\"id\")));\n      if (count++ == commitAtCount) {\n        // Ensure forceMerge really does merge\n        w2.commit();\n      }\n      w2.addDocument(doc);\n    }\n    if (VERBOSE) {\n      System.out.println(\"TEST: now force merge\");\n    }\n    w2.forceMerge(1);\n\n    DirectoryReader r1 = w1.getReader();\n    DirectoryReader r2 = w2.getReader();\n    if (VERBOSE) {\n      System.out.println(\"TEST: now compare r1=\" + r1 + \" r2=\" + r2);\n    }\n    assertEquals(sort, getOnlyLeafReader(r2).getIndexSort());\n    assertReaderEquals(\"left: sorted by hand; right: sorted by Lucene\", r1, r2);\n    IOUtils.close(w1, w2, r1, r2, dir1, dir2);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0a6d0e4c7ab9c0e1fc073fddd21f4784555be9cd":["a8ad56fe9e068c425e8ac38f2263714e167490c2"],"e065cebbdee472a02bde38052717a8cd7ee8ab3b":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"0ad30c6a479e764150a3316e57263319775f1df2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3d33e731a93d4b57e662ff094f64f94a745422d4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d470c8182e92b264680e34081b75e70a9f2b3c89"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["acd9883560fd89e6448b2b447302fe543040cd4f"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0ad30c6a479e764150a3316e57263319775f1df2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e065cebbdee472a02bde38052717a8cd7ee8ab3b"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["acd9883560fd89e6448b2b447302fe543040cd4f"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0a6d0e4c7ab9c0e1fc073fddd21f4784555be9cd"],"acd9883560fd89e6448b2b447302fe543040cd4f":["e065cebbdee472a02bde38052717a8cd7ee8ab3b"],"a8ad56fe9e068c425e8ac38f2263714e167490c2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["31741cf1390044e38a2ec3127cf302ba841bfd75"]},"commit2Childs":{"0a6d0e4c7ab9c0e1fc073fddd21f4784555be9cd":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"e065cebbdee472a02bde38052717a8cd7ee8ab3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","acd9883560fd89e6448b2b447302fe543040cd4f"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["e065cebbdee472a02bde38052717a8cd7ee8ab3b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ad30c6a479e764150a3316e57263319775f1df2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","d470c8182e92b264680e34081b75e70a9f2b3c89","3d33e731a93d4b57e662ff094f64f94a745422d4","a8ad56fe9e068c425e8ac38f2263714e167490c2"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"a8ad56fe9e068c425e8ac38f2263714e167490c2":["0a6d0e4c7ab9c0e1fc073fddd21f4784555be9cd"],"acd9883560fd89e6448b2b447302fe543040cd4f":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","92212fd254551a0b1156aafc3a1a6ed1a43932ad","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}