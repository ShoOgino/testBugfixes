{"path":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","commits":[{"id":"0977217be8980b17c612feb7ea3d1556ccf4aeed","date":1415742153,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d17901785875f5d523c1f729cd7898d2ab907c69","date":1419959847,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f582f18c13d4852b01d4fe0a0196432c5c6f2b7f","date":1421314520,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"de3f4a236750bdfe06353c2b1bfe6f5e97de3caa","date":1430124502,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b28ce1e2b0fd85f7ca7f72b9ab2ae1f5350c9005","date":1431617381,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n\n            // TODO: if we had a BinaryTermField we could fix\n            // this \"must be valid ut8f\" limitation:\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30","date":1431701935,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","pathOld":"lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester#lookup(CharSequence,Map[BytesRef,BooleanClause.Occur],int,boolean,boolean).mjava","sourceNew":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n      return lookup(key, toQuery(contextInfo), num, allTermsRequired, doHighlight);\n  }\n\n","sourceOld":"  /** Retrieve suggestions, specifying whether all terms\n   *  must match ({@code allTermsRequired}) and whether the hits\n   *  should be highlighted ({@code doHighlight}). */\n  public List<LookupResult> lookup(CharSequence key, Map<BytesRef, BooleanClause.Occur> contextInfo, int num, boolean allTermsRequired, boolean doHighlight) throws IOException {\n\n    if (searcherMgr == null) {\n      throw new IllegalStateException(\"suggester was not built\");\n    }\n\n    final BooleanClause.Occur occur;\n    if (allTermsRequired) {\n      occur = BooleanClause.Occur.MUST;\n    } else {\n      occur = BooleanClause.Occur.SHOULD;\n    }\n\n    BooleanQuery query;\n    Set<String> matchedTokens = new HashSet<>();\n    String prefixToken = null;\n\n    try (TokenStream ts = queryAnalyzer.tokenStream(\"\", new StringReader(key.toString()))) {\n      //long t0 = System.currentTimeMillis();\n      ts.reset();\n      final CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class);\n      String lastToken = null;\n      query = new BooleanQuery();\n      int maxEndOffset = -1;\n      matchedTokens = new HashSet<>();\n      while (ts.incrementToken()) {\n        if (lastToken != null) {  \n          matchedTokens.add(lastToken);\n          query.add(new TermQuery(new Term(TEXT_FIELD_NAME, lastToken)), occur);\n        }\n        lastToken = termAtt.toString();\n        if (lastToken != null) {\n          maxEndOffset = Math.max(maxEndOffset, offsetAtt.endOffset());\n        }\n      }\n      ts.end();\n\n      if (lastToken != null) {\n        Query lastQuery;\n        if (maxEndOffset == offsetAtt.endOffset()) {\n          // Use PrefixQuery (or the ngram equivalent) when\n          // there was no trailing discarded chars in the\n          // string (e.g. whitespace), so that if query does\n          // not end with a space we show prefix matches for\n          // that token:\n          lastQuery = getLastTokenQuery(lastToken);\n          prefixToken = lastToken;\n        } else {\n          // Use TermQuery for an exact match if there were\n          // trailing discarded chars (e.g. whitespace), so\n          // that if query ends with a space we only show\n          // exact matches for that term:\n          matchedTokens.add(lastToken);\n          lastQuery = new TermQuery(new Term(TEXT_FIELD_NAME, lastToken));\n        }\n        if (lastQuery != null) {\n          query.add(lastQuery, occur);\n        }\n      }\n\n      if (contextInfo != null) {\n        \n        boolean allMustNot = true;\n        for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n          if (entry.getValue() != BooleanClause.Occur.MUST_NOT) {\n            allMustNot = false;\n            break;\n          }\n        }\n\n        // do not make a subquery if all context booleans are must not\n        if (allMustNot == true) {\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            query.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey().utf8ToString())), BooleanClause.Occur.MUST_NOT);\n          }\n\n        } else {\n          BooleanQuery sub = new BooleanQuery();\n          query.add(sub, BooleanClause.Occur.MUST);\n\n          for (Map.Entry<BytesRef, BooleanClause.Occur> entry : contextInfo.entrySet()) {\n            // NOTE: we \"should\" wrap this in\n            // ConstantScoreQuery, or maybe send this as a\n            // Filter instead to search.\n            sub.add(new TermQuery(new Term(CONTEXTS_FIELD_NAME, entry.getKey())), entry.getValue());\n          }\n        }\n      }\n    }\n\n    // TODO: we could allow blended sort here, combining\n    // weight w/ score.  Now we ignore score and sort only\n    // by weight:\n\n    Query finalQuery = finishQuery(query, allTermsRequired);\n\n    //System.out.println(\"finalQuery=\" + query);\n\n    // Sort by weight, descending:\n    TopFieldCollector c = TopFieldCollector.create(SORT, num, true, false, false);\n\n    // We sorted postings by weight during indexing, so we\n    // only retrieve the first num hits now:\n    final MergePolicy mergePolicy = writer.getConfig().getMergePolicy();\n    Collector c2 = new EarlyTerminatingSortingCollector(c, SORT, num, (SortingMergePolicy) mergePolicy);\n    IndexSearcher searcher = searcherMgr.acquire();\n    List<LookupResult> results = null;\n    try {\n      //System.out.println(\"got searcher=\" + searcher);\n      searcher.search(finalQuery, c2);\n\n      TopFieldDocs hits = c.topDocs();\n\n      // Slower way if postings are not pre-sorted by weight:\n      // hits = searcher.search(query, null, num, SORT);\n      results = createResults(searcher, hits, num, key, doHighlight, matchedTokens, prefixToken);\n    } finally {\n      searcherMgr.release(searcher);\n    }\n\n    //System.out.println((System.currentTimeMillis() - t0) + \" msec for infix suggest\");\n    //System.out.println(results);\n\n    return results;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"0977217be8980b17c612feb7ea3d1556ccf4aeed":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b28ce1e2b0fd85f7ca7f72b9ab2ae1f5350c9005":["de3f4a236750bdfe06353c2b1bfe6f5e97de3caa"],"f582f18c13d4852b01d4fe0a0196432c5c6f2b7f":["d17901785875f5d523c1f729cd7898d2ab907c69"],"d17901785875f5d523c1f729cd7898d2ab907c69":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"de3f4a236750bdfe06353c2b1bfe6f5e97de3caa":["f582f18c13d4852b01d4fe0a0196432c5c6f2b7f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["b28ce1e2b0fd85f7ca7f72b9ab2ae1f5350c9005"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"]},"commit2Childs":{"0977217be8980b17c612feb7ea3d1556ccf4aeed":["d17901785875f5d523c1f729cd7898d2ab907c69"],"b28ce1e2b0fd85f7ca7f72b9ab2ae1f5350c9005":["d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30"],"f582f18c13d4852b01d4fe0a0196432c5c6f2b7f":["de3f4a236750bdfe06353c2b1bfe6f5e97de3caa"],"d17901785875f5d523c1f729cd7898d2ab907c69":["f582f18c13d4852b01d4fe0a0196432c5c6f2b7f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0977217be8980b17c612feb7ea3d1556ccf4aeed"],"de3f4a236750bdfe06353c2b1bfe6f5e97de3caa":["b28ce1e2b0fd85f7ca7f72b9ab2ae1f5350c9005"],"d2204dea8cf3dcfbffdf7b4d3459cf5287cc1c30":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}