{"path":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","commits":[{"id":"14d5815ecbef89580f5c48990bcd433f04f8563a","date":1399564106,"type":0,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"/dev/null","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, 10);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + cloudClient.getZkStateReader().getClusterState(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, 20); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aea1d78da2c058b98e64569bcd37981c733b52a8","date":1400551646,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + cloudClient.getZkStateReader().getClusterState(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, 10);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + cloudClient.getZkStateReader().getClusterState(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, 20); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9","date":1400695553,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + cloudClient.getZkStateReader().getClusterState(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","bugFix":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7605579001505896d48b07160075a5c8b8e128e","date":1400758727,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, 10);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + cloudClient.getZkStateReader().getClusterState(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, 20); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f9043cd220362869f58e50f635c13c362f8377da","date":1404227796,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3f5be45b5f54f240a9e1485e92e33a094299659","date":1405328334,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6e36353d7461af8d2329a78a71457cf8e3c1e88f","date":1411572107,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(),\n        notLeaders.size() == 2);\n    \n    sendDoc(1);\n    \n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2d14328dee83c3ec0478e7d711f7af48560ad5ef","date":1412617800,"type":3,"author":"Timothy Potter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d","date":1419896224,"type":3,"author":"Anshum Gupta","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37e7502644cd23597431d66e301299b1ead2fb9b","date":1422636984,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollection(testCollectionName, 1, 3, 1);\n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n    \n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ac97ea104d893f16aab430d9904473bc1f233f3c","date":1496249396,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3217321f3e1d7922898c6c633d17acfa840d6875","date":1496257480,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f","date":1496281877,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    try {\n      CollectionAdminRequest.Delete req = new CollectionAdminRequest.Delete();\n      req.setCollectionName(testCollectionName);\n      req.process(cloudClient);\n    } catch (Exception e) {\n      // don't fail the test\n      log.warn(\"Could not delete collection {} after test completed\", testCollectionName);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"197bbedf08450ade98a11f4a0001448059666bec","date":1498534625,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy0.close();\n    leaderProxy.close();\n    \n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    \n    sendDoc(3, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n\n    leaderProxy.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    \n    proxy0.close();\n    \n    // indexing during a partition\n    sendDoc(2);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    \n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    \n    proxy1.close();\n    \n    sendDoc(3);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a966532d92cf9ba2856f15a8140151bb6b518e4b","date":1588290631,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy0.close();\n    leaderProxy.close();\n    \n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    \n    sendDoc(3, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n\n    leaderProxy.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy0.close();\n    leaderProxy.close();\n    \n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    \n    sendDoc(3, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n\n    leaderProxy.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the \"+testCollectionName+\" collection\");\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e","date":1594223844,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/HttpPartitionTest#testRf3().mjava","sourceNew":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy0.close();\n    leaderProxy.close();\n    \n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    \n    sendDoc(3, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n\n    leaderProxy.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","sourceOld":"  protected void testRf3() throws Exception {\n    // create a collection that has 1 shard but 2 replicas\n    String testCollectionName = \"c8n_1x3\";\n    createCollectionRetry(testCollectionName, \"conf1\", 1, 3, 1);\n    \n    cloudClient.setDefaultCollection(testCollectionName);\n    \n    sendDoc(1);\n\n    List<Replica> notLeaders = \n        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n        + \" but found \" + notLeaders.size() + \"; clusterState: \"\n        + printClusterStateInfo(testCollectionName),\n        notLeaders.size() == 2);\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(getShardLeader(testCollectionName, \"shard1\", 1000)));\n\n    // ok, now introduce a network partition between the leader and the replica\n    SocketProxy proxy0 = getProxyForReplica(notLeaders.get(0));\n    SocketProxy leaderProxy = getProxyForReplica(getShardLeader(testCollectionName, \"shard1\", 1000));\n    \n    proxy0.close();\n    leaderProxy.close();\n    \n    // indexing during a partition\n    sendDoc(2, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy0.reopen();\n    \n    SocketProxy proxy1 = getProxyForReplica(notLeaders.get(1));\n    proxy1.close();\n    \n    sendDoc(3, null, leaderJetty);\n    \n    Thread.sleep(sleepMsBeforeHealPartition);\n    proxy1.reopen();\n\n    leaderProxy.reopen();\n    \n    // sent 4 docs in so far, verify they are on the leader and replica\n    notLeaders = ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive); \n    \n    sendDoc(4);\n    \n    assertDocsExistInAllReplicas(notLeaders, testCollectionName, 1, 4);\n\n    log.info(\"testRf3 succeeded ... deleting the {} collection\", testCollectionName);\n\n    // try to clean up\n    attemptCollectionDelete(cloudClient, testCollectionName);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"55980207f1977bd1463465de1659b821347e2fa8":["6e36353d7461af8d2329a78a71457cf8e3c1e88f","2d14328dee83c3ec0478e7d711f7af48560ad5ef"],"197bbedf08450ade98a11f4a0001448059666bec":["42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"ac97ea104d893f16aab430d9904473bc1f233f3c":["37e7502644cd23597431d66e301299b1ead2fb9b"],"84f20f331d8001864545c7021812d8c6509c7593":["28288370235ed02234a64753cdbf0c6ec096304a"],"2d14328dee83c3ec0478e7d711f7af48560ad5ef":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"3217321f3e1d7922898c6c633d17acfa840d6875":["37e7502644cd23597431d66e301299b1ead2fb9b","ac97ea104d893f16aab430d9904473bc1f233f3c"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["2d14328dee83c3ec0478e7d711f7af48560ad5ef"],"b7605579001505896d48b07160075a5c8b8e128e":["14d5815ecbef89580f5c48990bcd433f04f8563a","67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9"],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["84f20f331d8001864545c7021812d8c6509c7593"],"f9043cd220362869f58e50f635c13c362f8377da":["67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9":["aea1d78da2c058b98e64569bcd37981c733b52a8"],"28288370235ed02234a64753cdbf0c6ec096304a":["3217321f3e1d7922898c6c633d17acfa840d6875","197bbedf08450ade98a11f4a0001448059666bec"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["37e7502644cd23597431d66e301299b1ead2fb9b","3217321f3e1d7922898c6c633d17acfa840d6875"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["37e7502644cd23597431d66e301299b1ead2fb9b","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"b3f5be45b5f54f240a9e1485e92e33a094299659":["f9043cd220362869f58e50f635c13c362f8377da"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["e9017cf144952056066919f1ebc7897ff9bd71b1","197bbedf08450ade98a11f4a0001448059666bec"],"37e7502644cd23597431d66e301299b1ead2fb9b":["d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["b3f5be45b5f54f240a9e1485e92e33a094299659"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"]},"commit2Childs":{"55980207f1977bd1463465de1659b821347e2fa8":[],"197bbedf08450ade98a11f4a0001448059666bec":["28288370235ed02234a64753cdbf0c6ec096304a","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"ac97ea104d893f16aab430d9904473bc1f233f3c":["3217321f3e1d7922898c6c633d17acfa840d6875"],"2d14328dee83c3ec0478e7d711f7af48560ad5ef":["55980207f1977bd1463465de1659b821347e2fa8","d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d"],"84f20f331d8001864545c7021812d8c6509c7593":["a966532d92cf9ba2856f15a8140151bb6b518e4b"],"3217321f3e1d7922898c6c633d17acfa840d6875":["28288370235ed02234a64753cdbf0c6ec096304a","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f"],"d0ec3dbdc850ca18bf4aef9acb85f2ea0554306d":["37e7502644cd23597431d66e301299b1ead2fb9b"],"b7605579001505896d48b07160075a5c8b8e128e":[],"a966532d92cf9ba2856f15a8140151bb6b518e4b":["e46a76bb135597b8bf35930cfdb3702bdd1cbe6e"],"f9043cd220362869f58e50f635c13c362f8377da":["b3f5be45b5f54f240a9e1485e92e33a094299659"],"42dc7f2d60851668d9efa2d12baa1d4ebe54b12f":["197bbedf08450ade98a11f4a0001448059666bec","e9017cf144952056066919f1ebc7897ff9bd71b1"],"28288370235ed02234a64753cdbf0c6ec096304a":["84f20f331d8001864545c7021812d8c6509c7593"],"67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9":["b7605579001505896d48b07160075a5c8b8e128e","f9043cd220362869f58e50f635c13c362f8377da"],"e46a76bb135597b8bf35930cfdb3702bdd1cbe6e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"b3f5be45b5f54f240a9e1485e92e33a094299659":["6e36353d7461af8d2329a78a71457cf8e3c1e88f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["14d5815ecbef89580f5c48990bcd433f04f8563a"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":[],"37e7502644cd23597431d66e301299b1ead2fb9b":["ac97ea104d893f16aab430d9904473bc1f233f3c","3217321f3e1d7922898c6c633d17acfa840d6875","42dc7f2d60851668d9efa2d12baa1d4ebe54b12f","e9017cf144952056066919f1ebc7897ff9bd71b1"],"14d5815ecbef89580f5c48990bcd433f04f8563a":["b7605579001505896d48b07160075a5c8b8e128e","aea1d78da2c058b98e64569bcd37981c733b52a8"],"aea1d78da2c058b98e64569bcd37981c733b52a8":["67d524cb1b29233f0dfbd6e9cba60e5aa0341dd9"],"6e36353d7461af8d2329a78a71457cf8e3c1e88f":["55980207f1977bd1463465de1659b821347e2fa8","2d14328dee83c3ec0478e7d711f7af48560ad5ef"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["55980207f1977bd1463465de1659b821347e2fa8","b7605579001505896d48b07160075a5c8b8e128e","fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}