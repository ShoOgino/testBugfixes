{"path":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","commits":[{"id":"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf","date":1522951207,"type":1,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108","date":1533256859,"type":3,"author":"Erick","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","sourceNew":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"44dd40f6c2c1465aebf4677bab10f696c7ea18d8","date":1539566013,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","sourceNew":"  @Test\n  //Commented 14-Oct-2018 @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","sourceNew":"  @Test\n  //Commented 14-Oct-2018 @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"modelCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"uknownCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"checkpointCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  //Commented 14-Oct-2018 @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"modelCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"uknownCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    AbstractDistribZkTestBase.waitForRecoveriesToFinish(\"checkpointCollection\", cluster.getSolrClient().getZkStateReader(),\n        false, true, TIMEOUT);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":["b5fa1c8367f821057f943ece929329485ec708ba"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a1cae9aea470e88146567017129e8280d21ca76","date":1563504024,"type":3,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"modelCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"uknownCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"checkpointCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n      // use cacheMillis=0 to prevent cached results. it doesn't matter on the first run,\n      // but we want to ensure that when we re-use this expression later after\n      // training another model, we'll still get accurate results.\n      \"model(modelCollection, id=\\\"model\\\", cacheMillis=0),\" +\n      \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n      \"field=\\\"text_s\\\",\" +\n      \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  //Commented 14-Oct-2018 @LuceneTestCase.BadApple(bugUrl=\"https://issues.apache.org/jira/browse/SOLR-12028\") // 2-Aug-2018\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"modelCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"uknownCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"checkpointCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n        \"model(modelCollection, id=\\\"model\\\", cacheMillis=5000),\" +\n        \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n        \"field=\\\"text_s\\\",\" +\n        \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    //Sleep for 5 seconds to let model cache expire\n    Thread.sleep(5100);\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d7ced979f39d7651addfc7d805e1d9bfac215822","date":1589391432,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","pathOld":"solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamDecoratorTest#testClassifyStream().mjava","sourceNew":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"modelCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"uknownCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"checkpointCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    // find a node with a replica\n    ClusterState clusterState = cluster.getSolrClient().getClusterStateProvider().getClusterState();\n    DocCollection coll = clusterState.getCollection(COLLECTIONORALIAS);\n    String node = coll.getReplicas().iterator().next().getNodeName();\n    String url = null;\n    for (JettySolrRunner jetty : cluster.getJettySolrRunners()) {\n      if (jetty.getNodeName().equals(node)) {\n        url = jetty.getBaseUrl().toString()+\"/\"+COLLECTIONORALIAS;\n        break;\n      }\n    }\n    if (url == null) {\n      fail(\"unable to find a node with replica\");\n    }\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n      // use cacheMillis=0 to prevent cached results. it doesn't matter on the first run,\n      // but we want to ensure that when we re-use this expression later after\n      // training another model, we'll still get accurate results.\n      \"model(modelCollection, id=\\\"model\\\", cacheMillis=0),\" +\n      \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n      \"field=\\\"text_s\\\",\" +\n      \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","sourceOld":"  @Test\n  public void testClassifyStream() throws Exception {\n    Assume.assumeTrue(!useAlias);\n\n    CollectionAdminRequest.createCollection(\"modelCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"modelCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"uknownCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"uknownCollection\", 2, 2);\n    CollectionAdminRequest.createCollection(\"checkpointCollection\", \"ml\", 2, 1).process(cluster.getSolrClient());\n    cluster.waitForActiveCollection(\"checkpointCollection\", 2, 2);\n\n    UpdateRequest updateRequest = new UpdateRequest();\n\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"1\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"0\");\n    }\n\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(0), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(1), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    String url = cluster.getJettySolrRunners().get(0).getBaseUrl().toString() + \"/\" + COLLECTIONORALIAS;\n    TupleStream updateTrainModelStream;\n    ModifiableSolrParams paramsLoc;\n\n    StreamFactory factory = new StreamFactory()\n        .withCollectionZkHost(\"collection1\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"modelCollection\", cluster.getZkServer().getZkAddress())\n        .withCollectionZkHost(\"uknownCollection\", cluster.getZkServer().getZkAddress())\n        .withFunctionName(\"features\", FeaturesSelectionStream.class)\n        .withFunctionName(\"train\", TextLogitStream.class)\n        .withFunctionName(\"search\", CloudSolrStream.class)\n        .withFunctionName(\"update\", UpdateStream.class);\n\n    // train the model\n    String textLogitExpression = \"train(\" +\n        \"collection1, \" +\n        \"features(collection1, q=\\\"*:*\\\", featureSet=\\\"first\\\", field=\\\"tv_text\\\", outcome=\\\"out_i\\\", numTerms=4),\"+\n        \"q=\\\"*:*\\\", \" +\n        \"name=\\\"model\\\", \" +\n        \"field=\\\"tv_text\\\", \" +\n        \"outcome=\\\"out_i\\\", \" +\n        \"maxIterations=100)\";\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // classify unknown documents\n    String expr = \"classify(\" +\n      // use cacheMillis=0 to prevent cached results. it doesn't matter on the first run,\n      // but we want to ensure that when we re-use this expression later after\n      // training another model, we'll still get accurate results.\n      \"model(modelCollection, id=\\\"model\\\", cacheMillis=0),\" +\n      \"topic(checkpointCollection, uknownCollection, q=\\\"*:*\\\", fl=\\\"text_s, id\\\", id=\\\"1000000\\\", initialCheckpoint=\\\"0\\\"),\" +\n      \"field=\\\"text_s\\\",\" +\n      \"analyzerField=\\\"tv_text\\\")\";\n\n    paramsLoc = new ModifiableSolrParams();\n    paramsLoc.set(\"expr\", expr);\n    paramsLoc.set(\"qt\", \"/stream\");\n    SolrStream classifyStream = new SolrStream(url, paramsLoc);\n    Map<String, Double> idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"0\"), 0.001);\n    assertEquals(0, idToLabel.get(\"1\"), 0.001);\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(2), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(3), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(1.0, idToLabel.get(\"2\"), 0.001);\n    assertEquals(0, idToLabel.get(\"3\"), 0.001);\n\n\n    // Train another model\n    updateRequest = new UpdateRequest();\n    updateRequest.deleteByQuery(\"*:*\");\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n\n    updateRequest = new UpdateRequest();\n    for (int i = 0; i < 500; i+=2) {\n      updateRequest.add(id, String.valueOf(i), \"tv_text\", \"a b c c d\", \"out_i\", \"0\");\n      updateRequest.add(id, String.valueOf(i+1), \"tv_text\", \"a b e e f\", \"out_i\", \"1\");\n    }\n    updateRequest.commit(cluster.getSolrClient(), COLLECTIONORALIAS);\n    updateTrainModelStream = factory.constructStream(\"update(modelCollection, batchSize=5, \"+textLogitExpression+\")\");\n    getTuples(updateTrainModelStream);\n    cluster.getSolrClient().commit(\"modelCollection\");\n\n    // Add more documents and classify it\n    updateRequest = new UpdateRequest();\n    updateRequest.add(id, String.valueOf(4), \"text_s\", \"a b c c d\");\n    updateRequest.add(id, String.valueOf(5), \"text_s\", \"a b e e f\");\n    updateRequest.commit(cluster.getSolrClient(), \"uknownCollection\");\n\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    //Classify in parallel\n\n    // classify unknown documents\n\n    expr = \"parallel(collection1, workers=2, sort=\\\"_version_ asc\\\", classify(\" +\n           \"model(modelCollection, id=\\\"model\\\"),\" +\n           \"topic(checkpointCollection, uknownCollection, q=\\\"id:(4 5)\\\", fl=\\\"text_s, id, _version_\\\", id=\\\"2000000\\\", partitionKeys=\\\"id\\\", initialCheckpoint=\\\"0\\\"),\" +\n           \"field=\\\"text_s\\\",\" +\n           \"analyzerField=\\\"tv_text\\\"))\";\n\n    paramsLoc.set(\"expr\", expr);\n    classifyStream = new SolrStream(url, paramsLoc);\n    idToLabel = getIdToLabel(classifyStream, \"probability_d\");\n    assertEquals(idToLabel.size(), 2);\n    assertEquals(0, idToLabel.get(\"4\"), 0.001);\n    assertEquals(1.0, idToLabel.get(\"5\"), 0.001);\n\n    CollectionAdminRequest.deleteCollection(\"modelCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"uknownCollection\").process(cluster.getSolrClient());\n    CollectionAdminRequest.deleteCollection(\"checkpointCollection\").process(cluster.getSolrClient());\n  }\n\n","bugFix":["8c969f15cd04d31e520319c619a445ae21f02d72"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d7ced979f39d7651addfc7d805e1d9bfac215822":["8a1cae9aea470e88146567017129e8280d21ca76"],"8a1cae9aea470e88146567017129e8280d21ca76":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d7ced979f39d7651addfc7d805e1d9bfac215822"],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"44dd40f6c2c1465aebf4677bab10f696c7ea18d8":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"05a3c9b5f1dfb39879069eb1dac3ca104d3e4108":["44dd40f6c2c1465aebf4677bab10f696c7ea18d8"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["8a1cae9aea470e88146567017129e8280d21ca76"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf"],"d7ced979f39d7651addfc7d805e1d9bfac215822":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8a1cae9aea470e88146567017129e8280d21ca76":["d7ced979f39d7651addfc7d805e1d9bfac215822"],"8ff654a6d1fb7a79aedaa65c23cc052fdc770aaf":["05a3c9b5f1dfb39879069eb1dac3ca104d3e4108"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}