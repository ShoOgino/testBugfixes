{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","commits":[{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"/dev/null","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c","date":1310389132,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new ReusableAnalyzerBase() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new ReusableAnalyzerBase() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStream tokenStream(String fieldName, Reader reader) {\n          return new MockFixedLengthPayloadFilter(random,\n              new MockTokenizer(reader, MockTokenizer.WHITESPACE, true),\n              length);\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f49143da0a5d278a72f741432047fcfa6da996e","date":1316927425,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new ReusableAnalyzerBase() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new ReusableAnalyzerBase() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"id\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"content\").equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.optimize();\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir, true).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterCommit#testCommitOnCloseDiskUsage().mjava","sourceNew":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","sourceOld":"  /*\n   * Verify that a writer with \"commit on close\" indeed\n   * cleans up the temp segments created after opening\n   * that are not referenced by the starting segments\n   * file.  We check this by using MockDirectoryWrapper to\n   * measure max temp disk space used.\n   */\n  public void testCommitOnCloseDiskUsage() throws IOException {\n    // MemoryCodec, since it uses FST, is not necessarily\n    // \"additive\", ie if you add up N small FSTs, then merge\n    // them, the merged result can easily be larger than the\n    // sum because the merged FST may use array encoding for\n    // some arcs (which uses more space):\n\n    final String idFormat = _TestUtil.getPostingsFormat(\"id\");\n    final String contentFormat = _TestUtil.getPostingsFormat(\"content\");\n    assumeFalse(\"This test cannot run with Memory codec\", idFormat.equals(\"Memory\") || contentFormat.equals(\"Memory\"));\n    MockDirectoryWrapper dir = newDirectory();\n    Analyzer analyzer;\n    if (random.nextBoolean()) {\n      // no payloads\n     analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n        }\n      };\n    } else {\n      // fixed length payloads\n      final int length = random.nextInt(200);\n      analyzer = new Analyzer() {\n        @Override\n        public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n          Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n          return new TokenStreamComponents(tokenizer, new MockFixedLengthPayloadFilter(random, tokenizer, length));\n        }\n      };\n    }\n    \n    IndexWriter writer  = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).\n            setMaxBufferedDocs(10).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n    );\n    for(int j=0;j<30;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    writer.close();\n    dir.resetMaxUsedSizeInBytes();\n\n    dir.setTrackDiskUsage(true);\n    long startDiskUsage = dir.getMaxUsedSizeInBytes();\n    writer = new IndexWriter(\n        dir,\n        newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer)\n            .setOpenMode(OpenMode.APPEND).\n            setMaxBufferedDocs(10).\n            setMergeScheduler(new SerialMergeScheduler()).\n            setReaderPooling(false).\n            setMergePolicy(newLogMergePolicy(10))\n\n    );\n    for(int j=0;j<1470;j++) {\n      TestIndexWriter.addDocWithIndex(writer, j);\n    }\n    long midDiskUsage = dir.getMaxUsedSizeInBytes();\n    dir.resetMaxUsedSizeInBytes();\n    writer.forceMerge(1);\n    writer.close();\n\n    IndexReader.open(dir).close();\n\n    long endDiskUsage = dir.getMaxUsedSizeInBytes();\n\n    // Ending index is 50X as large as starting index; due\n    // to 3X disk usage normally we allow 150X max\n    // transient usage.  If something is wrong w/ deleter\n    // and it doesn't delete intermediate segments then it\n    // will exceed this 150X:\n    // System.out.println(\"start \" + startDiskUsage + \"; mid \" + midDiskUsage + \";end \" + endDiskUsage);\n    assertTrue(\"writer used too much space while adding documents: mid=\" + midDiskUsage + \" start=\" + startDiskUsage + \" end=\" + endDiskUsage + \" max=\" + (startDiskUsage*150),\n               midDiskUsage < 150*startDiskUsage);\n    assertTrue(\"writer used too much space after close: endDiskUsage=\" + endDiskUsage + \" startDiskUsage=\" + startDiskUsage + \" max=\" + (startDiskUsage*150),\n               endDiskUsage < 150*startDiskUsage);\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["7b91922b55d15444d554721b352861d028eb8278"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"7b91922b55d15444d554721b352861d028eb8278":["2f49143da0a5d278a72f741432047fcfa6da996e"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"2f49143da0a5d278a72f741432047fcfa6da996e":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","44d6f0ab53c1962856b9f48dedb7a2a6cc18905c"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["2f49143da0a5d278a72f741432047fcfa6da996e"],"7b91922b55d15444d554721b352861d028eb8278":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"79c2cb24929f2649a8875fb629086171f914d5ce":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"2f49143da0a5d278a72f741432047fcfa6da996e":["7b91922b55d15444d554721b352861d028eb8278"],"44d6f0ab53c1962856b9f48dedb7a2a6cc18905c":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}