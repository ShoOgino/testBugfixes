{"path":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","commits":[{"id":"9a70797e2ad3b67325d3043155af4baf6445fdd9","date":1227585729,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"/dev/null","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize, but conserve memory by not doubling\n        // resize at end??? we waste a maximum of 16K (average of 8K)\n        int[] newMaxTermCounts = new int[maxTermCounts.length+4096];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + field + \", memSize=\" + memSize()\n            + \", time=\"+(endTime-startTime)+\", phase1=\"+(midPoint-startTime)\n            + \", nTerms=\" + numTermsInField + \", bigTerms=\" + bigTerms.size()\n            + \", termInstances=\" + termInstances\n            );\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fa89a35683d73665c61d7af1d16f41649c25e5a7","date":1228684315,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize, but conserve memory by not doubling\n        // resize at end??? we waste a maximum of 16K (average of 8K)\n        int[] newMaxTermCounts = new int[maxTermCounts.length+4096];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize, but conserve memory by not doubling\n        // resize at end??? we waste a maximum of 16K (average of 8K)\n        int[] newMaxTermCounts = new int[maxTermCounts.length+4096];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + field + \", memSize=\" + memSize()\n            + \", time=\"+(endTime-startTime)+\", phase1=\"+(midPoint-startTime)\n            + \", nTerms=\" + numTermsInField + \", bigTerms=\" + bigTerms.size()\n            + \", termInstances=\" + termInstances\n            );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9d0c82b109c0e8a13be31461e8f9d4a9d65e2b0","date":1245166447,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize, but conserve memory by not doubling\n        // resize at end??? we waste a maximum of 16K (average of 8K)\n        int[] newMaxTermCounts = new int[maxTermCounts.length+4096];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#uninvert(SolrIndexSearcher).mjava","sourceNew":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","sourceOld":"  private void uninvert(SolrIndexSearcher searcher) throws IOException {\n    long startTime = System.currentTimeMillis();\n\n    IndexReader reader = searcher.getReader();\n    int maxDoc = reader.maxDoc();\n\n    int[] index = new int[maxDoc];       // immediate term numbers, or the index into the byte[] representing the last number\n    this.index = index;\n    final int[] lastTerm = new int[maxDoc];    // last term we saw for this document\n    final byte[][] bytes = new byte[maxDoc][]; // list of term numbers for the doc (delta encoded vInts)\n    maxTermCounts = new int[1024];\n\n    NumberedTermEnum te = ti.getEnumerator(reader);\n\n    // threshold, over which we use set intersections instead of counting\n    // to (1) save memory, and (2) speed up faceting.\n    // Add 2 for testing purposes so that there will always be some terms under\n    // the threshold even when the index is very small.\n    int threshold = maxDoc / 20 + 2;\n    // threshold = 2000000000; //////////////////////////////// USE FOR TESTING\n    int[] docs = new int[1000];\n    int[] freqs = new int[1000];\n\n    // we need a minimum of 9 bytes, but round up to 12 since the space would\n    // be wasted with most allocators anyway.\n    byte[] tempArr = new byte[12];\n\n    //\n    // enumerate all terms, and build an intermediate form of the un-inverted field.\n    //\n    // During this intermediate form, every document has a (potential) byte[]\n    // and the int[maxDoc()] array either contains the termNumber list directly\n    // or the *end* offset of the termNumber list in it's byte array (for faster\n    // appending and faster creation of the final form).\n    //\n    // idea... if things are too large while building, we could do a range of docs\n    // at a time (but it would be a fair amount slower to build)\n    // could also do ranges in parallel to take advantage of multiple CPUs\n\n    // OPTIONAL: remap the largest df terms to the lowest 128 (single byte)\n    // values.  This requires going over the field first to find the most\n    // frequent terms ahead of time.\n\n    for (;;) {\n      Term t = te.term();\n      if (t==null) break;\n\n      int termNum = te.getTermNumber();\n\n      if (termNum >= maxTermCounts.length) {\n        // resize by doubling - for very large number of unique terms, expanding\n        // by 4K and resultant GC will dominate uninvert times.  Resize at end if material\n        int[] newMaxTermCounts = new int[maxTermCounts.length*2];\n        System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, termNum);\n        maxTermCounts = newMaxTermCounts;\n      }\n\n      int df = te.docFreq();\n      if (df >= threshold) {\n        TopTerm topTerm = new TopTerm();\n        topTerm.term = t;\n        topTerm.termNum = termNum;\n        bigTerms.put(topTerm.termNum, topTerm);\n\n        DocSet set = searcher.getDocSet(new TermQuery(topTerm.term));\n        maxTermCounts[termNum] = set.size();\n\n        te.next();\n        continue;\n      }\n\n      termsInverted++;\n\n      TermDocs td = te.getTermDocs();\n      td.seek(te);\n      for(;;) {\n        int n = td.read(docs,freqs);\n        if (n <= 0) break;\n\n        maxTermCounts[termNum] += n;\n\n        for (int i=0; i<n; i++) {\n          termInstances++;\n          int doc = docs[i];\n          // add 2 to the term number to make room for special reserved values:\n          // 0 (end term) and 1 (index into byte array follows)\n          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;\n          lastTerm[doc] = termNum;\n          int val = index[doc];\n\n          if ((val & 0xff)==1) {\n            // index into byte array (actually the end of\n            // the doc-specific byte[] when building)\n            int pos = val >>> 8;\n            int ilen = vIntSize(delta);\n            byte[] arr = bytes[doc];\n            int newend = pos+ilen;\n            if (newend > arr.length) {\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n              byte[] newarr = new byte[newLen];\n              System.arraycopy(arr, 0, newarr, 0, pos);\n              arr = newarr;\n              bytes[doc] = newarr;\n            }\n            pos = writeInt(delta, arr, pos);\n            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]\n          } else {\n            // OK, this int has data in it... find the end (a zero starting byte - not\n            // part of another number, hence not following a byte with the high bit set).\n            int ipos;\n            if (val==0) {\n              ipos=0;\n            } else if ((val & 0x0000ff80)==0) {\n              ipos=1;\n            } else if ((val & 0x00ff8000)==0) {\n              ipos=2;\n            } else if ((val & 0xff800000)==0) {\n              ipos=3;\n            } else {\n              ipos=4;\n            }\n\n            int endPos = writeInt(delta, tempArr, ipos);\n            if (endPos <= 4) {\n              // value will fit in the integer... move bytes back\n              for (int j=ipos; j<endPos; j++) {\n                val |= (tempArr[j] & 0xff) << (j<<3);\n              }\n              index[doc] = val;\n            } else {\n              // value won't fit... move integer into byte[]\n              for (int j=0; j<ipos; j++) {\n                tempArr[j] = (byte)val;\n                val >>>=8;\n              }\n              // point at the end index in the byte[]\n              index[doc] = (endPos<<8) | 1;\n              bytes[doc] = tempArr;\n              tempArr = new byte[12];\n            }\n\n          }\n\n        }\n\n      }\n\n      te.next();\n    }\n\n    numTermsInField = te.getTermNumber();\n    te.close();\n\n    // free space if outrageously wasteful (tradeoff memory/cpu) \n\n    if ((maxTermCounts.length - numTermsInField) > 1024) { // too much waste!\n      int[] newMaxTermCounts = new int[numTermsInField];\n      System.arraycopy(maxTermCounts, 0, newMaxTermCounts, 0, numTermsInField);\n      maxTermCounts = newMaxTermCounts;\n   }\n\n    long midPoint = System.currentTimeMillis();\n\n    if (termInstances == 0) {\n      // we didn't invert anything\n      // lower memory consumption.\n      index = this.index = null;\n      tnums = null;\n    } else {\n\n      //\n      // transform intermediate form into the final form, building a single byte[]\n      // at a time, and releasing the intermediate byte[]s as we go to avoid\n      // increasing the memory footprint.\n      //\n      for (int pass = 0; pass<256; pass++) {\n        byte[] target = tnums[pass];\n        int pos=0;  // end in target;\n        if (target != null) {\n          pos = target.length;\n        } else {\n          target = new byte[4096];\n        }\n\n        // loop over documents, 0x00ppxxxx, 0x01ppxxxx, 0x02ppxxxx\n        // where pp is the pass (which array we are building), and xx is all values.\n        // each pass shares the same byte[] for termNumber lists.\n        for (int docbase = pass<<16; docbase<maxDoc; docbase+=(1<<24)) {\n          int lim = Math.min(docbase + (1<<16), maxDoc);\n          for (int doc=docbase; doc<lim; doc++) {\n            int val = index[doc];\n            if ((val&0xff) == 1) {\n              int len = val >>> 8;\n              index[doc] = (pos<<8)|1; // change index to point to start of array\n              if ((pos & 0xff000000) != 0) {\n                // we only have 24 bits for the array index\n                throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Too many values for UnInvertedField faceting on field \"+field);\n              }\n              byte[] arr = bytes[doc];\n              bytes[doc] = null;        // IMPORTANT: allow GC to avoid OOM\n              if (target.length <= pos + len) {\n                int newlen = target.length;\n                /*** we don't have to worry about the array getting too large\n                 * since the \"pos\" param will overflow first (only 24 bits available)\n                if ((newlen<<1) <= 0) {\n                  // overflow...\n                  newlen = Integer.MAX_VALUE;\n                  if (newlen <= pos + len) {\n                    throw new SolrException(400,\"Too many terms to uninvert field!\");\n                  }\n                } else {\n                  while (newlen <= pos + len) newlen<<=1;  // doubling strategy\n                }\n                ****/\n                while (newlen <= pos + len) newlen<<=1;  // doubling strategy                 \n                byte[] newtarget = new byte[newlen];\n                System.arraycopy(target, 0, newtarget, 0, pos);\n                target = newtarget;\n              }\n              System.arraycopy(arr, 0, target, pos, len);\n              pos += len + 1;  // skip single byte at end and leave it 0 for terminator\n            }\n          }\n        }\n\n        // shrink array\n        if (pos < target.length) {\n          byte[] newtarget = new byte[pos];\n          System.arraycopy(target, 0, newtarget, 0, pos);\n          target = newtarget;\n          if (target.length > (1<<24)*.9) {\n            SolrCore.log.warn(\"Approaching too many values for UnInvertedField faceting on field '\"+field+\"' : bucket size=\" + target.length);\n          }\n        }\n        \n        tnums[pass] = target;\n\n        if ((pass << 16) > maxDoc)\n          break;\n      }\n    }\n\n    long endTime = System.currentTimeMillis();\n\n    total_time = (int)(endTime-startTime);\n    phase1_time = (int)(midPoint-startTime);\n\n    SolrCore.log.info(\"UnInverted multi-valued field \" + toString());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fa89a35683d73665c61d7af1d16f41649c25e5a7":["9a70797e2ad3b67325d3043155af4baf6445fdd9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"ad94625fb8d088209f46650c8097196fec67f00c":["c9d0c82b109c0e8a13be31461e8f9d4a9d65e2b0"],"c9d0c82b109c0e8a13be31461e8f9d4a9d65e2b0":["fa89a35683d73665c61d7af1d16f41649c25e5a7"],"9a70797e2ad3b67325d3043155af4baf6445fdd9":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"fa89a35683d73665c61d7af1d16f41649c25e5a7":["c9d0c82b109c0e8a13be31461e8f9d4a9d65e2b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["9a70797e2ad3b67325d3043155af4baf6445fdd9"],"9a70797e2ad3b67325d3043155af4baf6445fdd9":["fa89a35683d73665c61d7af1d16f41649c25e5a7"],"ad94625fb8d088209f46650c8097196fec67f00c":[],"c9d0c82b109c0e8a13be31461e8f9d4a9d65e2b0":["ad94625fb8d088209f46650c8097196fec67f00c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ad94625fb8d088209f46650c8097196fec67f00c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"pathCommit":null}