{"path":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = IndexReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = IndexReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = IndexReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bc124b3b129ef11a255212f3af482b771c5b3a6c","date":1344947616,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":null,"sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","date":1345029782,"type":4,"author":"Uwe Schindler","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":null,"sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c188105a9aae04f56c24996f98f8333fc825d2e","date":1345031914,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":null,"sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c93396a1df03720cb20e2c2f513a6fa59b21e4c","date":1345032673,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","pathOld":"/dev/null","sourceNew":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b05c56a41b733e02a189c48895922b5bd8c7f3d1","date":1345033322,"type":4,"author":"Michael McCandless","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPayloadProcessorProvider#doTest(Random,boolean,int,boolean).mjava","sourceNew":null,"sourceOld":"  private void doTest(Random random, boolean addToEmptyIndex,\n      int numExpectedPayloads, boolean multipleCommits) throws IOException {\n    Directory[] dirs = new Directory[2];\n    populateDirs(random, dirs, multipleCommits);\n\n    Directory dir = newDirectory();\n    if (!addToEmptyIndex) {\n      populateDocs(random, dir, multipleCommits);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), NUM_DOCS);\n      verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), NUM_DOCS);\n    }\n\n    // Add two source dirs. By not adding the dest dir, we ensure its payloads\n    // won't get processed.\n    Map<Directory, ReaderPayloadProcessor> processors = new HashMap<Directory, ReaderPayloadProcessor>();\n    for (Directory d : dirs) {\n      processors.put(d, new PerTermPayloadProcessor());\n    }\n    IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    writer.setPayloadProcessorProvider(new PerDirPayloadProcessor(processors));\n\n    IndexReader[] readers = new IndexReader[dirs.length];\n    for (int i = 0; i < readers.length; i++) {\n      readers[i] = DirectoryReader.open(dirs[i]);\n    }\n    try {\n      writer.addIndexes(readers);\n    } finally {\n      for (IndexReader r : readers) {\n        r.close();\n      }\n    }\n    writer.close();\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p1\"), numExpectedPayloads);\n    // the second term should always have all payloads\n    numExpectedPayloads = NUM_DOCS * dirs.length\n        + (addToEmptyIndex ? 0 : NUM_DOCS);\n    verifyPayloadExists(dir, \"p\", new BytesRef(\"p2\"), numExpectedPayloads);\n    for (Directory d : dirs)\n      d.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["3c188105a9aae04f56c24996f98f8333fc825d2e"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","bc124b3b129ef11a255212f3af482b771c5b3a6c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["bc124b3b129ef11a255212f3af482b771c5b3a6c"]},"commit2Childs":{"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["3c188105a9aae04f56c24996f98f8333fc825d2e","bc124b3b129ef11a255212f3af482b771c5b3a6c","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198"],"3c188105a9aae04f56c24996f98f8333fc825d2e":["1c93396a1df03720cb20e2c2f513a6fa59b21e4c"],"b05c56a41b733e02a189c48895922b5bd8c7f3d1":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"1c93396a1df03720cb20e2c2f513a6fa59b21e4c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1"],"bc124b3b129ef11a255212f3af482b771c5b3a6c":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b05c56a41b733e02a189c48895922b5bd8c7f3d1","c1fe8ee1a5a1ef00a9c4793ec26f17bd90342198","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}