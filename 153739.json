{"path":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","commits":[{"id":"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0","date":1383367127,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedDeletes#FrozenBufferedDeletes(BufferedDeletes,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericUpdate> allUpdates = new ArrayList<NumericUpdate>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {\n      for (NumericUpdate update : fieldUpdates.values()) {\n        allUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    updates = allUpdates.toArray(new NumericUpdate[allUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY + numericUpdatesSize + updates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedDeletes(BufferedDeletes deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericUpdate> allUpdates = new ArrayList<NumericUpdate>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {\n      for (NumericUpdate update : fieldUpdates.values()) {\n        allUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    updates = allUpdates.toArray(new NumericUpdate[allUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY + numericUpdatesSize + updates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericUpdate> allUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {\n      for (NumericUpdate update : fieldUpdates.values()) {\n        allUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    updates = allUpdates.toArray(new NumericUpdate[allUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY + numericUpdatesSize + updates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericUpdate> allUpdates = new ArrayList<NumericUpdate>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {\n      for (NumericUpdate update : fieldUpdates.values()) {\n        allUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    updates = allUpdates.toArray(new NumericUpdate[allUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY + numericUpdatesSize + updates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"06805da26538ed636bd89b10c2699cc3834032ae","date":1395132972,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + numericDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF\n        + binaryUpdatesSize + binaryDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericUpdate> allUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericUpdate> fieldUpdates : deletes.numericUpdates.values()) {\n      for (NumericUpdate update : fieldUpdates.values()) {\n        allUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    updates = allUpdates.toArray(new NumericUpdate[allUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY + numericUpdatesSize + updates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":["8405d98acebb7e287bf7ac40e937ba05b8661285"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8405d98acebb7e287bf7ac40e937ba05b8661285","date":1401433291,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + numericDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF\n        + binaryUpdatesSize + binaryDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) terms.getSizeInBytes() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + numericDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF\n        + binaryUpdatesSize + binaryDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":["06805da26538ed636bd89b10c2699cc3834032ae"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4e1019a365f90871803e0de296165d291865c2ce","date":1402929675,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + numericDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF\n        + binaryUpdatesSize + binaryDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c6f080a2ab37c464dd98db173f6cbf10dc74f211","date":1402946779,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + numericDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF\n        + binaryUpdatesSize + binaryDVUpdates.length * RamUsageEstimator.NUM_BYTES_OBJECT_REF;\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e2b30bfb739689d33532e6b7d2d39582bd89a3a","date":1432237721,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    termCount = termsArray.length;\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":null,"sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":4,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":null,"sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":4,"author":"Karl Wright","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/FrozenBufferedUpdates#FrozenBufferedUpdates(BufferedUpdates,boolean).mjava","sourceNew":null,"sourceOld":"  public FrozenBufferedUpdates(BufferedUpdates deletes, boolean isSegmentPrivate) {\n    this.isSegmentPrivate = isSegmentPrivate;\n    assert !isSegmentPrivate || deletes.terms.size() == 0 : \"segment private package should only have del queries\"; \n    Term termsArray[] = deletes.terms.keySet().toArray(new Term[deletes.terms.size()]);\n    ArrayUtil.timSort(termsArray);\n    PrefixCodedTerms.Builder builder = new PrefixCodedTerms.Builder();\n    for (Term term : termsArray) {\n      builder.add(term);\n    }\n    terms = builder.finish();\n    \n    queries = new Query[deletes.queries.size()];\n    queryLimits = new int[deletes.queries.size()];\n    int upto = 0;\n    for(Map.Entry<Query,Integer> ent : deletes.queries.entrySet()) {\n      queries[upto] = ent.getKey();\n      queryLimits[upto] = ent.getValue();\n      upto++;\n    }\n\n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<NumericDocValuesUpdate> allNumericUpdates = new ArrayList<>();\n    int numericUpdatesSize = 0;\n    for (LinkedHashMap<Term,NumericDocValuesUpdate> numericUpdates : deletes.numericUpdates.values()) {\n      for (NumericDocValuesUpdate update : numericUpdates.values()) {\n        allNumericUpdates.add(update);\n        numericUpdatesSize += update.sizeInBytes();\n      }\n    }\n    numericDVUpdates = allNumericUpdates.toArray(new NumericDocValuesUpdate[allNumericUpdates.size()]);\n    \n    // TODO if a Term affects multiple fields, we could keep the updates key'd by Term\n    // so that it maps to all fields it affects, sorted by their docUpto, and traverse\n    // that Term only once, applying the update to all fields that still need to be\n    // updated. \n    List<BinaryDocValuesUpdate> allBinaryUpdates = new ArrayList<>();\n    int binaryUpdatesSize = 0;\n    for (LinkedHashMap<Term,BinaryDocValuesUpdate> binaryUpdates : deletes.binaryUpdates.values()) {\n      for (BinaryDocValuesUpdate update : binaryUpdates.values()) {\n        allBinaryUpdates.add(update);\n        binaryUpdatesSize += update.sizeInBytes();\n      }\n    }\n    binaryDVUpdates = allBinaryUpdates.toArray(new BinaryDocValuesUpdate[allBinaryUpdates.size()]);\n    \n    bytesUsed = (int) (terms.ramBytesUsed() + queries.length * BYTES_PER_DEL_QUERY \n        + numericUpdatesSize + RamUsageEstimator.shallowSizeOf(numericDVUpdates)\n        + binaryUpdatesSize + RamUsageEstimator.shallowSizeOf(binaryDVUpdates));\n    \n    numTermDeletes = deletes.numTermDeletes.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3e2b30bfb739689d33532e6b7d2d39582bd89a3a":["4e1019a365f90871803e0de296165d291865c2ce"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8405d98acebb7e287bf7ac40e937ba05b8661285":["06805da26538ed636bd89b10c2699cc3834032ae"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"06805da26538ed636bd89b10c2699cc3834032ae":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"4e1019a365f90871803e0de296165d291865c2ce":["8405d98acebb7e287bf7ac40e937ba05b8661285"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":["8405d98acebb7e287bf7ac40e937ba05b8661285","4e1019a365f90871803e0de296165d291865c2ce"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["28288370235ed02234a64753cdbf0c6ec096304a"]},"commit2Childs":{"3e2b30bfb739689d33532e6b7d2d39582bd89a3a":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["06805da26538ed636bd89b10c2699cc3834032ae"],"73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"8405d98acebb7e287bf7ac40e937ba05b8661285":["4e1019a365f90871803e0de296165d291865c2ce","c6f080a2ab37c464dd98db173f6cbf10dc74f211"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["73b0a97ef3bd519a5e43398ea9eabe6eed97f6b0"],"06805da26538ed636bd89b10c2699cc3834032ae":["8405d98acebb7e287bf7ac40e937ba05b8661285"],"4e1019a365f90871803e0de296165d291865c2ce":["3e2b30bfb739689d33532e6b7d2d39582bd89a3a","c6f080a2ab37c464dd98db173f6cbf10dc74f211"],"c6f080a2ab37c464dd98db173f6cbf10dc74f211":[],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":[],"28288370235ed02234a64753cdbf0c6ec096304a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["c6f080a2ab37c464dd98db173f6cbf10dc74f211","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}