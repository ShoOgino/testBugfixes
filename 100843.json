{"path":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    final int numDocs1 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newField(\"id\", docCount+\"\", StringField.TYPE_UNSTORED));\n      doc.add(newField(\"content\", \"silly content \" + docCount, TextField.TYPE_UNSTORED));\n      if (docCount == 4) {\n        Field f = newField(\"crash\", \"\", TextField.TYPE_UNSTORED);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    final int numDocs1 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newField(\"id\", docCount+\"\", StringField.TYPE_UNSTORED));\n      doc.add(newField(\"content\", \"silly content \" + docCount, TextField.TYPE_UNSTORED));\n      if (docCount == 4) {\n        Field f = newField(\"crash\", \"\", TextField.TYPE_UNSTORED);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newField(\"id\", docCount+\"\", StringField.TYPE_UNSTORED));\n      doc.add(newField(\"content\", \"silly content \" + docCount, TextField.TYPE_UNSTORED));\n      if (docCount == 4) {\n        Field f = newField(\"crash\", \"\", TextField.TYPE_UNSTORED);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    final int numDocs1 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newField(\"id\", docCount+\"\", StringField.TYPE_UNSTORED));\n      doc.add(newField(\"content\", \"silly content \" + docCount, TextField.TYPE_UNSTORED));\n      if (docCount == 4) {\n        Field f = newField(\"crash\", \"\", TextField.TYPE_UNSTORED);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random.nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newField(\"id\", docCount+\"\", StringField.TYPE_UNSTORED));\n      doc.add(newField(\"content\", \"silly content \" + docCount, TextField.TYPE_UNSTORED));\n      if (docCount == 4) {\n        Field f = newField(\"crash\", \"\", TextField.TYPE_UNSTORED);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newField(\"content\", \"good content\", TextField.TYPE_UNSTORED));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173","date":1365631993,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = new IndexSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(new StringReader(\"crash me on the 4th token\"), MockTokenizer.WHITESPACE, false);\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<Document>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.shutdown();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9e1499c5d26c936238506df90a3c02c76707722","date":1434449920,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"silly\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery();\n    pq.add(new Term(\"content\", \"good\"));\n    pq.add(new Term(\"content\", \"content\"));\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05fe562aa248790944d43cdd478f512572835ba0","date":1455901667,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n\n    IOException expected = expectThrows(IOException.class, () -> {\n      w.addDocuments(docs);\n    });\n    assertEquals(CRASH_FAIL_MESSAGE, expected.getMessage());\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n    try {\n      w.addDocuments(docs);\n      // BUG: CrashingFilter didn't\n      fail(\"did not hit expected exception\");\n    } catch (IOException ioe) {\n      // expected\n      assertEquals(CRASH_FAIL_MESSAGE, ioe.getMessage());\n    }\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":["2c6dc1a64ac36088ccb8d5e20b74c48c8d3bba27"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"470eaac3a77cf637b62126a5408b178d7be93eb1","date":1531830722,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n\n    IOException expected = expectThrows(IOException.class, () -> {\n      w.addDocuments(docs);\n    });\n    assertEquals(CRASH_FAIL_MESSAGE, expected.getMessage());\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.count(pq));\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.count(pq));\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n\n    IOException expected = expectThrows(IOException.class, () -> {\n      w.addDocuments(docs);\n    });\n    assertEquals(CRASH_FAIL_MESSAGE, expected.getMessage());\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterExceptions#testAddDocsNonAbortingException().mjava","sourceNew":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n\n    IOException expected = expectThrows(IOException.class, () -> {\n      w.addDocuments(docs);\n    });\n    assertEquals(CRASH_FAIL_MESSAGE, expected.getMessage());\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.count(pq));\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.count(pq));\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testAddDocsNonAbortingException() throws Exception {\n    final Directory dir = newDirectory();\n    final RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    final int numDocs1 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs1;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n    \n    final List<Document> docs = new ArrayList<>();\n    for(int docCount=0;docCount<7;docCount++) {\n      Document doc = new Document();\n      docs.add(doc);\n      doc.add(newStringField(\"id\", docCount+\"\", Field.Store.NO));\n      doc.add(newTextField(\"content\", \"silly content \" + docCount, Field.Store.NO));\n      if (docCount == 4) {\n        Field f = newTextField(\"crash\", \"\", Field.Store.NO);\n        doc.add(f);\n        MockTokenizer tokenizer = new MockTokenizer(MockTokenizer.WHITESPACE, false);\n        tokenizer.setReader(new StringReader(\"crash me on the 4th token\"));\n        tokenizer.setEnableChecks(false); // disable workflow checking as we forcefully close() in exceptional cases.\n        f.setTokenStream(new CrashingFilter(\"crash\", tokenizer));\n      }\n    }\n\n    IOException expected = expectThrows(IOException.class, () -> {\n      w.addDocuments(docs);\n    });\n    assertEquals(CRASH_FAIL_MESSAGE, expected.getMessage());\n\n    final int numDocs2 = random().nextInt(25);\n    for(int docCount=0;docCount<numDocs2;docCount++) {\n      Document doc = new Document();\n      doc.add(newTextField(\"content\", \"good content\", Field.Store.NO));\n      w.addDocument(doc);\n    }\n\n    final IndexReader r = w.getReader();\n    w.close();\n\n    final IndexSearcher s = newSearcher(r);\n    PhraseQuery pq = new PhraseQuery(\"content\", \"silly\", \"good\");\n    assertEquals(0, s.search(pq, 1).totalHits);\n\n    pq = new PhraseQuery(\"content\", \"good\", \"content\");\n    assertEquals(numDocs1+numDocs2, s.search(pq, 1).totalHits);\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"05fe562aa248790944d43cdd478f512572835ba0":["e9e1499c5d26c936238506df90a3c02c76707722"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["05fe562aa248790944d43cdd478f512572835ba0","470eaac3a77cf637b62126a5408b178d7be93eb1"],"e9e1499c5d26c936238506df90a3c02c76707722":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"470eaac3a77cf637b62126a5408b178d7be93eb1":["05fe562aa248790944d43cdd478f512572835ba0"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["470eaac3a77cf637b62126a5408b178d7be93eb1"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"05fe562aa248790944d43cdd478f512572835ba0":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","470eaac3a77cf637b62126a5408b178d7be93eb1"],"3b0e8c39ca08b5a02de6edcd33d6f3b90b865173":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["3b0e8c39ca08b5a02de6edcd33d6f3b90b865173"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"e9e1499c5d26c936238506df90a3c02c76707722":["05fe562aa248790944d43cdd478f512572835ba0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["e9e1499c5d26c936238506df90a3c02c76707722"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"470eaac3a77cf637b62126a5408b178d7be93eb1":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}