{"path":"modules/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","commits":[{"id":"99b17503f4e360f8140fe80a593268486cd718b4","date":1318337685,"type":1,"author":"Doron Cohen","isMerge":false,"pathNew":"modules/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/util/RandomSample#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (RandomSample.returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (RandomSample.returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (RandomSample.returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  } // end RandomSample.sample2()\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["99b17503f4e360f8140fe80a593268486cd718b4"],"99b17503f4e360f8140fe80a593268486cd718b4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"99b17503f4e360f8140fe80a593268486cd718b4":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["99b17503f4e360f8140fe80a593268486cd718b4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}