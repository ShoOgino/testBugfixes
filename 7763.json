{"path":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","commits":[{"id":"91814ce09be68efd0626969632e79ae47d8876d4","date":1425951537,"type":0,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","pathOld":"/dev/null","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      int workerNum = 0;\n      for(Slice slice : slices) {\n        HashMap params = new HashMap();\n\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", workerNum);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n\n        Collection<Replica> replicas = slice.getReplicas();\n        List<Replica> shuffler = new ArrayList();\n        for(Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n\n        Collections.shuffle(shuffler, new Random(time));\n        Replica rep = shuffler.get(0);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n        ++workerNum;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8d9f14d94de8864c593dc7504d7bfcd9271a79aa"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8d9f14d94de8864c593dc7504d7bfcd9271a79aa","date":1425997553,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      int workerNum = 0;\n      for(Slice slice : slices) {\n        HashMap params = new HashMap();\n\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", workerNum);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n\n        Collection<Replica> replicas = slice.getReplicas();\n        List<Replica> shuffler = new ArrayList();\n        for(Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n\n        Collections.shuffle(shuffler, new Random(time));\n        Replica rep = shuffler.get(0);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n        ++workerNum;\n      }\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":["91814ce09be68efd0626969632e79ae47d8876d4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":0,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","pathOld":"/dev/null","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f00f1c5fad501b66705121feb623f8cfbb6712f9","date":1431347838,"type":5,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/ParallelStream#constructStreams().mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/io/ParallelStream#constructStreams().mjava","sourceNew":"  protected void constructStreams() throws IOException {\n\n    try {\n      Object pushStream = null;\n\n      if (objectSerialize) {\n        ByteArrayOutputStream bout = new ByteArrayOutputStream();\n        ObjectOutputStream out = new ObjectOutputStream(bout);\n        out.writeObject(tupleStream);\n        byte[] bytes = bout.toByteArray();\n        String encoded = Base64.byteArrayToBase64(bytes, 0, bytes.length);\n        pushStream = URLEncoder.encode(encoded, \"UTF-8\");\n      } else {\n        pushStream = ((ExpressibleStream) tupleStream).toExpression(streamFactory);\n      }\n\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", pushStream);\n        params.put(\"qt\",\"/stream\");\n        params.put(\"objectSerialize\", objectSerialize);\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","sourceOld":"  protected void constructStreams() throws IOException {\n\n    try {\n      ZkStateReader zkStateReader = cloudSolrClient.getZkStateReader();\n      ClusterState clusterState = zkStateReader.getClusterState();\n      Collection<Slice> slices = clusterState.getActiveSlices(this.collection);\n      long time = System.currentTimeMillis();\n      List<Replica> shuffler = new ArrayList();\n      for(Slice slice : slices) {\n        Collection<Replica> replicas = slice.getReplicas();\n        for (Replica replica : replicas) {\n          shuffler.add(replica);\n        }\n      }\n\n      if(workers > shuffler.size()) {\n        throw new IOException(\"Number of workers exceeds nodes in the worker collection\");\n      }\n\n      Collections.shuffle(shuffler, new Random(time));\n\n      for(int w=0; w<workers; w++) {\n        HashMap params = new HashMap();\n        params.put(\"distrib\",\"false\"); // We are the aggregator.\n        params.put(\"numWorkers\", workers);\n        params.put(\"workerID\", w);\n        params.put(\"stream\", this.encoded);\n        params.put(\"qt\",\"/stream\");\n        Replica rep = shuffler.get(w);\n        ZkCoreNodeProps zkProps = new ZkCoreNodeProps(rep);\n        String url = zkProps.getCoreUrl();\n        SolrStream solrStream = new SolrStream(url, params);\n        solrStreams.add(solrStream);\n      }\n\n      assert(solrStreams.size() == workers);\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }\n\n","bugFix":null,"bugIntro":["58b0ac98a4f0daf02f6400e863ed311c169630c6"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f00f1c5fad501b66705121feb623f8cfbb6712f9":["8d9f14d94de8864c593dc7504d7bfcd9271a79aa"],"8d9f14d94de8864c593dc7504d7bfcd9271a79aa":["91814ce09be68efd0626969632e79ae47d8876d4"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8d9f14d94de8864c593dc7504d7bfcd9271a79aa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"91814ce09be68efd0626969632e79ae47d8876d4":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f00f1c5fad501b66705121feb623f8cfbb6712f9"]},"commit2Childs":{"f00f1c5fad501b66705121feb623f8cfbb6712f9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"8d9f14d94de8864c593dc7504d7bfcd9271a79aa":["f00f1c5fad501b66705121feb623f8cfbb6712f9","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","91814ce09be68efd0626969632e79ae47d8876d4"],"91814ce09be68efd0626969632e79ae47d8876d4":["8d9f14d94de8864c593dc7504d7bfcd9271a79aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}