{"path":"lucene/luke/src/java/org/apache/lucene/luke/models/analysis/AnalysisImpl#analyzeStepByStep(String).mjava","commits":[{"id":"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77","date":1561188146,"type":0,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/luke/src/java/org/apache/lucene/luke/models/analysis/AnalysisImpl#analyzeStepByStep(String).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public StepByStepResult analyzeStepByStep(String text){\n    Objects.requireNonNull(text);\n    if (analyzer == null) {\n      throw new LukeException(\"Analyzer is not set.\");\n    }\n\n    if (!(analyzer instanceof CustomAnalyzer)) {\n      throw new LukeException(\"Analyzer is not CustomAnalyzer.\");\n    }\n\n    List<NamedTokens> namedTokens = new ArrayList<>();\n    List<CharfilteredText> charfilteredTexts = new ArrayList<>();\n\n    try {\n      CustomAnalyzer customAnalyzer = (CustomAnalyzer)analyzer;\n      final List<CharFilterFactory> charFilterFactories = customAnalyzer.getCharFilterFactories();\n      Reader reader = new StringReader(text);\n      String charFilteredSource = text;\n      if (charFilterFactories.size() > 0) {\n        Reader cs = reader;\n        for (CharFilterFactory charFilterFactory : charFilterFactories) {\n          cs = charFilterFactory.create(reader);\n          Reader readerForWriteOut = new StringReader(charFilteredSource);\n          readerForWriteOut = charFilterFactory.create(readerForWriteOut);\n          charFilteredSource = writeCharStream(readerForWriteOut);\n          charfilteredTexts.add(new CharfilteredText(readerForWriteOut.getClass().getName(), charFilteredSource));\n        }\n        reader = cs;\n      }\n\n      final TokenizerFactory tokenizerFactory = customAnalyzer.getTokenizerFactory();\n      final List<TokenFilterFactory> tokenFilterFactories = customAnalyzer.getTokenFilterFactories();\n\n      TokenStream tokenStream = tokenizerFactory.create();\n      ((Tokenizer)tokenStream).setReader(reader);\n      List<Token> tokens = new ArrayList<>();\n      List<AttributeSource> attributeSources = analyzeTokenStream(tokenStream, tokens);\n      namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n      ListBasedTokenStream listBasedTokenStream = new ListBasedTokenStream(tokenStream, attributeSources);\n      for (TokenFilterFactory tokenFilterFactory : tokenFilterFactories) {\n        tokenStream = tokenFilterFactory.create(listBasedTokenStream);\n        tokens = new ArrayList<>();\n        attributeSources = analyzeTokenStream(tokenStream, tokens);\n        namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n        try {\n          listBasedTokenStream.close();\n        } catch (IOException e) {\n          // do nothing;\n        }\n        listBasedTokenStream = new ListBasedTokenStream(listBasedTokenStream, attributeSources);\n      }\n      try {\n        listBasedTokenStream.close();\n      } catch (IOException e) {\n        // do nothing.\n      } finally {\n        reader.close();\n      }\n      return new StepByStepResult(charfilteredTexts, namedTokens);\n    } catch (Exception e) {\n      throw new LukeException(e.getMessage(), e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"dde00f8ce3ea6870a348e607a273123f0895ec87","date":1561189287,"type":0,"author":"Tomoko Uchida","isMerge":true,"pathNew":"lucene/luke/src/java/org/apache/lucene/luke/models/analysis/AnalysisImpl#analyzeStepByStep(String).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public StepByStepResult analyzeStepByStep(String text){\n    Objects.requireNonNull(text);\n    if (analyzer == null) {\n      throw new LukeException(\"Analyzer is not set.\");\n    }\n\n    if (!(analyzer instanceof CustomAnalyzer)) {\n      throw new LukeException(\"Analyzer is not CustomAnalyzer.\");\n    }\n\n    List<NamedTokens> namedTokens = new ArrayList<>();\n    List<CharfilteredText> charfilteredTexts = new ArrayList<>();\n\n    try {\n      CustomAnalyzer customAnalyzer = (CustomAnalyzer)analyzer;\n      final List<CharFilterFactory> charFilterFactories = customAnalyzer.getCharFilterFactories();\n      Reader reader = new StringReader(text);\n      String charFilteredSource = text;\n      if (charFilterFactories.size() > 0) {\n        Reader cs = reader;\n        for (CharFilterFactory charFilterFactory : charFilterFactories) {\n          cs = charFilterFactory.create(reader);\n          Reader readerForWriteOut = new StringReader(charFilteredSource);\n          readerForWriteOut = charFilterFactory.create(readerForWriteOut);\n          charFilteredSource = writeCharStream(readerForWriteOut);\n          charfilteredTexts.add(new CharfilteredText(readerForWriteOut.getClass().getName(), charFilteredSource));\n        }\n        reader = cs;\n      }\n\n      final TokenizerFactory tokenizerFactory = customAnalyzer.getTokenizerFactory();\n      final List<TokenFilterFactory> tokenFilterFactories = customAnalyzer.getTokenFilterFactories();\n\n      TokenStream tokenStream = tokenizerFactory.create();\n      ((Tokenizer)tokenStream).setReader(reader);\n      List<Token> tokens = new ArrayList<>();\n      List<AttributeSource> attributeSources = analyzeTokenStream(tokenStream, tokens);\n      namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n      ListBasedTokenStream listBasedTokenStream = new ListBasedTokenStream(tokenStream, attributeSources);\n      for (TokenFilterFactory tokenFilterFactory : tokenFilterFactories) {\n        tokenStream = tokenFilterFactory.create(listBasedTokenStream);\n        tokens = new ArrayList<>();\n        attributeSources = analyzeTokenStream(tokenStream, tokens);\n        namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n        try {\n          listBasedTokenStream.close();\n        } catch (IOException e) {\n          // do nothing;\n        }\n        listBasedTokenStream = new ListBasedTokenStream(listBasedTokenStream, attributeSources);\n      }\n      try {\n        listBasedTokenStream.close();\n      } catch (IOException e) {\n        // do nothing.\n      } finally {\n        reader.close();\n      }\n      return new StepByStepResult(charfilteredTexts, namedTokens);\n    } catch (Exception e) {\n      throw new LukeException(e.getMessage(), e);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4a1b9d0d0992557a1e5ce918e02fe32aeb267468","date":1561868844,"type":3,"author":"Tomoko Uchida","isMerge":false,"pathNew":"lucene/luke/src/java/org/apache/lucene/luke/models/analysis/AnalysisImpl#analyzeStepByStep(String).mjava","pathOld":"lucene/luke/src/java/org/apache/lucene/luke/models/analysis/AnalysisImpl#analyzeStepByStep(String).mjava","sourceNew":"  @Override\n  public StepByStepResult analyzeStepByStep(String text){\n    Objects.requireNonNull(text);\n    if (analyzer == null) {\n      throw new LukeException(\"Analyzer is not set.\");\n    }\n\n    if (!(analyzer instanceof CustomAnalyzer)) {\n      throw new LukeException(\"Analyzer is not CustomAnalyzer.\");\n    }\n\n    List<NamedTokens> namedTokens = new ArrayList<>();\n    List<CharfilteredText> charfilteredTexts = new ArrayList<>();\n\n    try {\n      CustomAnalyzer customAnalyzer = (CustomAnalyzer)analyzer;\n      final List<CharFilterFactory> charFilterFactories = customAnalyzer.getCharFilterFactories();\n      Reader reader = new StringReader(text);\n      String charFilteredSource = text;\n      if (charFilterFactories.size() > 0) {\n        Reader cs = reader;\n        for (CharFilterFactory charFilterFactory : charFilterFactories) {\n          cs = charFilterFactory.create(reader);\n          Reader readerForWriteOut = new StringReader(charFilteredSource);\n          readerForWriteOut = charFilterFactory.create(readerForWriteOut);\n          charFilteredSource = writeCharStream(readerForWriteOut);\n          charfilteredTexts.add(new CharfilteredText(CharFilterFactory.findSPIName(charFilterFactory.getClass()), charFilteredSource));\n        }\n        reader = cs;\n      }\n\n      final TokenizerFactory tokenizerFactory = customAnalyzer.getTokenizerFactory();\n      final List<TokenFilterFactory> tokenFilterFactories = customAnalyzer.getTokenFilterFactories();\n\n      TokenStream tokenStream = tokenizerFactory.create();\n      ((Tokenizer)tokenStream).setReader(reader);\n      List<Token> tokens = new ArrayList<>();\n      List<AttributeSource> attributeSources = analyzeTokenStream(tokenStream, tokens);\n      namedTokens.add(new NamedTokens(TokenizerFactory.findSPIName(tokenizerFactory.getClass()), tokens));\n\n      ListBasedTokenStream listBasedTokenStream = new ListBasedTokenStream(tokenStream, attributeSources);\n      for (TokenFilterFactory tokenFilterFactory : tokenFilterFactories) {\n        tokenStream = tokenFilterFactory.create(listBasedTokenStream);\n        tokens = new ArrayList<>();\n        attributeSources = analyzeTokenStream(tokenStream, tokens);\n        namedTokens.add(new NamedTokens(TokenFilterFactory.findSPIName(tokenFilterFactory.getClass()), tokens));\n        try {\n          listBasedTokenStream.close();\n        } catch (IOException e) {\n          // do nothing;\n        }\n        listBasedTokenStream = new ListBasedTokenStream(listBasedTokenStream, attributeSources);\n      }\n      try {\n        listBasedTokenStream.close();\n      } catch (IOException e) {\n        // do nothing.\n      } finally {\n        reader.close();\n      }\n      return new StepByStepResult(charfilteredTexts, namedTokens);\n    } catch (Exception e) {\n      throw new LukeException(e.getMessage(), e);\n    }\n  }\n\n","sourceOld":"  @Override\n  public StepByStepResult analyzeStepByStep(String text){\n    Objects.requireNonNull(text);\n    if (analyzer == null) {\n      throw new LukeException(\"Analyzer is not set.\");\n    }\n\n    if (!(analyzer instanceof CustomAnalyzer)) {\n      throw new LukeException(\"Analyzer is not CustomAnalyzer.\");\n    }\n\n    List<NamedTokens> namedTokens = new ArrayList<>();\n    List<CharfilteredText> charfilteredTexts = new ArrayList<>();\n\n    try {\n      CustomAnalyzer customAnalyzer = (CustomAnalyzer)analyzer;\n      final List<CharFilterFactory> charFilterFactories = customAnalyzer.getCharFilterFactories();\n      Reader reader = new StringReader(text);\n      String charFilteredSource = text;\n      if (charFilterFactories.size() > 0) {\n        Reader cs = reader;\n        for (CharFilterFactory charFilterFactory : charFilterFactories) {\n          cs = charFilterFactory.create(reader);\n          Reader readerForWriteOut = new StringReader(charFilteredSource);\n          readerForWriteOut = charFilterFactory.create(readerForWriteOut);\n          charFilteredSource = writeCharStream(readerForWriteOut);\n          charfilteredTexts.add(new CharfilteredText(readerForWriteOut.getClass().getName(), charFilteredSource));\n        }\n        reader = cs;\n      }\n\n      final TokenizerFactory tokenizerFactory = customAnalyzer.getTokenizerFactory();\n      final List<TokenFilterFactory> tokenFilterFactories = customAnalyzer.getTokenFilterFactories();\n\n      TokenStream tokenStream = tokenizerFactory.create();\n      ((Tokenizer)tokenStream).setReader(reader);\n      List<Token> tokens = new ArrayList<>();\n      List<AttributeSource> attributeSources = analyzeTokenStream(tokenStream, tokens);\n      namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n      ListBasedTokenStream listBasedTokenStream = new ListBasedTokenStream(tokenStream, attributeSources);\n      for (TokenFilterFactory tokenFilterFactory : tokenFilterFactories) {\n        tokenStream = tokenFilterFactory.create(listBasedTokenStream);\n        tokens = new ArrayList<>();\n        attributeSources = analyzeTokenStream(tokenStream, tokens);\n        namedTokens.add(new NamedTokens(tokenStream.getClass().getName(), tokens));\n        try {\n          listBasedTokenStream.close();\n        } catch (IOException e) {\n          // do nothing;\n        }\n        listBasedTokenStream = new ListBasedTokenStream(listBasedTokenStream, attributeSources);\n      }\n      try {\n        listBasedTokenStream.close();\n      } catch (IOException e) {\n        // do nothing.\n      } finally {\n        reader.close();\n      }\n      return new StepByStepResult(charfilteredTexts, namedTokens);\n    } catch (Exception e) {\n      throw new LukeException(e.getMessage(), e);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4a1b9d0d0992557a1e5ce918e02fe32aeb267468":["dde00f8ce3ea6870a348e607a273123f0895ec87"],"dde00f8ce3ea6870a348e607a273123f0895ec87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77"],"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["4a1b9d0d0992557a1e5ce918e02fe32aeb267468"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["dde00f8ce3ea6870a348e607a273123f0895ec87","7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77"],"4a1b9d0d0992557a1e5ce918e02fe32aeb267468":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"dde00f8ce3ea6870a348e607a273123f0895ec87":["4a1b9d0d0992557a1e5ce918e02fe32aeb267468"],"7b7dac0d1d148a67a2728aa772cf93b6a3ef6e77":["dde00f8ce3ea6870a348e607a273123f0895ec87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}