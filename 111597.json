{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","commits":[{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","pathOld":"/dev/null","sourceNew":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes, false);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"));\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","sourceNew":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes, false);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes, false);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"));\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"762c80e29fe0c3bb83aabe2e64af6379273cec7b","date":1484347562,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","sourceNew":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes, false);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"507e7decdf00981d09a74632ea30299a4ce6ba72","date":1484600874,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","sourceNew":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes, false);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429","date":1590107358,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testRandomizedRoundTrip().mjava","sourceNew":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        rTokenStream.close();\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n    rTokenStream.close();\n  }\n\n","sourceOld":"  @Repeat(iterations = 10)\n  //@Seed(\"947083AB20AB2D4F\")\n  public void testRandomizedRoundTrip() throws Exception {\n    final int distinct = TestUtil.nextInt(random(), 1, 10);\n\n    String[] terms = new String[distinct];\n    BytesRef[] termBytes = new BytesRef[distinct];\n    for (int i = 0; i < distinct; ++i) {\n      terms[i] = TestUtil.randomRealisticUnicodeString(random());\n      termBytes[i] = new BytesRef(terms[i]);\n    }\n\n    final BaseTermVectorsFormatTestCase.RandomTokenStream rTokenStream =\n        new BaseTermVectorsFormatTestCase.RandomTokenStream(TestUtil.nextInt(random(), 1, 10), terms, termBytes);\n    //check to see if the token streams might have non-deterministic testable result\n    final boolean storeTermVectorPositions = random().nextBoolean();\n    final int[] startOffsets = rTokenStream.getStartOffsets();\n    final int[] positionsIncrements = rTokenStream.getPositionsIncrements();\n    for (int i = 1; i < positionsIncrements.length; i++) {\n      if (storeTermVectorPositions && positionsIncrements[i] != 0) {\n        continue;\n      }\n      //TODO should RandomTokenStream ensure endOffsets for tokens at same position and same startOffset are greater\n      // than previous token's endOffset?  That would increase the testable possibilities.\n      if (startOffsets[i] == startOffsets[i-1]) {\n        if (VERBOSE)\n          System.out.println(\"Skipping test because can't easily validate random token-stream is correct.\");\n        return;\n      }\n    }\n\n    //sanity check itself\n    assertTokenStreamContents(rTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        rTokenStream.getPositionsIncrements());\n\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(storeTermVectorPositions);\n    //payloads require positions; it will throw an error otherwise\n    myFieldType.setStoreTermVectorPayloads(storeTermVectorPositions && random().nextBoolean());\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", rTokenStream, myFieldType));\n    writer.addDocument(doc);\n\n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream vectorTokenStream =\n        TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    //sometimes check payloads\n    PayloadAttribute payloadAttribute = null;\n    if (myFieldType.storeTermVectorPayloads() && usually()) {\n      payloadAttribute = vectorTokenStream.addAttribute(PayloadAttribute.class);\n    }\n    assertTokenStreamContents(vectorTokenStream,\n        rTokenStream.getTerms(), rTokenStream.getStartOffsets(), rTokenStream.getEndOffsets(),\n        myFieldType.storeTermVectorPositions() ? rTokenStream.getPositionsIncrements() : null);\n    //test payloads\n    if (payloadAttribute != null) {\n      vectorTokenStream.reset();\n      for (int i = 0; vectorTokenStream.incrementToken(); i++) {\n        assertEquals(rTokenStream.getPayloads()[i], payloadAttribute.getPayload());\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5d62e4938659e263e96ae8188e11aea8a940aea5":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"507e7decdf00981d09a74632ea30299a4ce6ba72":["5d62e4938659e263e96ae8188e11aea8a940aea5","762c80e29fe0c3bb83aabe2e64af6379273cec7b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c37ab80ad12b466f3dc92e4baa7b0cbf9aded429"],"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429":["762c80e29fe0c3bb83aabe2e64af6379273cec7b"]},"commit2Childs":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"762c80e29fe0c3bb83aabe2e64af6379273cec7b":["507e7decdf00981d09a74632ea30299a4ce6ba72","c37ab80ad12b466f3dc92e4baa7b0cbf9aded429"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["762c80e29fe0c3bb83aabe2e64af6379273cec7b","507e7decdf00981d09a74632ea30299a4ce6ba72"],"507e7decdf00981d09a74632ea30299a4ce6ba72":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"c37ab80ad12b466f3dc92e4baa7b0cbf9aded429":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["507e7decdf00981d09a74632ea30299a4ce6ba72","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}