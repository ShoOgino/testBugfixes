{"path":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b","date":1288192616,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","date":1288424244,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        Collections.sort(eField_TermDocInfos.getValue(), new Comparator<InstantiatedTermDocumentInformation>() {\n          public int compare(InstantiatedTermDocumentInformation instantiatedTermDocumentInformation, InstantiatedTermDocumentInformation instantiatedTermDocumentInformation1) {\n            return instantiatedTermDocumentInformation.getTerm().getTerm().compareTo(instantiatedTermDocumentInformation1.getTerm().getTerm());\n          }\n        });\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      Arrays.sort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3e00f80591de714c6975f454e33e0fa5218b5902","date":1294514405,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05","date":1294877328,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6ecd298fdc085e7eba27afa7fae58df1ba1a2808","date":1295102557,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16843358872ed92ba92888ab99df297550b9a36a","date":1295144724,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd9325c7ff9928fabe81c28553b41fc7aa57dfab","date":1295896411,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb9b72f7c3d7827c64dd4ec580ded81778da361d","date":1295897920,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarity.computeNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, similarity.encodeNormValue(1.0f));\n      } else {\n        Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, similarity.encodeNormValue(1.0f));\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          float norm = eFieldTermDocInfoFactoriesByTermText.getKey().boost;\n          norm *= document.getDocument().getBoost();\n          norm *= similarity.lengthNorm(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName, eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(eFieldTermDocInfoFactoriesByTermText.getKey().fieldName)[document.getDocumentNumber()] = similarity.encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5ce8d53d5582eaa6a0c771c9b119d480f41da59c","date":1297466174,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f1bdbf92da222965b46c0a942c3857ba56e5c638","date":1298297608,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(fieldName, invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f3cee3d20b0c786e6fca20539454262e29edcab","date":1310101685,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f0b9507caf22f292ac0e5e59f62db4275adf4511","date":1310107283,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          final float norm = similarityProvider.get(fieldName).computeNorm(invertState);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).encodeNormValue(norm);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost);\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost * document.getDocument().getBoost());\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexWriter#commit().mjava","sourceNew":null,"sourceOld":"  /**\n   * Locks the index and commits the buffered documents.\n   */\n  public void commit() throws IOException {\n\n    // todo write lock, unless held by caller\n\n    boolean orderedTermsDirty = false;\n    Set<InstantiatedTerm> dirtyTerms = new HashSet<InstantiatedTerm>(1000);\n    \n    Map<String, FieldSetting> fieldSettingsByFieldName = new HashMap<String, FieldSetting>();\n    for (String fieldName : fieldNameBuffer) {\n      fieldSettingsByFieldName.put(fieldName, new FieldSetting(fieldName));\n    }\n\n    InstantiatedDocument[] documentsByNumber = new InstantiatedDocument[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n    System.arraycopy(index.getDocumentsByNumber(), 0, documentsByNumber, 0, index.getDocumentsByNumber().length);\n    int documentNumber = index.getDocumentsByNumber().length;\n\n    List<InstantiatedTerm> orderedTerms = new ArrayList<InstantiatedTerm>(index.getOrderedTerms().length + 5000);\n    for (InstantiatedTerm instantiatedTerm : index.getOrderedTerms()) {\n      orderedTerms.add(instantiatedTerm);\n    }\n\n    // update norm array with fake values for new documents\n    Map<String, byte[]> normsByFieldNameAndDocumentNumber = new HashMap<String, byte[]>(index.getTermsByFieldAndText().size());\n    Set<String> fieldNames = new HashSet<String>(20);\n    fieldNames.addAll(index.getNormsByFieldNameAndDocumentNumber().keySet());\n    fieldNames.addAll(fieldNameBuffer);\n    for (String field : index.getTermsByFieldAndText().keySet()) {\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      byte[] oldNorms = index.getNormsByFieldNameAndDocumentNumber().get(field);\n      if (oldNorms != null) {\n        System.arraycopy(oldNorms, 0, norms, 0, oldNorms.length);\n        Arrays.fill(norms, oldNorms.length, norms.length, (byte) 0);\n      } else {\n        Arrays.fill(norms, 0, norms.length, (byte) 0);\n      }\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n      fieldNames.remove(field);\n    }\n    for (String field : fieldNames) {\n      //System.out.println(field);\n      byte[] norms = new byte[index.getDocumentsByNumber().length + termDocumentInformationFactoryByDocument.size()];\n      Arrays.fill(norms, 0, norms.length, (byte) 0);\n      normsByFieldNameAndDocumentNumber.put(field, norms);\n    }\n    fieldNames.clear();\n    index.setNormsByFieldNameAndDocumentNumber(normsByFieldNameAndDocumentNumber);\n\n    for (Map.Entry<InstantiatedDocument, Map<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>>> eDocumentTermDocInfoByTermTextAndField : termDocumentInformationFactoryByDocument.entrySet()) {\n\n      InstantiatedDocument document = eDocumentTermDocInfoByTermTextAndField.getKey();\n\n      // assign document number\n      document.setDocumentNumber(documentNumber++);\n      documentsByNumber[document.getDocumentNumber()] = document;\n\n      // set norms, prepare document and create optimized size collections.\n\n      int numFieldsWithTermVectorsInDocument = 0;\n      int termsInDocument = 0;\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldTermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().storeTermVector) {\n          numFieldsWithTermVectorsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n        }\n        termsInDocument += eFieldTermDocInfoFactoriesByTermText.getValue().size();\n\n        if (eFieldTermDocInfoFactoriesByTermText.getKey().indexed && !eFieldTermDocInfoFactoriesByTermText.getKey().omitNorms) {\n          final String fieldName = eFieldTermDocInfoFactoriesByTermText.getKey().fieldName;\n          final FieldInvertState invertState = new FieldInvertState();\n          invertState.setBoost(eFieldTermDocInfoFactoriesByTermText.getKey().boost);\n          invertState.setLength(eFieldTermDocInfoFactoriesByTermText.getKey().fieldLength);\n          normsByFieldNameAndDocumentNumber.get(fieldName)[document.getDocumentNumber()] = similarityProvider.get(fieldName).computeNorm(invertState);\n        } else {\n          System.currentTimeMillis();\n        }\n\n      }\n\n      /** used for term vectors only, i think.. */\n      Map<InstantiatedTerm, InstantiatedTermDocumentInformation> informationByTermOfCurrentDocument = new HashMap<InstantiatedTerm, InstantiatedTermDocumentInformation>(termsInDocument);\n\n\n      Map<String, FieldSetting> documentFieldSettingsByFieldName = new HashMap<String, FieldSetting>(eDocumentTermDocInfoByTermTextAndField.getValue().size());\n\n      // terms...\n      for (Map.Entry<FieldSetting, Map<String /*text*/, TermDocumentInformationFactory>> eFieldSetting_TermDocInfoFactoriesByTermText : eDocumentTermDocInfoByTermTextAndField.getValue().entrySet()) {\n        documentFieldSettingsByFieldName.put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eFieldSetting_TermDocInfoFactoriesByTermText.getKey());\n\n        // find or create term\n        for (Map.Entry<String /*text*/, TermDocumentInformationFactory> eTermText_TermDocInfoFactory : eFieldSetting_TermDocInfoFactoriesByTermText.getValue().entrySet()) {\n\n          // get term..\n          InstantiatedTerm term;\n          Map<String, InstantiatedTerm> termsByText = index.getTermsByFieldAndText().get(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName);\n          if (termsByText == null) {\n            termsByText = new HashMap<String, InstantiatedTerm>(1000);\n            index.getTermsByFieldAndText().put(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, termsByText);\n            term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n            termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n            int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n            pos = -1 - pos;\n            orderedTerms.add(pos, term);\n            orderedTermsDirty = true;\n          } else {\n            term = termsByText.get(eTermText_TermDocInfoFactory.getKey());\n            if (term == null) {\n              term = new InstantiatedTerm(eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName, eTermText_TermDocInfoFactory.getKey());\n              termsByText.put(eTermText_TermDocInfoFactory.getKey(), term);\n              int pos = Collections.binarySearch(orderedTerms, term, InstantiatedTerm.comparator);\n              pos = -1 - pos;\n              orderedTerms.add(pos, term);\n              orderedTermsDirty = true;\n            }\n          }\n\n          // create association term document information\n          //\n          // [Term]-- {0..*} | {0..* ordered} --(field)[Document]\n          //\n          //                 |\n          //        [TermDocumentInformation]\n\n          int[] positions = new int[eTermText_TermDocInfoFactory.getValue().termPositions.size()];\n          for (int i = 0; i < positions.length; i++) {\n            positions[i] = eTermText_TermDocInfoFactory.getValue().termPositions.get(i);\n          }\n\n          byte[][] payloads = new byte[eTermText_TermDocInfoFactory.getValue().payloads.size()][];\n          for (int i = 0; i < payloads.length; i++) {\n            payloads[i] = eTermText_TermDocInfoFactory.getValue().payloads.get(i);\n          }\n\n          // couple\n\n          InstantiatedTermDocumentInformation info = new InstantiatedTermDocumentInformation(term, document, /*eTermText_TermDocInfoFactory.getValue().termFrequency,*/ positions, payloads);\n\n          // todo optimize, this should be cached and updated to array in batches rather than appending the array once for every position!\n          InstantiatedTermDocumentInformation[] associatedDocuments;\n          if (term.getAssociatedDocuments() != null) {\n            associatedDocuments = new InstantiatedTermDocumentInformation[term.getAssociatedDocuments().length + 1];\n            System.arraycopy(term.getAssociatedDocuments(), 0, associatedDocuments, 0, term.getAssociatedDocuments().length);\n          } else {\n            associatedDocuments = new InstantiatedTermDocumentInformation[1];\n          }\n          associatedDocuments[associatedDocuments.length - 1] = info;          \n          term.setAssociatedDocuments(associatedDocuments);\n          term.addPositionsCount(positions.length);\n\n          // todo optimize, only if term vector?\n          informationByTermOfCurrentDocument.put(term, info);\n\n\n          dirtyTerms.add(term);\n        }\n\n        // term vector offsets\n        if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().storeOffsetWithTermVector) {\n          for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> e : informationByTermOfCurrentDocument.entrySet()) {\n            if (eFieldSetting_TermDocInfoFactoriesByTermText.getKey().fieldName.equals(e.getKey().field())) {\n              TermDocumentInformationFactory factory = eFieldSetting_TermDocInfoFactoriesByTermText.getValue().get(e.getKey().text());\n              e.getValue().setTermOffsets(factory.termOffsets.toArray(new TermVectorOffsetInfo[factory.termOffsets.size()]));\n            }\n          }\n        }\n      }\n\n      Map<String, List<InstantiatedTermDocumentInformation>> termDocumentInformationsByField = new HashMap<String, List<InstantiatedTermDocumentInformation>>();\n      for (Map.Entry<InstantiatedTerm, InstantiatedTermDocumentInformation> eTerm_TermDocumentInformation : informationByTermOfCurrentDocument.entrySet()) {\n        List<InstantiatedTermDocumentInformation> termDocumentInformations = termDocumentInformationsByField.get(eTerm_TermDocumentInformation.getKey().field());\n        if (termDocumentInformations == null) {\n          termDocumentInformations = new ArrayList<InstantiatedTermDocumentInformation>();\n          termDocumentInformationsByField.put(eTerm_TermDocumentInformation.getKey().field(), termDocumentInformations);\n        }\n        termDocumentInformations.add(eTerm_TermDocumentInformation.getValue());\n      }\n\n      for (Map.Entry<String, List<InstantiatedTermDocumentInformation>> eField_TermDocInfos : termDocumentInformationsByField.entrySet()) {\n\n        CollectionUtil.quickSort(eField_TermDocInfos.getValue(), tdComp);\n\n        // add term vector\n        if (documentFieldSettingsByFieldName.get(eField_TermDocInfos.getKey()).storeTermVector) {\n          if (document.getVectorSpace() == null) {\n            document.setVectorSpace(new HashMap<String, List<InstantiatedTermDocumentInformation>>(documentFieldSettingsByFieldName.size()));\n          }\n          document.getVectorSpace().put(eField_TermDocInfos.getKey(), eField_TermDocInfos.getValue());\n        }\n\n      }\n      fieldSettingsByFieldName.putAll(documentFieldSettingsByFieldName);\n    }\n\n    // order document informations in dirty terms\n    for (InstantiatedTerm term : dirtyTerms) {\n      // todo optimize, i believe this is useless, that the natural order is document number?\n      ArrayUtil.mergeSort(term.getAssociatedDocuments(), InstantiatedTermDocumentInformation.documentNumberComparator);\n\n//      // update association class reference for speedy skipTo()\n//      for (int i = 0; i < term.getAssociatedDocuments().length; i++) {\n//        term.getAssociatedDocuments()[i].setIndexFromTerm(i);\n//      }\n    }\n\n\n    // flush to writer\n    index.setDocumentsByNumber(documentsByNumber);\n    index.setOrderedTerms(orderedTerms.toArray(new InstantiatedTerm[orderedTerms.size()]));\n\n    for (FieldSetting fieldSetting : fieldSettingsByFieldName.values()) {\n      index.getFieldSettings().merge(fieldSetting);\n    }\n    // set term index\n    if (orderedTermsDirty) {\n      // todo optimize, only update from start position\n      for (int i = 0; i < index.getOrderedTerms().length; i++) {\n        index.getOrderedTerms()[i].setTermIndex(i);\n      }\n\n    }\n\n    // remove deleted documents\n    IndexReader indexDeleter = index.indexReaderFactory();\n    if (unflushedDeletions.size() > 0) {\n      for (Term term : unflushedDeletions) {\n        indexDeleter.deleteDocuments(term);\n      }\n      unflushedDeletions.clear();\n    }\n\n\n    // all done, clear buffers\n    unflushedDocuments.clear();\n    termDocumentInformationFactoryByDocument.clear();\n    fieldNameBuffer.clear();\n\n\n    // update deleted documents bitset\n    if (index.getDeletedDocuments() != null) {\n      BitVector deletedDocuments = new BitVector(index.getDocumentsByNumber().length);\n      for (int i = 0; i < index.getDeletedDocuments().size(); i++) {\n        if (index.getDeletedDocuments().get(i)) {\n          deletedDocuments.set(i);\n        }\n      }\n      index.setDeletedDocuments(deletedDocuments);\n    }\n\n    index.setVersion(System.currentTimeMillis());\n\n    // todo unlock\n\n    indexDeleter.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["3e00f80591de714c6975f454e33e0fa5218b5902"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["5ce8d53d5582eaa6a0c771c9b119d480f41da59c"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":["29ef99d61cda9641b6250bf9567329a6e65f901d","5ce8d53d5582eaa6a0c771c9b119d480f41da59c"],"16843358872ed92ba92888ab99df297550b9a36a":["868da859b43505d9d2a023bfeae6dd0c795f5295","6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"3cc749c053615f5871f3b95715fe292f34e70a53":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["9454a6510e2db155fb01faa5c049b06ece95fab9","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["16843358872ed92ba92888ab99df297550b9a36a","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"1291e4568eb7d9463d751627596ef14baf4c1603":["5ce8d53d5582eaa6a0c771c9b119d480f41da59c","0f3cee3d20b0c786e6fca20539454262e29edcab"],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"3e00f80591de714c6975f454e33e0fa5218b5902":["ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":["5ce8d53d5582eaa6a0c771c9b119d480f41da59c","0f3cee3d20b0c786e6fca20539454262e29edcab"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ca0ffea399542e8aac8ed7608f34f8ec4cb8904d","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5ce8d53d5582eaa6a0c771c9b119d480f41da59c":["fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0f3cee3d20b0c786e6fca20539454262e29edcab"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["9454a6510e2db155fb01faa5c049b06ece95fab9","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"6ecd298fdc085e7eba27afa7fae58df1ba1a2808":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","5ce8d53d5582eaa6a0c771c9b119d480f41da59c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3cc749c053615f5871f3b95715fe292f34e70a53"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"7a54e23e03b47f3d568ab3020bdd386e4b2f0a05":["868da859b43505d9d2a023bfeae6dd0c795f5295","6ecd298fdc085e7eba27afa7fae58df1ba1a2808"],"0f3cee3d20b0c786e6fca20539454262e29edcab":["1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"f1bdbf92da222965b46c0a942c3857ba56e5c638":[],"16843358872ed92ba92888ab99df297550b9a36a":["bb9b72f7c3d7827c64dd4ec580ded81778da361d"],"3cc749c053615f5871f3b95715fe292f34e70a53":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"bb9b72f7c3d7827c64dd4ec580ded81778da361d":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"fd9325c7ff9928fabe81c28553b41fc7aa57dfab":["bb9b72f7c3d7827c64dd4ec580ded81778da361d","29ef99d61cda9641b6250bf9567329a6e65f901d","5ce8d53d5582eaa6a0c771c9b119d480f41da59c"],"1291e4568eb7d9463d751627596ef14baf4c1603":[],"ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","3e00f80591de714c6975f454e33e0fa5218b5902","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"3e00f80591de714c6975f454e33e0fa5218b5902":["7a54e23e03b47f3d568ab3020bdd386e4b2f0a05"],"f0b9507caf22f292ac0e5e59f62db4275adf4511":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":["f1bdbf92da222965b46c0a942c3857ba56e5c638"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"5ce8d53d5582eaa6a0c771c9b119d480f41da59c":["0f3cee3d20b0c786e6fca20539454262e29edcab","f1bdbf92da222965b46c0a942c3857ba56e5c638","1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["3cc749c053615f5871f3b95715fe292f34e70a53"],"ca0ffea399542e8aac8ed7608f34f8ec4cb8904d":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["16843358872ed92ba92888ab99df297550b9a36a"],"6ecd298fdc085e7eba27afa7fae58df1ba1a2808":["16843358872ed92ba92888ab99df297550b9a36a","fd9325c7ff9928fabe81c28553b41fc7aa57dfab"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ae7aa98ce0c64f3b2b81087d14ff9ae992b4903b","ca0ffea399542e8aac8ed7608f34f8ec4cb8904d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["f1bdbf92da222965b46c0a942c3857ba56e5c638","1291e4568eb7d9463d751627596ef14baf4c1603","f0b9507caf22f292ac0e5e59f62db4275adf4511","bde51b089eb7f86171eb3406e38a274743f9b7ac","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}