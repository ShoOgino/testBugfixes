{"path":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","commits":[{"id":"d211216c83f01894810543d1c107160a9ae3650b","date":1488289605,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(Object,TermsEnum[],SegmentMap,float).mjava","sourceNew":"    OrdinalMap(IndexReader.CacheKey owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      this.segmentMap = segmentMap;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n      final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      this.firstSegments = firstSegments.build();\n      this.globalOrdDeltas = globalOrdDeltas.build();\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n          + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n          + segmentMap.ramBytesUsed();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final PackedLongValues deltas = ordDeltas[i].build();\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final PackedLongValues.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","sourceOld":"    OrdinalMap(Object owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      this.segmentMap = segmentMap;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n      final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      this.firstSegments = firstSegments.build();\n      this.globalOrdDeltas = globalOrdDeltas.build();\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n          + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n          + segmentMap.ramBytesUsed();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final PackedLongValues deltas = ordDeltas[i].build();\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final PackedLongValues.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"957c610636f393a85a38f1af670540028db13e6b","date":1500044517,"type":4,"author":"Mike McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","sourceNew":null,"sourceOld":"    OrdinalMap(IndexReader.CacheKey owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      this.segmentMap = segmentMap;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n      final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      this.firstSegments = firstSegments.build();\n      this.globalOrdDeltas = globalOrdDeltas.build();\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n          + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n          + segmentMap.ramBytesUsed();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final PackedLongValues deltas = ordDeltas[i].build();\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final PackedLongValues.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aaf90fc29510e72665ac7934f34c3d1c25efad64","date":1500354819,"type":4,"author":"Cao Manh Dat","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/core/src/java/org/apache/lucene/index/MultiDocValues.OrdinalMap#OrdinalMap(IndexReader.CacheKey,TermsEnum[],SegmentMap,float).mjava","sourceNew":null,"sourceOld":"    OrdinalMap(IndexReader.CacheKey owner, TermsEnum subs[], SegmentMap segmentMap, float acceptableOverheadRatio) throws IOException {\n      // create the ordinal mappings by pulling a termsenum over each sub's \n      // unique terms, and walking a multitermsenum over those\n      this.owner = owner;\n      this.segmentMap = segmentMap;\n      // even though we accept an overhead ratio, we keep these ones with COMPACT\n      // since they are only used to resolve values given a global ord, which is\n      // slow anyway\n      PackedLongValues.Builder globalOrdDeltas = PackedLongValues.monotonicBuilder(PackedInts.COMPACT);\n      PackedLongValues.Builder firstSegments = PackedLongValues.packedBuilder(PackedInts.COMPACT);\n      final PackedLongValues.Builder[] ordDeltas = new PackedLongValues.Builder[subs.length];\n      for (int i = 0; i < ordDeltas.length; i++) {\n        ordDeltas[i] = PackedLongValues.monotonicBuilder(acceptableOverheadRatio);\n      }\n      long[] ordDeltaBits = new long[subs.length];\n      long segmentOrds[] = new long[subs.length];\n      ReaderSlice slices[] = new ReaderSlice[subs.length];\n      TermsEnumIndex indexes[] = new TermsEnumIndex[slices.length];\n      for (int i = 0; i < slices.length; i++) {\n        slices[i] = new ReaderSlice(0, 0, i);\n        indexes[i] = new TermsEnumIndex(subs[segmentMap.newToOld(i)], i);\n      }\n      MultiTermsEnum mte = new MultiTermsEnum(slices);\n      mte.reset(indexes);\n      long globalOrd = 0;\n      while (mte.next() != null) {        \n        TermsEnumWithSlice matches[] = mte.getMatchArray();\n        int firstSegmentIndex = Integer.MAX_VALUE;\n        long globalOrdDelta = Long.MAX_VALUE;\n        for (int i = 0; i < mte.getMatchCount(); i++) {\n          int segmentIndex = matches[i].index;\n          long segmentOrd = matches[i].terms.ord();\n          long delta = globalOrd - segmentOrd;\n          // We compute the least segment where the term occurs. In case the\n          // first segment contains most (or better all) values, this will\n          // help save significant memory\n          if (segmentIndex < firstSegmentIndex) {\n            firstSegmentIndex = segmentIndex;\n            globalOrdDelta = delta;\n          }\n          // for each per-segment ord, map it back to the global term.\n          while (segmentOrds[segmentIndex] <= segmentOrd) {\n            ordDeltaBits[segmentIndex] |= delta;\n            ordDeltas[segmentIndex].add(delta);\n            segmentOrds[segmentIndex]++;\n          }\n        }\n        // for each unique term, just mark the first segment index/delta where it occurs\n        assert firstSegmentIndex < segmentOrds.length;\n        firstSegments.add(firstSegmentIndex);\n        globalOrdDeltas.add(globalOrdDelta);\n        globalOrd++;\n      }\n      this.firstSegments = firstSegments.build();\n      this.globalOrdDeltas = globalOrdDeltas.build();\n      // ordDeltas is typically the bottleneck, so let's see what we can do to make it faster\n      segmentToGlobalOrds = new LongValues[subs.length];\n      long ramBytesUsed = BASE_RAM_BYTES_USED + this.globalOrdDeltas.ramBytesUsed()\n          + this.firstSegments.ramBytesUsed() + RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds)\n          + segmentMap.ramBytesUsed();\n      for (int i = 0; i < ordDeltas.length; ++i) {\n        final PackedLongValues deltas = ordDeltas[i].build();\n        if (ordDeltaBits[i] == 0L) {\n          // segment ords perfectly match global ordinals\n          // likely in case of low cardinalities and large segments\n          segmentToGlobalOrds[i] = LongValues.IDENTITY;\n        } else {\n          final int bitsRequired = ordDeltaBits[i] < 0 ? 64 : PackedInts.bitsRequired(ordDeltaBits[i]);\n          final long monotonicBits = deltas.ramBytesUsed() * 8;\n          final long packedBits = bitsRequired * deltas.size();\n          if (deltas.size() <= Integer.MAX_VALUE\n              && packedBits <= monotonicBits * (1 + acceptableOverheadRatio)) {\n            // monotonic compression mostly adds overhead, let's keep the mapping in plain packed ints\n            final int size = (int) deltas.size();\n            final PackedInts.Mutable newDeltas = PackedInts.getMutable(size, bitsRequired, acceptableOverheadRatio);\n            final PackedLongValues.Iterator it = deltas.iterator();\n            for (int ord = 0; ord < size; ++ord) {\n              newDeltas.set(ord, it.next());\n            }\n            assert !it.hasNext();\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + newDeltas.get((int) ord);\n              }\n            };\n            ramBytesUsed += newDeltas.ramBytesUsed();\n          } else {\n            segmentToGlobalOrds[i] = new LongValues() {\n              @Override\n              public long get(long ord) {\n                return ord + deltas.get(ord);\n              }\n            };\n            ramBytesUsed += deltas.ramBytesUsed();\n          }\n          ramBytesUsed += RamUsageEstimator.shallowSizeOf(segmentToGlobalOrds[i]);\n        }\n      }\n      this.ramBytesUsed = ramBytesUsed;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"957c610636f393a85a38f1af670540028db13e6b":["d211216c83f01894810543d1c107160a9ae3650b"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":["d211216c83f01894810543d1c107160a9ae3650b","957c610636f393a85a38f1af670540028db13e6b"],"d211216c83f01894810543d1c107160a9ae3650b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["957c610636f393a85a38f1af670540028db13e6b"]},"commit2Childs":{"957c610636f393a85a38f1af670540028db13e6b":["aaf90fc29510e72665ac7934f34c3d1c25efad64","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":[],"d211216c83f01894810543d1c107160a9ae3650b":["957c610636f393a85a38f1af670540028db13e6b","aaf90fc29510e72665ac7934f34c3d1c25efad64"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d211216c83f01894810543d1c107160a9ae3650b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["aaf90fc29510e72665ac7934f34c3d1c25efad64","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}