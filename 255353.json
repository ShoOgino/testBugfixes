{"path":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#initTokenBucketsArray().mjava","commits":[{"id":"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","date":1475611903,"type":0,"author":"David Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#initTokenBucketsArray().mjava","pathOld":"/dev/null","sourceNew":"  private TokenLL[] initTokenBucketsArray() throws IOException {\n    // Estimate the number of non-empty positions (number of tokens, excluding same-position synonyms).\n    int positionsEstimate;\n    if (offsetLength == -1) { // no clue what the char length is.\n      // Estimate the number of position slots we need from term stats based on Wikipedia.\n      int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();\n      if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat\n        int size = (int) vector.size();\n        if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way\n          size = 128;\n        }\n        sumTotalTermFreq = (int) (size * 2.4);\n      }\n      positionsEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this\n    } else {\n      // guess number of token positions by this factor.\n      positionsEstimate = (int) (offsetLength / AVG_CHARS_PER_POSITION);\n    }\n    // apply the load factor.\n    return new TokenLL[Math.max(1, (int) (positionsEstimate * loadFactor))];\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#initTokenBucketsArray().mjava","pathOld":"/dev/null","sourceNew":"  private TokenLL[] initTokenBucketsArray() throws IOException {\n    // Estimate the number of non-empty positions (number of tokens, excluding same-position synonyms).\n    int positionsEstimate;\n    if (offsetLength == -1) { // no clue what the char length is.\n      // Estimate the number of position slots we need from term stats based on Wikipedia.\n      int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();\n      if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat\n        int size = (int) vector.size();\n        if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way\n          size = 128;\n        }\n        sumTotalTermFreq = (int) (size * 2.4);\n      }\n      positionsEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this\n    } else {\n      // guess number of token positions by this factor.\n      positionsEstimate = (int) (offsetLength / AVG_CHARS_PER_POSITION);\n    }\n    // apply the load factor.\n    return new TokenLL[Math.max(1, (int) (positionsEstimate * loadFactor))];\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2e9861e4a2b724d9fc51b618714c579491b78d7","date":1479244606,"type":4,"author":"David Smiley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#initTokenBucketsArray().mjava","sourceNew":null,"sourceOld":"  private TokenLL[] initTokenBucketsArray() throws IOException {\n    // Estimate the number of non-empty positions (number of tokens, excluding same-position synonyms).\n    int positionsEstimate;\n    if (offsetLength == -1) { // no clue what the char length is.\n      // Estimate the number of position slots we need from term stats based on Wikipedia.\n      int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();\n      if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat\n        int size = (int) vector.size();\n        if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way\n          size = 128;\n        }\n        sumTotalTermFreq = (int) (size * 2.4);\n      }\n      positionsEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this\n    } else {\n      // guess number of token positions by this factor.\n      positionsEstimate = (int) (offsetLength / AVG_CHARS_PER_POSITION);\n    }\n    // apply the load factor.\n    return new TokenLL[Math.max(1, (int) (positionsEstimate * loadFactor))];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a1ef55e1fff7ff44354432770ad8bc19be1fcc75","date":1479266056,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamFromTermVector#initTokenBucketsArray().mjava","sourceNew":null,"sourceOld":"  private TokenLL[] initTokenBucketsArray() throws IOException {\n    // Estimate the number of non-empty positions (number of tokens, excluding same-position synonyms).\n    int positionsEstimate;\n    if (offsetLength == -1) { // no clue what the char length is.\n      // Estimate the number of position slots we need from term stats based on Wikipedia.\n      int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();\n      if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat\n        int size = (int) vector.size();\n        if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way\n          size = 128;\n        }\n        sumTotalTermFreq = (int) (size * 2.4);\n      }\n      positionsEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this\n    } else {\n      // guess number of token positions by this factor.\n      positionsEstimate = (int) (offsetLength / AVG_CHARS_PER_POSITION);\n    }\n    // apply the load factor.\n    return new TokenLL[Math.max(1, (int) (positionsEstimate * loadFactor))];\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f2e9861e4a2b724d9fc51b618714c579491b78d7"],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f2e9861e4a2b724d9fc51b618714c579491b78d7"]},"commit2Childs":{"f2e9861e4a2b724d9fc51b618714c579491b78d7":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a1ef55e1fff7ff44354432770ad8bc19be1fcc75":[],"1f5ce59aaf4a055cc9ec62c15a89c263a05ff4b2":["f2e9861e4a2b724d9fc51b618714c579491b78d7","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a1ef55e1fff7ff44354432770ad8bc19be1fcc75","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}