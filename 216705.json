{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","commits":[{"id":"163fe85a71d778fd2b7747f65ca27b54829e2e57","date":1279898785,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":1,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void testOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1085ea837da8f1e96697e17cf73e1d08e7329261","date":1281469548,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":["87c966e9308847938a7c905c2e46a56d8df788b8"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        dir.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      modifier.close();\n      startDir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3e038f02e7d2c5b563efede5b85ecdd26a0cc635","date":1281780240,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        dir.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      modifier.close();\n      startDir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        dir.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      modifier.close();\n      startDir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        dir.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      modifier.close();\n      startDir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory(random);\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(random,\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53a31399f2471493d67b19a95c028a74e0113b6a","date":1289817072,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ab1f5591dc05f1f2b5407d809c9699f75554a32","date":1290008586,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"44fcbde6fb2ac44ee3b45e013e54a42911e689ff","date":1292065621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockRAMDirectory startDir = new MockRAMDirectory();\n    IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(new Field(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      MockRAMDirectory dir = new MockRAMDirectory(startDir);\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)).setMaxBufferedDocs(1000)\n          .setMaxBufferedDeleteTerms(1000));\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate, diskFree);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(new Field(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(new Field(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success)\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n\n        searcher.close();\n        newReader.close();\n\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n\n      dir.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = new IndexSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        dir.setRandomIOExceptionRate(0.0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b8939b978dc1222f57c36825d26fe2c5fb0560ca","date":1302784666,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        dir.setRandomIOExceptionRate(0.0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c2baae7f293a07b1a1457a82bb0dd7f7ce3cd2a6","date":1302794165,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        dir.setRandomIOExceptionRate(0.0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1eb46686a27187e42311e77666a2c7026f461ebc","date":1302858020,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        dir.setRandomIOExceptionRate(0.0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"639c36565ce03aed5b0fce7c9e4448e53a1f7efd","date":1308580104,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, IOContext.DEFAULT));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b6f9be74ca7baaef11857ad002cad40419979516","date":1309449808,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, IOContext.DEFAULT));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ddc4c914be86e34b54f70023f45a60fa7f04e929","date":1310115160,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5d004d0e0b3f65bb40da76d476d659d7888270e8","date":1310158940,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n         \n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                      Field.Index.NOT_ANALYZED));\n      d.add(newField(\"content\", \"aaa \" + i, Field.Store.NO,\n                      Field.Index.ANALYZED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), Field.Store.YES,\n                                Field.Index.NOT_ANALYZED));\n                d.add(newField(\"content\", \"bbb \" + i, Field.Store.NO,\n                                Field.Index.ANALYZED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n      modifier.setInfoStream(VERBOSE ? System.out : null);\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c5b026d03cbbb03ca4c0b97d14e9839682281dc","date":1323049298,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3615ce4a1f785ae1b779244de52c6a7d99227e60","date":1323422019,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","date":1323437438,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir, true);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e7c2454a6a8237bfd0e953f5b940838408c9055","date":1323649300,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","date":1323720782,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        searcher.close();\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#doTestOperationsOnDiskFull(boolean).mjava","sourceNew":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure if modifier tries to commit but hits disk full that modifier\n   * remains consistent and usable. Similar to TestIndexReader.testDiskFull().\n   */\n  private void doTestOperationsOnDiskFull(boolean updates) throws IOException {\n\n    Term searchTerm = new Term(\"content\", \"aaa\");\n    int START_COUNT = 157;\n    int END_COUNT = 144;\n\n    // First build up a starting index:\n    MockDirectoryWrapper startDir = newDirectory();\n    // TODO: find the resource leak that only occurs sometimes here.\n    startDir.setNoDeleteOpenFile(false);\n    IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));\n    for (int i = 0; i < 157; i++) {\n      Document d = new Document();\n      d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n      d.add(newField(\"content\", \"aaa \" + i, TextField.TYPE_UNSTORED));\n      writer.addDocument(d);\n    }\n    writer.close();\n\n    long diskUsage = startDir.sizeInBytes();\n    long diskFree = diskUsage + 10;\n\n    IOException err = null;\n\n    boolean done = false;\n\n    // Iterate w/ ever increasing free disk space:\n    while (!done) {\n      if (VERBOSE) {\n        System.out.println(\"TEST: cycle\");\n      }\n      MockDirectoryWrapper dir = new MockDirectoryWrapper(random, new RAMDirectory(startDir, newIOContext(random)));\n      dir.setPreventDoubleWrite(false);\n      IndexWriter modifier = new IndexWriter(dir,\n                                             newIndexWriterConfig(\n                                                                  TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false))\n                                             .setMaxBufferedDocs(1000)\n                                             .setMaxBufferedDeleteTerms(1000)\n                                             .setMergeScheduler(new ConcurrentMergeScheduler()));\n      ((ConcurrentMergeScheduler) modifier.getConfig().getMergeScheduler()).setSuppressExceptions();\n\n      // For each disk size, first try to commit against\n      // dir that will hit random IOExceptions & disk\n      // full; after, give it infinite disk space & turn\n      // off random IOExceptions & retry w/ same reader:\n      boolean success = false;\n\n      for (int x = 0; x < 2; x++) {\n        if (VERBOSE) {\n          System.out.println(\"TEST: x=\" + x);\n        }\n\n        double rate = 0.1;\n        double diskRatio = ((double)diskFree) / diskUsage;\n        long thisDiskFree;\n        String testName;\n\n        if (0 == x) {\n          thisDiskFree = diskFree;\n          if (diskRatio >= 2.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 4.0) {\n            rate /= 2;\n          }\n          if (diskRatio >= 6.0) {\n            rate = 0.0;\n          }\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: \" + diskFree + \" bytes\");\n          }\n          testName = \"disk full during reader.close() @ \" + thisDiskFree\n            + \" bytes\";\n        } else {\n          thisDiskFree = 0;\n          rate = 0.0;\n          if (VERBOSE) {\n            System.out.println(\"\\ncycle: same writer: unlimited disk space\");\n          }\n          testName = \"reader re-use after disk full\";\n        }\n\n        dir.setMaxSizeInBytes(thisDiskFree);\n        dir.setRandomIOExceptionRate(rate);\n\n        try {\n          if (0 == x) {\n            int docId = 12;\n            for (int i = 0; i < 13; i++) {\n              if (updates) {\n                Document d = new Document();\n                d.add(newField(\"id\", Integer.toString(i), StringField.TYPE_STORED));\n                d.add(newField(\"content\", \"bbb \" + i, TextField.TYPE_UNSTORED));\n                modifier.updateDocument(new Term(\"id\", Integer.toString(docId)), d);\n              } else { // deletes\n                modifier.deleteDocuments(new Term(\"id\", Integer.toString(docId)));\n                // modifier.setNorm(docId, \"contents\", (float)2.0);\n              }\n              docId += 12;\n            }\n          }\n          modifier.close();\n          success = true;\n          if (0 == x) {\n            done = true;\n          }\n        }\n        catch (IOException e) {\n          if (VERBOSE) {\n            System.out.println(\"  hit IOException: \" + e);\n            e.printStackTrace(System.out);\n          }\n          err = e;\n          if (1 == x) {\n            e.printStackTrace();\n            fail(testName + \" hit IOException after disk space was freed up\");\n          }\n        }\n        // prevent throwing a random exception here!!\n        final double randomIOExceptionRate = dir.getRandomIOExceptionRate();\n        final long maxSizeInBytes = dir.getMaxSizeInBytes();\n        dir.setRandomIOExceptionRate(0.0);\n        dir.setMaxSizeInBytes(0);\n        if (!success) {\n          // Must force the close else the writer can have\n          // open files which cause exc in MockRAMDir.close\n          if (VERBOSE) {\n            System.out.println(\"TEST: now rollback\");\n          }\n          modifier.rollback();\n        }\n\n        // If the close() succeeded, make sure there are\n        // no unreferenced files.\n        if (success) {\n          _TestUtil.checkIndex(dir);\n          TestIndexWriter.assertNoUnreferencedFiles(dir, \"after writer.close\");\n        }\n        dir.setRandomIOExceptionRate(randomIOExceptionRate);\n        dir.setMaxSizeInBytes(maxSizeInBytes);\n\n        // Finally, verify index is not corrupt, and, if\n        // we succeeded, we see all docs changed, and if\n        // we failed, we see either all docs or no docs\n        // changed (transactional semantics):\n        IndexReader newReader = null;\n        try {\n          newReader = IndexReader.open(dir);\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName\n               + \":exception when creating IndexReader after disk full during close: \"\n               + e);\n        }\n\n        IndexSearcher searcher = newSearcher(newReader);\n        ScoreDoc[] hits = null;\n        try {\n          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n        }\n        catch (IOException e) {\n          e.printStackTrace();\n          fail(testName + \": exception when searching: \" + e);\n        }\n        int result2 = hits.length;\n        if (success) {\n          if (x == 0 && result2 != END_COUNT) {\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + END_COUNT);\n          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {\n            // It's possible that the first exception was\n            // \"recoverable\" wrt pending deletes, in which\n            // case the pending deletes are retained and\n            // then re-flushing (with plenty of disk\n            // space) will succeed in flushing the\n            // deletes:\n            fail(testName\n                 + \": method did not throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        } else {\n          // On hitting exception we still may have added\n          // all docs:\n          if (result2 != START_COUNT && result2 != END_COUNT) {\n            err.printStackTrace();\n            fail(testName\n                 + \": method did throw exception but hits.length for search on term 'aaa' is \"\n                 + result2 + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n          }\n        }\n        newReader.close();\n        if (result2 == END_COUNT) {\n          break;\n        }\n      }\n      dir.close();\n      modifier.close();\n\n      // Try again with 10 more bytes of free space:\n      diskFree += 10;\n    }\n    startDir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["53a31399f2471493d67b19a95c028a74e0113b6a"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["06584e6e98d592b34e1329b384182f368d2025e8","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["0e7c2454a6a8237bfd0e953f5b940838408c9055"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["06584e6e98d592b34e1329b384182f368d2025e8"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"53a31399f2471493d67b19a95c028a74e0113b6a":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["a05409176bd65129d67a785ee70e881e238a9aef"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6f9be74ca7baaef11857ad002cad40419979516":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd"],"a05409176bd65129d67a785ee70e881e238a9aef":["3e038f02e7d2c5b563efede5b85ecdd26a0cc635"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":["a3776dccca01c11e7046323cfad46a3b4a471233","ddc4c914be86e34b54f70023f45a60fa7f04e929"],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","b6f9be74ca7baaef11857ad002cad40419979516"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1085ea837da8f1e96697e17cf73e1d08e7329261"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","4b103252dee6afa1b6d7a622c773d178788eb85a"],"1085ea837da8f1e96697e17cf73e1d08e7329261":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","163fe85a71d778fd2b7747f65ca27b54829e2e57"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["132903c28af3aa6f67284b78de91c0f0a99488c2","53a31399f2471493d67b19a95c028a74e0113b6a"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"06584e6e98d592b34e1329b384182f368d2025e8":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["b8939b978dc1222f57c36825d26fe2c5fb0560ca","1eb46686a27187e42311e77666a2c7026f461ebc"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["9ab1f5591dc05f1f2b5407d809c9699f75554a32","44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"962d04139994fce5193143ef35615499a9a96d78":["bde51b089eb7f86171eb3406e38a274743f9b7ac","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3e038f02e7d2c5b563efede5b85ecdd26a0cc635":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff"],"1eb46686a27187e42311e77666a2c7026f461ebc":["c2baae7f293a07b1a1457a82bb0dd7f7ce3cd2a6","b8939b978dc1222f57c36825d26fe2c5fb0560ca"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["06584e6e98d592b34e1329b384182f368d2025e8","1c5b026d03cbbb03ca4c0b97d14e9839682281dc"],"c2baae7f293a07b1a1457a82bb0dd7f7ce3cd2a6":["962d04139994fce5193143ef35615499a9a96d78"],"b8939b978dc1222f57c36825d26fe2c5fb0560ca":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"44fcbde6fb2ac44ee3b45e013e54a42911e689ff":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","ab5cb6a74aefb78aa0569857970b9151dfe2e787","790e1fde4caa765b3faaad3fbcd25c6973450336"],"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"93ccd971aca7fb61b7f1b946e44714cfc80bfc7c":[],"639c36565ce03aed5b0fce7c9e4448e53a1f7efd":["b6f9be74ca7baaef11857ad002cad40419979516"],"1c5b026d03cbbb03ca4c0b97d14e9839682281dc":["3615ce4a1f785ae1b779244de52c6a7d99227e60"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["53a31399f2471493d67b19a95c028a74e0113b6a","9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"53a31399f2471493d67b19a95c028a74e0113b6a":["44fcbde6fb2ac44ee3b45e013e54a42911e689ff","9ab1f5591dc05f1f2b5407d809c9699f75554a32"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["962d04139994fce5193143ef35615499a9a96d78","b8939b978dc1222f57c36825d26fe2c5fb0560ca"],"0e7c2454a6a8237bfd0e953f5b940838408c9055":["3a119bbc8703c10faa329ec201c654b3a35a1e3e","93ccd971aca7fb61b7f1b946e44714cfc80bfc7c"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3242a09f703274d3b9283f2064a1a33064b53a1b","4b103252dee6afa1b6d7a622c773d178788eb85a","163fe85a71d778fd2b7747f65ca27b54829e2e57"],"b6f9be74ca7baaef11857ad002cad40419979516":["ddc4c914be86e34b54f70023f45a60fa7f04e929"],"a05409176bd65129d67a785ee70e881e238a9aef":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"5d004d0e0b3f65bb40da76d476d659d7888270e8":[],"ddc4c914be86e34b54f70023f45a60fa7f04e929":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5","5d004d0e0b3f65bb40da76d476d659d7888270e8"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["06584e6e98d592b34e1329b384182f368d2025e8"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["962d04139994fce5193143ef35615499a9a96d78"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"1085ea837da8f1e96697e17cf73e1d08e7329261":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["3242a09f703274d3b9283f2064a1a33064b53a1b","1085ea837da8f1e96697e17cf73e1d08e7329261"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"9ab1f5591dc05f1f2b5407d809c9699f75554a32":["ab5cb6a74aefb78aa0569857970b9151dfe2e787"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["3e038f02e7d2c5b563efede5b85ecdd26a0cc635"],"06584e6e98d592b34e1329b384182f368d2025e8":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","1c5b026d03cbbb03ca4c0b97d14e9839682281dc","3615ce4a1f785ae1b779244de52c6a7d99227e60"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["639c36565ce03aed5b0fce7c9e4448e53a1f7efd","ddc4c914be86e34b54f70023f45a60fa7f04e929","135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"962d04139994fce5193143ef35615499a9a96d78":["c2baae7f293a07b1a1457a82bb0dd7f7ce3cd2a6"],"3e038f02e7d2c5b563efede5b85ecdd26a0cc635":["a05409176bd65129d67a785ee70e881e238a9aef"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["f2c5f0cb44df114db4228c8f77861714b5cabaea","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"1eb46686a27187e42311e77666a2c7026f461ebc":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5d004d0e0b3f65bb40da76d476d659d7888270e8"],"3615ce4a1f785ae1b779244de52c6a7d99227e60":["ba5bc70a1fc1e0abc1eb4171af0d6f2532711c00","0e7c2454a6a8237bfd0e953f5b940838408c9055"],"c2baae7f293a07b1a1457a82bb0dd7f7ce3cd2a6":["1eb46686a27187e42311e77666a2c7026f461ebc"],"b8939b978dc1222f57c36825d26fe2c5fb0560ca":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","1eb46686a27187e42311e77666a2c7026f461ebc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["93ccd971aca7fb61b7f1b946e44714cfc80bfc7c","5d004d0e0b3f65bb40da76d476d659d7888270e8","135621f3a0670a9394eb563224a3b76cc4dddc0f","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}