{"path":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"59fc0e55b44c555c39d950def9414b5596c6ebe2","date":1327620010,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"78a55f24d9b493c2a1cecf79f1d78279062b545b","date":1327688152,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fd92b8bcc88e969302510acf77bd6970da3994c4","date":1327839530,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96d207426bd26fa5c1014e26d21d87603aea68b7","date":1327944562,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5cab9a86bd67202d20b6adc463008c8e982b070a","date":1327966443,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(new SlowMultiReaderWrapper(searcher.getIndexReader()));\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c750c9942b5f894af0f235b503bec55f2db8d1d3","date":1329857574,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":["9a70797e2ad3b67325d3043155af4baf6445fdd9"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","date":1331075828,"type":3,"author":"Ryan McKinley","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","date":1373996650,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"19275ba31e621f6da1b83bf13af75233876fd3d4","date":1374846698,"type":3,"author":"Han Jiang","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","date":1376375609,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr, true) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1","date":1392536197,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        OpenBitSet bs = (OpenBitSet)((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<Integer>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRef prefixBr = new BytesRef(prefix);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRef charsRef = new CharsRef();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getAtomicReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","date":1419400138,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find its start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find it's start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d12bbc45d641864ffe03291bc30f178eb34e434c","date":1426001646,"type":4,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find its start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":4,"author":"Ryan Ernst","isMerge":true,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getCounts(SolrIndexSearcher,DocSet,int,int,Integer,boolean,String,String).mjava","sourceNew":null,"sourceOld":"  public NamedList<Integer> getCounts(SolrIndexSearcher searcher, DocSet baseDocs, int offset, int limit, Integer mincount, boolean missing, String sort, String prefix) throws IOException {\n    use.incrementAndGet();\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    NamedList<Integer> res = new NamedList<>();  // order is important\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    //System.out.println(\"GET COUNTS field=\" + field + \" baseSize=\" + baseSize + \" minCount=\" + mincount + \" maxDoc=\" + maxDoc + \" numTermsInField=\" + numTermsInField);\n    if (baseSize >= mincount) {\n\n      final int[] index = this.index;\n      // tricky: we add more more element than we need because we will reuse this array later\n      // for ordering term ords before converting to term labels.\n      final int[] counts = new int[numTermsInField + 1];\n\n      //\n      // If there is prefix, find its start and end term numbers\n      //\n      int startTerm = 0;\n      int endTerm = numTermsInField;  // one past the end\n\n      TermsEnum te = getOrdTermsEnum(searcher.getLeafReader());\n      if (te != null && prefix != null && prefix.length() > 0) {\n        final BytesRefBuilder prefixBr = new BytesRefBuilder();\n        prefixBr.copyChars(prefix);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          startTerm = numTermsInField;\n        } else {\n          startTerm = (int) te.ord();\n        }\n        prefixBr.append(UnicodeUtil.BIG_TERM);\n        if (te.seekCeil(prefixBr.get()) == TermsEnum.SeekStatus.END) {\n          endTerm = numTermsInField;\n        } else {\n          endTerm = (int) te.ord();\n        }\n      }\n\n      /***********\n      // Alternative 2: get the docSet of the prefix (could take a while) and\n      // then do the intersection with the baseDocSet first.\n      if (prefix != null && prefix.length() > 0) {\n        docs = searcher.getDocSet(new ConstantScorePrefixQuery(new Term(field, ft.toInternal(prefix))), docs);\n        // The issue with this method are problems of returning 0 counts for terms w/o\n        // the prefix.  We can't just filter out those terms later because it may\n        // mean that we didn't collect enough terms in the queue (in the sorted case).\n      }\n      ***********/\n\n      boolean doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && startTerm==0 && endTerm==numTermsInField\n              && docs instanceof BitDocSet;\n\n      if (doNegative) {\n        FixedBitSet bs = ((BitDocSet)docs).getBits().clone();\n        bs.flip(0, maxDoc);\n        // TODO: when iterator across negative elements is available, use that\n        // instead of creating a new bitset and inverting.\n        docs = new BitDocSet(bs, maxDoc - baseSize);\n        // simply negating will mean that we have deleted docs in the set.\n        // that should be OK, as their entries in our table should be empty.\n        //System.out.println(\"  NEG\");\n      }\n\n      // For the biggest terms, do straight set intersections\n      for (TopTerm tt : bigTerms.values()) {\n        //System.out.println(\"  do big termNum=\" + tt.termNum + \" term=\" + tt.term.utf8ToString());\n        // TODO: counts could be deferred if sorted==false\n        if (tt.termNum >= startTerm && tt.termNum < endTerm) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(new Term(field, tt.term)), docs);\n          //System.out.println(\"    count=\" + counts[tt.termNum]);\n        } else {\n          //System.out.println(\"SKIP term=\" + tt.termNum);\n        }\n      }\n\n      // TODO: we could short-circuit counting altogether for sorted faceting\n      // where we already have enough terms from the bigTerms\n\n      // TODO: we could shrink the size of the collection array, and\n      // additionally break when the termNumber got above endTerm, but\n      // it would require two extra conditionals in the inner loop (although\n      // they would be predictable for the non-prefix case).\n      // Perhaps a different copy of the code would be warranted.\n\n      if (termInstances > 0) {\n        DocIterator iter = docs.iterator();\n        while (iter.hasNext()) {\n          int doc = iter.nextDoc();\n          //System.out.println(\"iter doc=\" + doc);\n          int code = index[doc];\n\n          if ((code & 0xff)==1) {\n            //System.out.println(\"  ptr\");\n            int pos = code>>>8;\n            int whichArray = (doc >>> 16) & 0xff;\n            byte[] arr = tnums[whichArray];\n            int tnum = 0;\n            for(;;) {\n              int delta = 0;\n              for(;;) {\n                byte b = arr[pos++];\n                delta = (delta << 7) | (b & 0x7f);\n                if ((b & 0x80) == 0) break;\n              }\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              //System.out.println(\"    tnum=\" + tnum);\n              counts[tnum]++;\n            }\n          } else {\n            //System.out.println(\"  inlined\");\n            int tnum = 0;\n            int delta = 0;\n            for (;;) {\n              delta = (delta << 7) | (code & 0x7f);\n              if ((code & 0x80)==0) {\n                if (delta==0) break;\n                tnum += delta - TNUM_OFFSET;\n                //System.out.println(\"    tnum=\" + tnum);\n                counts[tnum]++;\n                delta = 0;\n              }\n              code >>>= 8;\n            }\n          }\n        }\n      }\n      final CharsRefBuilder charsRef = new CharsRefBuilder();\n\n      int off=offset;\n      int lim=limit>=0 ? limit : Integer.MAX_VALUE;\n\n      if (sort.equals(FacetParams.FACET_SORT_COUNT) || sort.equals(FacetParams.FACET_SORT_COUNT_LEGACY)) {\n        int maxsize = limit>0 ? offset+limit : Integer.MAX_VALUE-1;\n        maxsize = Math.min(maxsize, numTermsInField);\n        LongPriorityQueue queue = new LongPriorityQueue(Math.min(maxsize,1000), maxsize, Long.MIN_VALUE);\n\n        int min=mincount-1;  // the smallest value in the top 'N' values\n        //System.out.println(\"START=\" + startTerm + \" END=\" + endTerm);\n        for (int i=startTerm; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c>min) {\n            // NOTE: we use c>min rather than c>=min as an optimization because we are going in\n            // index order, so we already know that the keys are ordered.  This can be very\n            // important if a lot of the counts are repeated (like zero counts would be).\n\n            // smaller term numbers sort higher, so subtract the term number instead\n            long pair = (((long)c)<<32) + (Integer.MAX_VALUE - i);\n            boolean displaced = queue.insert(pair);\n            if (displaced) min=(int)(queue.top() >>> 32);\n          }\n        }\n\n        // now select the right page from the results\n\n        // if we are deep paging, we don't have to order the highest \"offset\" counts.\n        int collectCount = Math.max(0, queue.size() - off);\n        assert collectCount <= lim;\n\n        // the start and end indexes of our list \"sorted\" (starting with the highest value)\n        int sortedIdxStart = queue.size() - (collectCount - 1);\n        int sortedIdxEnd = queue.size() + 1;\n        final long[] sorted = queue.sort(collectCount);\n\n        final int[] indirect = counts;  // reuse the counts array for the index into the tnums array\n        assert indirect.length >= sortedIdxEnd;\n\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          long pair = sorted[i];\n          int c = (int)(pair >>> 32);\n          int tnum = Integer.MAX_VALUE - (int)pair;\n\n          indirect[i] = i;   // store the index for indirect sorting\n          sorted[i] = tnum;  // reuse the \"sorted\" array to store the term numbers for indirect sorting\n\n          // add a null label for now... we'll fill it in later.\n          res.add(null, c);\n        }\n\n        // now sort the indexes by the term numbers\n        PrimUtils.sort(sortedIdxStart, sortedIdxEnd, indirect, new PrimUtils.IntComparator() {\n          @Override\n          public int compare(int a, int b) {\n            return (int)sorted[a] - (int)sorted[b];\n          }\n\n          @Override\n          public boolean lessThan(int a, int b) {\n            return sorted[a] < sorted[b];\n          }\n\n          @Override\n          public boolean equals(int a, int b) {\n            return sorted[a] == sorted[b];\n          }\n        });\n\n        // convert the term numbers to term values and set\n        // as the label\n        //System.out.println(\"sortStart=\" + sortedIdxStart + \" end=\" + sortedIdxEnd);\n        for (int i=sortedIdxStart; i<sortedIdxEnd; i++) {\n          int idx = indirect[i];\n          int tnum = (int)sorted[idx];\n          final String label = getReadableValue(getTermValue(te, tnum), ft, charsRef);\n          //System.out.println(\"  label=\" + label);\n          res.setName(idx - sortedIdxStart, label);\n        }\n\n      } else {\n        // add results in index order\n        int i=startTerm;\n        if (mincount<=0) {\n          // if mincount<=0, then we won't discard any terms and we know exactly\n          // where to start.\n          i=startTerm+off;\n          off=0;\n        }\n\n        for (; i<endTerm; i++) {\n          int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n          if (c<mincount || --off>=0) continue;\n          if (--lim<0) break;\n\n          final String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n          res.add(label, c);\n        }\n      }\n    }\n\n\n    if (missing) {\n      // TODO: a faster solution for this?\n      res.add(null, SimpleFacets.getFieldMissingCount(searcher, baseDocs, field));\n    }\n\n    //System.out.println(\"  res=\" + res);\n\n    return res;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":["c26f00b574427b55127e869b935845554afde1fa","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":["c750c9942b5f894af0f235b503bec55f2db8d1d3","19275ba31e621f6da1b83bf13af75233876fd3d4"],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":["5cab9a86bd67202d20b6adc463008c8e982b070a","c750c9942b5f894af0f235b503bec55f2db8d1d3"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["fd92b8bcc88e969302510acf77bd6970da3994c4"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["c26f00b574427b55127e869b935845554afde1fa","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["59fc0e55b44c555c39d950def9414b5596c6ebe2","96d207426bd26fa5c1014e26d21d87603aea68b7"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1","d12bbc45d641864ffe03291bc30f178eb34e434c"],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["c26f00b574427b55127e869b935845554afde1fa"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"d12bbc45d641864ffe03291bc30f178eb34e434c":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"c750c9942b5f894af0f235b503bec55f2db8d1d3":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["c750c9942b5f894af0f235b503bec55f2db8d1d3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d12bbc45d641864ffe03291bc30f178eb34e434c"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"78a55f24d9b493c2a1cecf79f1d78279062b545b":[],"716d18f3a9b0993bc679d7fa7abdc9bfb03411ec":[],"9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab":[],"c26f00b574427b55127e869b935845554afde1fa":["78a55f24d9b493c2a1cecf79f1d78279062b545b","fd92b8bcc88e969302510acf77bd6970da3994c4","59fc0e55b44c555c39d950def9414b5596c6ebe2"],"96d207426bd26fa5c1014e26d21d87603aea68b7":["5cab9a86bd67202d20b6adc463008c8e982b070a"],"fd92b8bcc88e969302510acf77bd6970da3994c4":["96d207426bd26fa5c1014e26d21d87603aea68b7"],"19275ba31e621f6da1b83bf13af75233876fd3d4":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","a69cf7f1b4cac5d5b1363402b565cd535f13e6a1"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"5cab9a86bd67202d20b6adc463008c8e982b070a":["9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","c750c9942b5f894af0f235b503bec55f2db8d1d3"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"8aa2bb13f56a3ad540fd2dc5e882e1ed4bf799d1":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","d12bbc45d641864ffe03291bc30f178eb34e434c"],"a69cf7f1b4cac5d5b1363402b565cd535f13e6a1":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"59fc0e55b44c555c39d950def9414b5596c6ebe2":["78a55f24d9b493c2a1cecf79f1d78279062b545b","fd92b8bcc88e969302510acf77bd6970da3994c4","5cab9a86bd67202d20b6adc463008c8e982b070a"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"c750c9942b5f894af0f235b503bec55f2db8d1d3":["716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","eee5f2a24465d2c9a5f86ab84b7c35041a30fda8"],"d12bbc45d641864ffe03291bc30f178eb34e434c":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"eee5f2a24465d2c9a5f86ab84b7c35041a30fda8":["19275ba31e621f6da1b83bf13af75233876fd3d4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["78a55f24d9b493c2a1cecf79f1d78279062b545b","716d18f3a9b0993bc679d7fa7abdc9bfb03411ec","9946ea6d8ddf0b8c20b2ca6a816b7168b023a6ab","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}