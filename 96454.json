{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d572389229127c297dd1fa5ce4758e1cec41e799","date":1273610938,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fb10b6bcde550b87d8f10e5f010bd8f3021023b6","date":1274974592,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexesNoOptimize(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexesNoOptimize(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexesNoOptimize will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexesNoOptimize should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexesNoOptimize(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexesNoOptimize(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexesNoOptimize(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b21422ff1d1d56499dec481f193b402e5e8def5b","date":1281472367,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c","date":1281477834,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new MockRAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockRAMDirectory startDir = new MockRAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockRAMDirectory startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new MockRAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockRAMDirectory startDir = new MockRAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a05409176bd65129d67a785ee70e881e238a9aef","date":1282582843,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockRAMDirectory startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3eaa8ca351aa7eefa5bd5bf249e9dd9bb0830c54","date":1282658201,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setTrackDiskUsage(true);\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setTrackDiskUsage(true);\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory(random);\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory(random);\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setTrackDiskUsage(true);\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84b590669deb3d3a471cec6cb13b104b2ee94418","date":1288889547,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":null,"sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setTrackDiskUsage(true);\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":4,"author":"Simon Willnauer","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":null,"sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = newDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      MockDirectoryWrapper startDir = newDirectory();\n      IndexWriter writer = new IndexWriter(startDir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockDirectoryWrapper dir = new MockDirectoryWrapper(new RAMDirectory(startDir));\n          writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setTrackDiskUsage(true);\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n      for (Directory dir : dirs)\n        dir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testAddIndexOnDiskFull().mjava","sourceNew":null,"sourceOld":"    /*\n      Test: make sure when we run out of disk space or hit\n      random IOExceptions in any of the addIndexes(*) calls\n      that 1) index is not corrupt (searcher can open/search\n      it) and 2) transactional semantics are followed:\n      either all or none of the incoming documents were in\n      fact added.\n    */\n    public void testAddIndexOnDiskFull() throws IOException\n    {\n      int START_COUNT = 57;\n      int NUM_DIR = 50;\n      int END_COUNT = START_COUNT + NUM_DIR*25;\n\n      // Build up a bunch of dirs that have indexes which we\n      // will then merge together by calling addIndexes(*):\n      Directory[] dirs = new Directory[NUM_DIR];\n      long inputDiskUsage = 0;\n      for(int i=0;i<NUM_DIR;i++) {\n        dirs[i] = new RAMDirectory();\n        IndexWriter writer  = new IndexWriter(dirs[i], new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n        for(int j=0;j<25;j++) {\n          addDocWithIndex(writer, 25*i+j);\n        }\n        writer.close();\n        String[] files = dirs[i].listAll();\n        for(int j=0;j<files.length;j++) {\n          inputDiskUsage += dirs[i].fileLength(files[j]);\n        }\n      }\n\n      // Now, build a starting index that has START_COUNT docs.  We\n      // will then try to addIndexesNoOptimize into a copy of this:\n      RAMDirectory startDir = new RAMDirectory();\n      IndexWriter writer = new IndexWriter(startDir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()));\n      for(int j=0;j<START_COUNT;j++) {\n        addDocWithIndex(writer, j);\n      }\n      writer.close();\n\n      // Make sure starting index seems to be working properly:\n      Term searchTerm = new Term(\"content\", \"aaa\");        \n      IndexReader reader = IndexReader.open(startDir, true);\n      assertEquals(\"first docFreq\", 57, reader.docFreq(searchTerm));\n\n      IndexSearcher searcher = new IndexSearcher(reader);\n      ScoreDoc[] hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;\n      assertEquals(\"first number of hits\", 57, hits.length);\n      searcher.close();\n      reader.close();\n\n      // Iterate with larger and larger amounts of free\n      // disk space.  With little free disk space,\n      // addIndexes will certainly run out of space &\n      // fail.  Verify that when this happens, index is\n      // not corrupt and index in fact has added no\n      // documents.  Then, we increase disk space by 2000\n      // bytes each iteration.  At some point there is\n      // enough free disk space and addIndexes should\n      // succeed and index should show all documents were\n      // added.\n\n      // String[] files = startDir.listAll();\n      long diskUsage = startDir.sizeInBytes();\n\n      long startDiskUsage = 0;\n      String[] files = startDir.listAll();\n      for(int i=0;i<files.length;i++) {\n        startDiskUsage += startDir.fileLength(files[i]);\n      }\n\n      for(int iter=0;iter<3;iter++) {\n\n        if (VERBOSE)\n          System.out.println(\"TEST: iter=\" + iter);\n\n        // Start with 100 bytes more than we are currently using:\n        long diskFree = diskUsage+100;\n\n        int method = iter;\n\n        boolean success = false;\n        boolean done = false;\n\n        String methodName;\n        if (0 == method) {\n          methodName = \"addIndexes(Directory[]) + optimize()\";\n        } else if (1 == method) {\n          methodName = \"addIndexes(IndexReader[])\";\n        } else {\n          methodName = \"addIndexes(Directory[])\";\n        }\n\n        while(!done) {\n\n          // Make a new dir that will enforce disk usage:\n          MockRAMDirectory dir = new MockRAMDirectory(startDir);\n          writer = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setOpenMode(OpenMode.APPEND));\n          IOException err = null;\n\n          MergeScheduler ms = writer.getConfig().getMergeScheduler();\n          for(int x=0;x<2;x++) {\n            if (ms instanceof ConcurrentMergeScheduler)\n              // This test intentionally produces exceptions\n              // in the threads that CMS launches; we don't\n              // want to pollute test output with these.\n              if (0 == x)\n                ((ConcurrentMergeScheduler) ms).setSuppressExceptions();\n              else\n                ((ConcurrentMergeScheduler) ms).clearSuppressExceptions();\n\n            // Two loops: first time, limit disk space &\n            // throw random IOExceptions; second time, no\n            // disk space limit:\n\n            double rate = 0.05;\n            double diskRatio = ((double) diskFree)/diskUsage;\n            long thisDiskFree;\n\n            String testName = null;\n\n            if (0 == x) {\n              thisDiskFree = diskFree;\n              if (diskRatio >= 2.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 4.0) {\n                rate /= 2;\n              }\n              if (diskRatio >= 6.0) {\n                rate = 0.0;\n              }\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with disk full at \" + diskFree + \" bytes\";\n            } else {\n              thisDiskFree = 0;\n              rate = 0.0;\n              if (VERBOSE)\n                testName = \"disk full test \" + methodName + \" with unlimited disk space\";\n            }\n\n            if (VERBOSE)\n              System.out.println(\"\\ncycle: \" + testName);\n\n            dir.setMaxSizeInBytes(thisDiskFree);\n            dir.setRandomIOExceptionRate(rate, diskFree);\n\n            try {\n\n              if (0 == method) {\n                writer.addIndexes(dirs);\n                writer.optimize();\n              } else if (1 == method) {\n                IndexReader readers[] = new IndexReader[dirs.length];\n                for(int i=0;i<dirs.length;i++) {\n                  readers[i] = IndexReader.open(dirs[i], true);\n                }\n                try {\n                  writer.addIndexes(readers);\n                } finally {\n                  for(int i=0;i<dirs.length;i++) {\n                    readers[i].close();\n                  }\n                }\n              } else {\n                writer.addIndexes(dirs);\n              }\n\n              success = true;\n              if (VERBOSE) {\n                System.out.println(\"  success!\");\n              }\n\n              if (0 == x) {\n                done = true;\n              }\n\n            } catch (IOException e) {\n              success = false;\n              err = e;\n              if (VERBOSE) {\n                System.out.println(\"  hit IOException: \" + e);\n                e.printStackTrace(System.out);\n              }\n\n              if (1 == x) {\n                e.printStackTrace(System.out);\n                fail(methodName + \" hit IOException after disk space was freed up\");\n              }\n            }\n\n            // Make sure all threads from\n            // ConcurrentMergeScheduler are done\n            _TestUtil.syncConcurrentMerges(writer);\n\n            if (VERBOSE) {\n              System.out.println(\"  now test readers\");\n            }\n\n            // Finally, verify index is not corrupt, and, if\n            // we succeeded, we see all docs added, and if we\n            // failed, we see either all docs or no docs added\n            // (transactional semantics):\n            try {\n              reader = IndexReader.open(dir, true);\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when creating IndexReader: \" + e);\n            }\n            int result = reader.docFreq(searchTerm);\n            if (success) {\n              if (result != START_COUNT) {\n                fail(testName + \": method did not throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result != START_COUNT && result != END_COUNT) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but docFreq('aaa') is \" + result + \" instead of expected \" + START_COUNT + \" or \" + END_COUNT);\n              }\n            }\n\n            searcher = new IndexSearcher(reader);\n            try {\n              hits = searcher.search(new TermQuery(searchTerm), null, END_COUNT).scoreDocs;\n            } catch (IOException e) {\n              e.printStackTrace(System.out);\n              fail(testName + \": exception when searching: \" + e);\n            }\n            int result2 = hits.length;\n            if (success) {\n              if (result2 != result) {\n                fail(testName + \": method did not throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            } else {\n              // On hitting exception we still may have added\n              // all docs:\n              if (result2 != result) {\n                err.printStackTrace(System.out);\n                fail(testName + \": method did throw exception but hits.length for search on term 'aaa' is \" + result2 + \" instead of expected \" + result);\n              }\n            }\n\n            searcher.close();\n            reader.close();\n            if (VERBOSE) {\n              System.out.println(\"  count is \" + result);\n            }\n\n            if (done || result == END_COUNT) {\n              break;\n            }\n          }\n\n          if (VERBOSE) {\n            System.out.println(\"  start disk = \" + startDiskUsage + \"; input disk = \" + inputDiskUsage + \"; max used = \" + dir.getMaxUsedSizeInBytes());\n          }\n\n          if (done) {\n            // Javadocs state that temp free Directory space\n            // required is at most 2X total input size of\n            // indices so let's make sure:\n            assertTrue(\"max free Directory space required exceeded 1X the total input index sizes during \" + methodName +\n                       \": max temp usage = \" + (dir.getMaxUsedSizeInBytes()-startDiskUsage) + \" bytes; \" +\n                       \"starting disk usage = \" + startDiskUsage + \" bytes; \" +\n                       \"input index disk usage = \" + inputDiskUsage + \" bytes\",\n                       (dir.getMaxUsedSizeInBytes()-startDiskUsage) < 2*(startDiskUsage + inputDiskUsage));\n          }\n\n          // Make sure we don't hit disk full during close below:\n          dir.setMaxSizeInBytes(0);\n          dir.setRandomIOExceptionRate(0.0, 0);\n\n          writer.close();\n\n          // Wait for all BG threads to finish else\n          // dir.close() will throw IOException because\n          // there are still open files\n          _TestUtil.syncConcurrentMerges(ms);\n\n          dir.close();\n\n          // Try again with 5000 more bytes of free space:\n          diskFree += 5000;\n        }\n      }\n\n      startDir.close();\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"85a883878c0af761245ab048babc63d099f835f3":["1f653cfcf159baeaafe5d01682a911e95bba4012","84b590669deb3d3a471cec6cb13b104b2ee94418"],"d572389229127c297dd1fa5ce4758e1cec41e799":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6","84b590669deb3d3a471cec6cb13b104b2ee94418"],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["d572389229127c297dd1fa5ce4758e1cec41e799"],"84b590669deb3d3a471cec6cb13b104b2ee94418":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["3eaa8ca351aa7eefa5bd5bf249e9dd9bb0830c54"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["b21422ff1d1d56499dec481f193b402e5e8def5b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a05409176bd65129d67a785ee70e881e238a9aef":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"3eaa8ca351aa7eefa5bd5bf249e9dd9bb0830c54":["a05409176bd65129d67a785ee70e881e238a9aef"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["84b590669deb3d3a471cec6cb13b104b2ee94418"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["a05409176bd65129d67a785ee70e881e238a9aef"],"85a883878c0af761245ab048babc63d099f835f3":[],"d572389229127c297dd1fa5ce4758e1cec41e799":["fb10b6bcde550b87d8f10e5f010bd8f3021023b6"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"fb10b6bcde550b87d8f10e5f010bd8f3021023b6":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b21422ff1d1d56499dec481f193b402e5e8def5b"],"84b590669deb3d3a471cec6cb13b104b2ee94418":["85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["85a883878c0af761245ab048babc63d099f835f3","84b590669deb3d3a471cec6cb13b104b2ee94418"],"1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a05409176bd65129d67a785ee70e881e238a9aef":["3eaa8ca351aa7eefa5bd5bf249e9dd9bb0830c54"],"3eaa8ca351aa7eefa5bd5bf249e9dd9bb0830c54":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"b21422ff1d1d56499dec481f193b402e5e8def5b":["1a2e46fd1b7cbc52d7d6461a6ef99e7107ae2a9c"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d572389229127c297dd1fa5ce4758e1cec41e799"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}