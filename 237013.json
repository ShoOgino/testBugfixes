{"path":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","commits":[{"id":"fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7","date":1282945751,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"/dev/null","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = new Field(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = new Field(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 1000*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a78a90fc9701e511308346ea29f4f5e548bb39fe","9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4","1e538cbd7f572f646f36c73053aea561d95c2cb2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4","date":1283166150,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = new Field(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = new Field(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = new Field(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = new Field(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 1000*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"132903c28af3aa6f67284b78de91c0f0a99488c2","date":1284282129,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = new Field(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = new Field(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"/dev/null","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer()).setMergePolicy(newInOrderLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e538cbd7f572f646f36c73053aea561d95c2cb2","date":1306660868,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = File.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f83af14a2a8131b14d7aee6274c740334e0363d3","date":1307579822,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = 100*RANDOM_MULTIPLIER;\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68b4c6c95719e3020e407649a20b68538649beeb","date":1308055822,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9ed208afa1e7aa98899ddb1dedfddedddf898253","date":1308079587,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory dir = new MMapDirectory(path);\n    dir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      dir.setUseUnmap(true);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    Field junk = newField(\"junk\", \"\", Field.Store.YES, Field.Index.NOT_ANALYZED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","pathOld":"lucene/src/test/org/apache/lucene/store/TestMultiMMap#assertChunking(Random,int).mjava","sourceNew":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  private void assertChunking(Random random, int chunkSize) throws Exception {\n    File path = _TestUtil.createTempFile(\"mmap\" + chunkSize, \"tmp\", workDir);\n    path.delete();\n    path.mkdirs();\n    MMapDirectory mmapDir = new MMapDirectory(path);\n    mmapDir.setMaxChunkSize(chunkSize);\n    // we will map a lot, try to turn on the unmap hack\n    if (MMapDirectory.UNMAP_SUPPORTED)\n      mmapDir.setUseUnmap(true);\n    MockDirectoryWrapper dir = new MockDirectoryWrapper(random, mmapDir);\n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMergePolicy(newLogMergePolicy()));\n    Document doc = new Document();\n    Field docid = newField(\"docid\", \"0\", StringField.TYPE_STORED);\n    Field junk = newField(\"junk\", \"\", StringField.TYPE_STORED);\n    doc.add(docid);\n    doc.add(junk);\n    \n    int numDocs = 100;\n    for (int i = 0; i < numDocs; i++) {\n      docid.setValue(\"\" + i);\n      junk.setValue(_TestUtil.randomUnicodeString(random));\n      writer.addDocument(doc);\n    }\n    IndexReader reader = writer.getReader();\n    writer.close();\n    \n    int numAsserts = atLeast(100);\n    for (int i = 0; i < numAsserts; i++) {\n      int docID = random.nextInt(numDocs);\n      assertEquals(\"\" + docID, reader.document(docID).get(\"docid\"));\n    }\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"f83af14a2a8131b14d7aee6274c740334e0363d3":["1e538cbd7f572f646f36c73053aea561d95c2cb2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1e538cbd7f572f646f36c73053aea561d95c2cb2":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","132903c28af3aa6f67284b78de91c0f0a99488c2"],"c19f985e36a65cc969e8e564fe337a0d41512075":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["c19f985e36a65cc969e8e564fe337a0d41512075"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","1e538cbd7f572f646f36c73053aea561d95c2cb2"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["132903c28af3aa6f67284b78de91c0f0a99488c2","c19f985e36a65cc969e8e564fe337a0d41512075"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["68b4c6c95719e3020e407649a20b68538649beeb"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","1e538cbd7f572f646f36c73053aea561d95c2cb2"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","f83af14a2a8131b14d7aee6274c740334e0363d3"],"9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4":["fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7"],"68b4c6c95719e3020e407649a20b68538649beeb":["f83af14a2a8131b14d7aee6274c740334e0363d3"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"9ed208afa1e7aa98899ddb1dedfddedddf898253":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","68b4c6c95719e3020e407649a20b68538649beeb"],"fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c19f985e36a65cc969e8e564fe337a0d41512075","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["2e10cb22a8bdb44339e282925a29182bb2f3174d","f83af14a2a8131b14d7aee6274c740334e0363d3"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"f83af14a2a8131b14d7aee6274c740334e0363d3":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","68b4c6c95719e3020e407649a20b68538649beeb","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1e538cbd7f572f646f36c73053aea561d95c2cb2":["f83af14a2a8131b14d7aee6274c740334e0363d3","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"132903c28af3aa6f67284b78de91c0f0a99488c2":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","29ef99d61cda9641b6250bf9567329a6e65f901d"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["1e538cbd7f572f646f36c73053aea561d95c2cb2","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","a3776dccca01c11e7046323cfad46a3b4a471233"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["9ed208afa1e7aa98899ddb1dedfddedddf898253"],"9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4":["132903c28af3aa6f67284b78de91c0f0a99488c2"],"68b4c6c95719e3020e407649a20b68538649beeb":["1509f151d7692d84fae414b2b799ac06ba60fcb4","9ed208afa1e7aa98899ddb1dedfddedddf898253"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"962d04139994fce5193143ef35615499a9a96d78":[],"9ed208afa1e7aa98899ddb1dedfddedddf898253":[],"fdefa116bcbcf81f5fad6c30040fcc2d2a79f7f7":["9d52699d66c4ed4d7d8dffd7b013aac8eb4cd3f4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["962d04139994fce5193143ef35615499a9a96d78","9ed208afa1e7aa98899ddb1dedfddedddf898253","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}