{"path":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","commits":[{"id":"000498895a9d8c442dd10d03121bd753ec00bc0e","date":1389468193,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","pathOld":"/dev/null","sourceNew":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","sourceNew":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","sourceNew":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.shutdown();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae230518a1a68acc124bef8df61ef94bd7c1295e","date":1417181719,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","sourceNew":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"));\n\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n\n    ts.reset();\n    for(Token token : tokens) {\n      assertTrue(ts.incrementToken());\n      assertEquals(token.toString(), termAtt.toString());\n      assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n      assertEquals(token.getPayload(), payloadAtt.getPayload());\n      assertEquals(token.startOffset(), offsetAtt.startOffset());\n      assertEquals(token.endOffset(), offsetAtt.endOffset());\n    }\n\n    assertFalse(ts.incrementToken());\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    for(int i=0;i<2;i++) {\n      // Do this twice, once passing true and then passing\n      // false: they are entirely different code paths\n      // under-the-hood:\n      TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"), i == 0);\n\n      CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n      PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n      OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n      PayloadAttribute payloadAtt = ts.getAttribute(PayloadAttribute.class);\n\n      for(Token token : tokens) {\n        assertTrue(ts.incrementToken());\n        assertEquals(token.toString(), termAtt.toString());\n        assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n        assertEquals(token.getPayload(), payloadAtt.getPayload());\n        assertEquals(token.startOffset(), offsetAtt.startOffset());\n        assertEquals(token.endOffset(), offsetAtt.endOffset());\n      }\n\n      assertFalse(ts.incrementToken());\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5d62e4938659e263e96ae8188e11aea8a940aea5","date":1430230314,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/highlight/TokenSourcesTest#testPayloads().mjava","sourceNew":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream ts = TokenSources.getTermVectorTokenStreamOrNull(\"field\", reader.getTermVectors(0), -1);\n\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n\n    ts.reset();\n    for(Token token : tokens) {\n      assertTrue(ts.incrementToken());\n      assertEquals(token.toString(), termAtt.toString());\n      assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n      assertEquals(token.getPayload(), payloadAtt.getPayload());\n      assertEquals(token.startOffset(), offsetAtt.startOffset());\n      assertEquals(token.endOffset(), offsetAtt.endOffset());\n    }\n\n    assertFalse(ts.incrementToken());\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  // LUCENE-5294\n  public void testPayloads() throws Exception {\n    Directory dir = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir);\n    FieldType myFieldType = new FieldType(TextField.TYPE_NOT_STORED);\n    myFieldType.setStoreTermVectors(true);\n    myFieldType.setStoreTermVectorOffsets(true);\n    myFieldType.setStoreTermVectorPositions(true);\n    myFieldType.setStoreTermVectorPayloads(true);\n\n    curOffset = 0;\n\n    Token[] tokens = new Token[] {\n      getToken(\"foxes\"),\n      getToken(\"can\"),\n      getToken(\"jump\"),\n      getToken(\"high\")\n    };\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", new CannedTokenStream(tokens), myFieldType));\n    writer.addDocument(doc);\n  \n    IndexReader reader = writer.getReader();\n    writer.close();\n    assertEquals(1, reader.numDocs());\n\n    TokenStream ts = TokenSources.getTokenStream(reader.getTermVectors(0).terms(\"field\"));\n\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    PositionIncrementAttribute posIncAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    OffsetAttribute offsetAtt = ts.getAttribute(OffsetAttribute.class);\n    PayloadAttribute payloadAtt = ts.addAttribute(PayloadAttribute.class);\n\n    ts.reset();\n    for(Token token : tokens) {\n      assertTrue(ts.incrementToken());\n      assertEquals(token.toString(), termAtt.toString());\n      assertEquals(token.getPositionIncrement(), posIncAtt.getPositionIncrement());\n      assertEquals(token.getPayload(), payloadAtt.getPayload());\n      assertEquals(token.startOffset(), offsetAtt.startOffset());\n      assertEquals(token.endOffset(), offsetAtt.endOffset());\n    }\n\n    assertFalse(ts.incrementToken());\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5d62e4938659e263e96ae8188e11aea8a940aea5"]},"commit2Childs":{"ae230518a1a68acc124bef8df61ef94bd7c1295e":["5d62e4938659e263e96ae8188e11aea8a940aea5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["000498895a9d8c442dd10d03121bd753ec00bc0e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["ae230518a1a68acc124bef8df61ef94bd7c1295e"],"5d62e4938659e263e96ae8188e11aea8a940aea5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"000498895a9d8c442dd10d03121bd753ec00bc0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}