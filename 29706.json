{"path":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","commits":[{"id":"e741dd53eb8ed0c960425039fecf15b4bd8f77c9","date":1292444666,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab5cb6a74aefb78aa0569857970b9151dfe2e787","date":1292842407,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"/dev/null","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","pathOld":"solr/src/test/org/apache/solr/analysis/TestUAX29URLEmailTokenizerFactory#testMaxTokenLength().mjava","sourceNew":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","sourceOld":"  public void testMaxTokenLength() throws Exception {\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0 ; i < 100 ; ++i) {\n      builder.append(\"abcdefg\"); // 7 * 100 = 700 char \"word\"\n    }\n    String longWord = builder.toString();\n    String content = \"one two three \" + longWord + \" four five six\";\n    Reader reader = new StringReader(content);\n    Map<String,String> args = new HashMap<String,String>();\n    args.put(\"luceneMatchVersion\", DEFAULT_VERSION_PARAM.get(\"luceneMatchVersion\"));\n    args.put(\"maxTokenLength\", \"1000\");\n    UAX29URLEmailTokenizerFactory factory = new UAX29URLEmailTokenizerFactory();\n    factory.init(args);\n    Tokenizer stream = factory.create(reader);\n    assertTokenStreamContents(stream, \n        new String[] {\"one\", \"two\", \"three\", longWord, \"four\", \"five\", \"six\" });\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["e741dd53eb8ed0c960425039fecf15b4bd8f77c9"],"c26f00b574427b55127e869b935845554afde1fa":["e741dd53eb8ed0c960425039fecf15b4bd8f77c9","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e741dd53eb8ed0c960425039fecf15b4bd8f77c9"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["e741dd53eb8ed0c960425039fecf15b4bd8f77c9"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e741dd53eb8ed0c960425039fecf15b4bd8f77c9"],"e741dd53eb8ed0c960425039fecf15b4bd8f77c9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","e741dd53eb8ed0c960425039fecf15b4bd8f77c9"],"ab5cb6a74aefb78aa0569857970b9151dfe2e787":[],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"e741dd53eb8ed0c960425039fecf15b4bd8f77c9":["c903c3d15906a3da96b8c0c2fb704491005fdbdb","c26f00b574427b55127e869b935845554afde1fa","ab5cb6a74aefb78aa0569857970b9151dfe2e787","a258fbb26824fd104ed795e5d9033d2d040049ee","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ab5cb6a74aefb78aa0569857970b9151dfe2e787","a258fbb26824fd104ed795e5d9033d2d040049ee","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}