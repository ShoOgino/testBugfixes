{"path":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","commits":[{"id":"f42902644d0afd11c1be38fd0b0e8237bf4f4d7e","date":1408694655,"type":0,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = atomicReader.terms(textFieldName).iterator(null);\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(atomicReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator(null);\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(atomicReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = atomicReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator(null);\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator(null);\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(leafReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = atomicReader.terms(textFieldName).iterator(null);\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(atomicReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator(null);\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(atomicReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = atomicReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","date":1428522487,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(leafReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator(null);\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator(null);\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(leafReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"444d4b906d0e3398f87d6a5c4967c508f11a7f0b","date":1466507434,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiFields.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(leafReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiFields.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = leafReader.terms(textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(leafReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(leafReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = leafReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiTerms.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiTerms.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiTerms.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiFields.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiFields.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiFields.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33bfee30277584028170135002def66f9d57732b","date":1547842233,"type":3,"author":"Tommaso Teofili","isMerge":false,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiTerms.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiTerms.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiTerms.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiTerms.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiTerms.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiTerms.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"52ec154a31cf63bc47d2cc0b49e171a4e75aa99d","date":1548322018,"type":3,"author":"Tommaso Teofili","isMerge":true,"pathNew":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","pathOld":"lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier#reInitCache(int,boolean).mjava","sourceNew":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiTerms.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiTerms.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiTerms.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","sourceOld":"  /**\n   * This function is building the frame of the cache. The cache is storing the\n   * word occurrences to the memory after those searched once. This cache can\n   * made 2-100x speedup in proper use, but can eat lot of memory. There is an\n   * option to lower the memory consume, if a word have really low occurrence in\n   * the index you could filter it out. The other parameter is switching between\n   * the term searching, if it true, just the terms in the skeleton will be\n   * searched, but if it false the terms whoes not in the cache will be searched\n   * out too (but not cached).\n   *\n   * @param minTermOccurrenceInCache Lower cache size with higher value.\n   * @param justCachedTerms          The switch for fully exclude low occurrence docs.\n   * @throws IOException If there is a low-level I/O error.\n   */\n  public void reInitCache(int minTermOccurrenceInCache, boolean justCachedTerms) throws IOException {\n    this.justCachedTerms = justCachedTerms;\n\n    this.docsWithClassSize = countDocsWithClass();\n    termCClassHitCache.clear();\n    cclasses.clear();\n    classTermFreq.clear();\n\n    // build the cache for the word\n    Map<String, Long> frequencyMap = new HashMap<>();\n    for (String textFieldName : textFieldNames) {\n      TermsEnum termsEnum = MultiTerms.getTerms(indexReader, textFieldName).iterator();\n      while (termsEnum.next() != null) {\n        BytesRef term = termsEnum.term();\n        String termText = term.utf8ToString();\n        long frequency = termsEnum.docFreq();\n        Long lastfreq = frequencyMap.get(termText);\n        if (lastfreq != null) frequency += lastfreq;\n        frequencyMap.put(termText, frequency);\n      }\n    }\n    for (Map.Entry<String, Long> entry : frequencyMap.entrySet()) {\n      if (entry.getValue() > minTermOccurrenceInCache) {\n        termCClassHitCache.put(entry.getKey(), new ConcurrentHashMap<BytesRef, Integer>());\n      }\n    }\n\n    // fill the class list\n    Terms terms = MultiTerms.getTerms(indexReader, classFieldName);\n    TermsEnum termsEnum = terms.iterator();\n    while ((termsEnum.next()) != null) {\n      cclasses.add(BytesRef.deepCopyOf(termsEnum.term()));\n    }\n    // fill the classTermFreq map\n    for (BytesRef cclass : cclasses) {\n      double avgNumberOfUniqueTerms = 0;\n      for (String textFieldName : textFieldNames) {\n        terms = MultiTerms.getTerms(indexReader, textFieldName);\n        long numPostings = terms.getSumDocFreq(); // number of term/doc pairs\n        avgNumberOfUniqueTerms += numPostings / (double) terms.getDocCount();\n      }\n      int docsWithC = indexReader.docFreq(new Term(classFieldName, cclass));\n      classTermFreq.put(cclass, avgNumberOfUniqueTerms * docsWithC);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"f42902644d0afd11c1be38fd0b0e8237bf4f4d7e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"52ec154a31cf63bc47d2cc0b49e171a4e75aa99d":["04e775de416dd2d8067b10db1c8af975a1d5017e","33bfee30277584028170135002def66f9d57732b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"04e775de416dd2d8067b10db1c8af975a1d5017e":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"33bfee30277584028170135002def66f9d57732b":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["f42902644d0afd11c1be38fd0b0e8237bf4f4d7e"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82","444d4b906d0e3398f87d6a5c4967c508f11a7f0b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["52ec154a31cf63bc47d2cc0b49e171a4e75aa99d"]},"commit2Childs":{"444d4b906d0e3398f87d6a5c4967c508f11a7f0b":["04e775de416dd2d8067b10db1c8af975a1d5017e","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"0a773283ef5eab2e9c7136eeb66574a4b7a2dc82":["444d4b906d0e3398f87d6a5c4967c508f11a7f0b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"f42902644d0afd11c1be38fd0b0e8237bf4f4d7e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"52ec154a31cf63bc47d2cc0b49e171a4e75aa99d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["f42902644d0afd11c1be38fd0b0e8237bf4f4d7e"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["52ec154a31cf63bc47d2cc0b49e171a4e75aa99d","33bfee30277584028170135002def66f9d57732b"],"33bfee30277584028170135002def66f9d57732b":["52ec154a31cf63bc47d2cc0b49e171a4e75aa99d"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["0a773283ef5eab2e9c7136eeb66574a4b7a2dc82"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}