{"path":"lucene/backwards/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"backwards/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":null,"sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map test4Map = new HashMap();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), true, \n                                           IndexWriter.MaxFieldLength.LIMITED);\n      assertTrue(writer != null);\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          float tf = sim.tf(freq);\n          float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      float score = hits[0].score;\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = (Integer)test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (Iterator iterator = vectorEntrySet.iterator(); iterator.hasNext();) {\n         TermVectorEntry tve = (TermVectorEntry) iterator.next();\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq = (Integer) test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = (SortedSet) map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}