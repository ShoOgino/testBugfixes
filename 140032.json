{"path":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#trimFields().mjava","commits":[{"id":"5a0af3a442be522899177e5e11384a45a6784a3f","date":1205348952,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#trimFields().mjava","pathOld":"/dev/null","sourceNew":"  /** If there are fields we've seen but did not see again\n   *  in the last run, then free them up.  Also reduce\n   *  postings hash size. */\n  void trimFields() {\n\n    int upto = 0;\n    for(int i=0;i<numAllFieldData;i++) {\n      DocumentsWriterFieldData fp = allFieldDataArray[i];\n      if (fp.lastGen == -1) {\n        // This field was not seen since the previous\n        // flush, so, free up its resources now\n\n        // Unhash\n        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n        DocumentsWriterFieldData last = null;\n        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];\n        while(fp0 != fp) {\n          last = fp0;\n          fp0 = fp0.next;\n        }\n        assert fp0 != null;\n\n        if (last == null)\n          fieldDataHash[hashPos] = fp.next;\n        else\n          last.next = fp.next;\n\n        if (docWriter.infoStream != null)\n          docWriter.infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n      } else {\n        // Reset\n        fp.lastGen = -1;\n        allFieldDataArray[upto++] = fp;\n          \n        if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n          int hashSize = fp.postingsHashSize;\n\n          // Reduce hash so it's between 25-50% full\n          while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n            hashSize >>= 1;\n          hashSize <<= 1;\n\n          if (hashSize != fp.postingsHash.length)\n            fp.rehashPostings(hashSize);\n        }\n      }\n    }\n\n    // If we didn't see any norms for this field since\n    // last flush, free it\n    for(int i=0;i<docWriter.norms.length;i++) {\n      BufferedNorms n = docWriter.norms[i];\n      if (n != null && n.upto == 0)\n        docWriter.norms[i] = null;\n    }\n\n    numAllFieldData = upto;\n\n    // Also pare back PostingsVectors if it's excessively\n    // large\n    if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n      final int newSize;\n      if (0 == maxPostingsVectors)\n        newSize = 1;\n      else\n        newSize = (int) (1.5*maxPostingsVectors);\n      PostingVector[] newArray = new PostingVector[newSize];\n      System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n      postingsVectors = newArray;\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c8dbef2a63b627af8940769d3c019224bda63b9f","date":1206695177,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#trimFields().mjava","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#trimFields().mjava","sourceNew":"  /** If there are fields we've seen but did not see again\n   *  in the last run, then free them up.  Also reduce\n   *  postings hash size. */\n  void trimFields() {\n\n    int upto = 0;\n    for(int i=0;i<numAllFieldData;i++) {\n      DocumentsWriterFieldData fp = allFieldDataArray[i];\n      if (fp.lastGen == -1) {\n        // This field was not seen since the previous\n        // flush, so, free up its resources now\n\n        // Unhash\n        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n        DocumentsWriterFieldData last = null;\n        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];\n        while(fp0 != fp) {\n          last = fp0;\n          fp0 = fp0.next;\n        }\n\n        if (last == null)\n          fieldDataHash[hashPos] = fp.next;\n        else\n          last.next = fp.next;\n\n        if (docWriter.infoStream != null)\n          docWriter.infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n      } else {\n        // Reset\n        fp.lastGen = -1;\n        allFieldDataArray[upto++] = fp;\n          \n        if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n          int hashSize = fp.postingsHashSize;\n\n          // Reduce hash so it's between 25-50% full\n          while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n            hashSize >>= 1;\n          hashSize <<= 1;\n\n          if (hashSize != fp.postingsHash.length)\n            fp.rehashPostings(hashSize);\n        }\n      }\n    }\n\n    // If we didn't see any norms for this field since\n    // last flush, free it\n    for(int i=0;i<docWriter.norms.length;i++) {\n      BufferedNorms n = docWriter.norms[i];\n      if (n != null && n.upto == 0)\n        docWriter.norms[i] = null;\n    }\n\n    numAllFieldData = upto;\n\n    // Also pare back PostingsVectors if it's excessively\n    // large\n    if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n      final int newSize;\n      if (0 == maxPostingsVectors)\n        newSize = 1;\n      else\n        newSize = (int) (1.5*maxPostingsVectors);\n      PostingVector[] newArray = new PostingVector[newSize];\n      System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n      postingsVectors = newArray;\n    }\n  }\n\n","sourceOld":"  /** If there are fields we've seen but did not see again\n   *  in the last run, then free them up.  Also reduce\n   *  postings hash size. */\n  void trimFields() {\n\n    int upto = 0;\n    for(int i=0;i<numAllFieldData;i++) {\n      DocumentsWriterFieldData fp = allFieldDataArray[i];\n      if (fp.lastGen == -1) {\n        // This field was not seen since the previous\n        // flush, so, free up its resources now\n\n        // Unhash\n        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n        DocumentsWriterFieldData last = null;\n        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];\n        while(fp0 != fp) {\n          last = fp0;\n          fp0 = fp0.next;\n        }\n        assert fp0 != null;\n\n        if (last == null)\n          fieldDataHash[hashPos] = fp.next;\n        else\n          last.next = fp.next;\n\n        if (docWriter.infoStream != null)\n          docWriter.infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n      } else {\n        // Reset\n        fp.lastGen = -1;\n        allFieldDataArray[upto++] = fp;\n          \n        if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n          int hashSize = fp.postingsHashSize;\n\n          // Reduce hash so it's between 25-50% full\n          while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n            hashSize >>= 1;\n          hashSize <<= 1;\n\n          if (hashSize != fp.postingsHash.length)\n            fp.rehashPostings(hashSize);\n        }\n      }\n    }\n\n    // If we didn't see any norms for this field since\n    // last flush, free it\n    for(int i=0;i<docWriter.norms.length;i++) {\n      BufferedNorms n = docWriter.norms[i];\n      if (n != null && n.upto == 0)\n        docWriter.norms[i] = null;\n    }\n\n    numAllFieldData = upto;\n\n    // Also pare back PostingsVectors if it's excessively\n    // large\n    if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n      final int newSize;\n      if (0 == maxPostingsVectors)\n        newSize = 1;\n      else\n        newSize = (int) (1.5*maxPostingsVectors);\n      PostingVector[] newArray = new PostingVector[newSize];\n      System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n      postingsVectors = newArray;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":4,"author":"Michael McCandless","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/DocumentsWriterThreadState#trimFields().mjava","sourceNew":null,"sourceOld":"  /** If there are fields we've seen but did not see again\n   *  in the last run, then free them up.  Also reduce\n   *  postings hash size. */\n  void trimFields() {\n\n    int upto = 0;\n    for(int i=0;i<numAllFieldData;i++) {\n      DocumentsWriterFieldData fp = allFieldDataArray[i];\n      if (fp.lastGen == -1) {\n        // This field was not seen since the previous\n        // flush, so, free up its resources now\n\n        // Unhash\n        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;\n        DocumentsWriterFieldData last = null;\n        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];\n        while(fp0 != fp) {\n          last = fp0;\n          fp0 = fp0.next;\n        }\n\n        if (last == null)\n          fieldDataHash[hashPos] = fp.next;\n        else\n          last.next = fp.next;\n\n        if (docWriter.infoStream != null)\n          docWriter.infoStream.println(\"  remove field=\" + fp.fieldInfo.name);\n\n      } else {\n        // Reset\n        fp.lastGen = -1;\n        allFieldDataArray[upto++] = fp;\n          \n        if (fp.numPostings > 0 && ((float) fp.numPostings) / fp.postingsHashSize < 0.2) {\n          int hashSize = fp.postingsHashSize;\n\n          // Reduce hash so it's between 25-50% full\n          while (fp.numPostings < (hashSize>>1) && hashSize >= 2)\n            hashSize >>= 1;\n          hashSize <<= 1;\n\n          if (hashSize != fp.postingsHash.length)\n            fp.rehashPostings(hashSize);\n        }\n      }\n    }\n\n    // If we didn't see any norms for this field since\n    // last flush, free it\n    for(int i=0;i<docWriter.norms.length;i++) {\n      BufferedNorms n = docWriter.norms[i];\n      if (n != null && n.upto == 0)\n        docWriter.norms[i] = null;\n    }\n\n    numAllFieldData = upto;\n\n    // Also pare back PostingsVectors if it's excessively\n    // large\n    if (maxPostingsVectors * 1.5 < postingsVectors.length) {\n      final int newSize;\n      if (0 == maxPostingsVectors)\n        newSize = 1;\n      else\n        newSize = (int) (1.5*maxPostingsVectors);\n      PostingVector[] newArray = new PostingVector[newSize];\n      System.arraycopy(postingsVectors, 0, newArray, 0, newSize);\n      postingsVectors = newArray;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c8dbef2a63b627af8940769d3c019224bda63b9f":["5a0af3a442be522899177e5e11384a45a6784a3f"],"5a0af3a442be522899177e5e11384a45a6784a3f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"5350389bf83287111f7760b9e3db3af8e3648474":["c8dbef2a63b627af8940769d3c019224bda63b9f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["5350389bf83287111f7760b9e3db3af8e3648474"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5a0af3a442be522899177e5e11384a45a6784a3f"],"c8dbef2a63b627af8940769d3c019224bda63b9f":["5350389bf83287111f7760b9e3db3af8e3648474"],"5a0af3a442be522899177e5e11384a45a6784a3f":["c8dbef2a63b627af8940769d3c019224bda63b9f"],"5350389bf83287111f7760b9e3db3af8e3648474":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}