{"path":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","commits":[{"id":"22859cb40e09867e7da8de84a31956c07259f82f","date":1441822065,"type":0,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection);\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29252e837df815b8d01fd6dff973126cced351c5","date":1521709907,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","sourceNew":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.CORE_NODE_NAME_PROP, replicaCoreNodeName,\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.FORCE_SET_STATE_PROP, \"false\");\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","sourceOld":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection);\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","date":1521731438,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","sourceNew":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.CORE_NODE_NAME_PROP, replicaCoreNodeName,\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.FORCE_SET_STATE_PROP, \"false\");\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","sourceOld":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection);\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180","date":1539076849,"type":4,"author":"Cao Manh Dat","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/LeaderInitiatedRecoveryThread#publishDownState(String,String,String,String,boolean).mjava","sourceNew":null,"sourceOld":"  public boolean publishDownState(String replicaCoreName, String replicaCoreNodeName, String replicaNodeName, String replicaUrl, boolean forcePublishState) {\n    boolean sendRecoveryCommand = true;\n    boolean publishDownState = false;\n\n    if (zkController.getZkStateReader().getClusterState().liveNodesContain(replicaNodeName)) {\n      try {\n        // create a znode that requires the replica needs to \"ack\" to verify it knows it was out-of-sync\n        updateLIRState(replicaCoreNodeName);\n\n        log.info(\"Put replica core={} coreNodeName={} on \" +\n            replicaNodeName + \" into leader-initiated recovery.\", replicaCoreName, replicaCoreNodeName);\n        publishDownState = true;\n      } catch (Exception e) {\n        Throwable setLirZnodeFailedCause = SolrException.getRootCause(e);\n        log.error(\"Leader failed to set replica \" +\n            nodeProps.getCoreUrl() + \" state to DOWN due to: \" + setLirZnodeFailedCause, setLirZnodeFailedCause);\n        if (setLirZnodeFailedCause instanceof KeeperException.SessionExpiredException\n            || setLirZnodeFailedCause instanceof KeeperException.ConnectionLossException\n            || setLirZnodeFailedCause instanceof ZkController.NotLeaderException) {\n          // our session is expired, which means our state is suspect, so don't go\n          // putting other replicas in recovery (see SOLR-6511)\n          sendRecoveryCommand = false;\n          forcePublishState = false; // no need to force publish any state in this case\n        } // else will go ahead and try to send the recovery command once after this error\n      }\n    } else  {\n      log.info(\"Node \" + replicaNodeName +\n              \" is not live, so skipping leader-initiated recovery for replica: core={} coreNodeName={}\",\n          replicaCoreName, replicaCoreNodeName);\n      // publishDownState will be false to avoid publishing the \"down\" state too many times\n      // as many errors can occur together and will each call into this method (SOLR-6189)\n      forcePublishState = false; // no need to force publish the state because replica is not live\n      sendRecoveryCommand = false; // no need to send recovery messages as well\n    }\n\n    try {\n      if (publishDownState || forcePublishState) {\n        ZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, \"state\",\n            ZkStateReader.STATE_PROP, Replica.State.DOWN.toString(),\n            ZkStateReader.BASE_URL_PROP, nodeProps.getBaseUrl(),\n            ZkStateReader.CORE_NAME_PROP, nodeProps.getCoreName(),\n            ZkStateReader.CORE_NODE_NAME_PROP, replicaCoreNodeName,\n            ZkStateReader.NODE_NAME_PROP, nodeProps.getNodeName(),\n            ZkStateReader.SHARD_ID_PROP, shardId,\n            ZkStateReader.COLLECTION_PROP, collection,\n            ZkStateReader.FORCE_SET_STATE_PROP, \"false\");\n        log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\n            replicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n        zkController.getOverseerJobQueue().offer(Utils.toJSON(m));\n      }\n    } catch (Exception e) {\n      log.error(\"Could not publish 'down' state for replicaUrl: {}\", replicaUrl, e);\n    }\n\n    return sendRecoveryCommand;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["22859cb40e09867e7da8de84a31956c07259f82f","29252e837df815b8d01fd6dff973126cced351c5"],"22859cb40e09867e7da8de84a31956c07259f82f":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"29252e837df815b8d01fd6dff973126cced351c5":["22859cb40e09867e7da8de84a31956c07259f82f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"]},"commit2Childs":{"6815b5b5d6334b2245dd7be2f8b6cca949bf7f43":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"22859cb40e09867e7da8de84a31956c07259f82f":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43","29252e837df815b8d01fd6dff973126cced351c5"],"29252e837df815b8d01fd6dff973126cced351c5":["6815b5b5d6334b2245dd7be2f8b6cca949bf7f43"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["22859cb40e09867e7da8de84a31956c07259f82f"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}