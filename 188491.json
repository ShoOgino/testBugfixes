{"path":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","commits":[{"id":"790c3f61c9b891d66d919c5d10db9fa5216eb0f1","date":1274818604,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n    assertTrue(tps.next());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n    assertTrue(tps.next());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    TermPositions tps = reader.termPositions(new Term(\"content\", \"another\"));\n    assertTrue(tps.next());\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"683d3f90dda2bbb999c3ce855706d74563a53680","date":1285654576,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer#testWickedLongTerm().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","sourceNew":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new ClassicAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    ClassicAnalyzer sa = new ClassicAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer#testWickedLongTerm().mjava","sourceNew":null,"sourceOld":"  /**\n   * Make sure we skip wicked long terms.\n  */\n  public void testWickedLongTerm() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n      TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n\n    char[] chars = new char[IndexWriter.MAX_TERM_LENGTH];\n    Arrays.fill(chars, 'x');\n    Document doc = new Document();\n    final String bigTerm = new String(chars);\n\n    // This produces a too-long term:\n    String contents = \"abc xyz x\" + bigTerm + \" another term\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"abc bbb ccc\", Field.Store.NO, Field.Index.ANALYZED));\n    writer.addDocument(doc);\n    writer.close();\n\n    IndexReader reader = IndexReader.open(dir, true);\n\n    // Make sure all terms < max size were indexed\n    assertEquals(2, reader.docFreq(new Term(\"content\", \"abc\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"bbb\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"term\")));\n    assertEquals(1, reader.docFreq(new Term(\"content\", \"another\")));\n\n    // Make sure position is still incremented when\n    // massive term is skipped:\n    DocsAndPositionsEnum tps = MultiFields.getTermPositionsEnum(reader,\n                                                                MultiFields.getDeletedDocs(reader),\n                                                                \"content\",\n                                                                new BytesRef(\"another\"));\n    assertTrue(tps.nextDoc() != DocsEnum.NO_MORE_DOCS);\n    assertEquals(1, tps.freq());\n    assertEquals(3, tps.nextPosition());\n\n    // Make sure the doc that has the massive term is in\n    // the index:\n    assertEquals(\"document with wicked long term should is not in the index!\", 2, reader.numDocs());\n\n    reader.close();\n\n    // Make sure we can add a document with exactly the\n    // maximum length term, and search on that term:\n    doc = new Document();\n    doc.add(new Field(\"content\", bigTerm, Field.Store.NO, Field.Index.ANALYZED));\n    StandardAnalyzer sa = new StandardAnalyzer(TEST_VERSION_CURRENT);\n    sa.setMaxTokenLength(100000);\n    writer  = new IndexWriter(dir, new IndexWriterConfig(TEST_VERSION_CURRENT, sa));\n    writer.addDocument(doc);\n    writer.close();\n    reader = IndexReader.open(dir, true);\n    assertEquals(1, reader.docFreq(new Term(\"content\", bigTerm)));\n    reader.close();\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"683d3f90dda2bbb999c3ce855706d74563a53680":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"5f4e87790277826a2aea119328600dfb07761f32":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1","28427ef110c4c5bf5b4057731b83110bd1e13724"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","683d3f90dda2bbb999c3ce855706d74563a53680"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["683d3f90dda2bbb999c3ce855706d74563a53680"],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"683d3f90dda2bbb999c3ce855706d74563a53680":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["790c3f61c9b891d66d919c5d10db9fa5216eb0f1"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["683d3f90dda2bbb999c3ce855706d74563a53680","5f4e87790277826a2aea119328600dfb07761f32"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"790c3f61c9b891d66d919c5d10db9fa5216eb0f1":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}