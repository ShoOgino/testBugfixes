{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    normMap.add(\"&uuml;\", \"ü\");\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    normMap.add(\"&uuml;\", \"ü\");\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545","date":1336310014,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap normMap = new NormalizeCharMap();\n    normMap.add(\"&uuml;\", \"ü\");\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc706b1e03a539d44d99998108feb684bb44cbb2","date":1342522408,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fe33227f6805edab2036cbb80645cc4e2d1fa424","date":1342713534,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharStream charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, CharReader.get( new StringReader( INPUT ) ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    Tokenizer stream = new PatternTokenizer(Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(Pattern.compile(\"Günther\"), 0);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    TokenStream stream = new PatternTokenizer(charStream, Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(charStream, Pattern.compile(\"Günther\"), 0);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    Tokenizer stream = new PatternTokenizer(Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(Pattern.compile(\"Günther\"), 0);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<String>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    Tokenizer stream = new PatternTokenizer(Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(Pattern.compile(\"Günther\"), 0);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75","date":1399205975,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/pattern/TestPatternTokenizer#testOffsetCorrection().mjava","sourceNew":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    Tokenizer stream = new PatternTokenizer(newAttributeFactory(), Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(newAttributeFactory(), Pattern.compile(\"Günther\"), 0);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","sourceOld":"  public void testOffsetCorrection() throws Exception {\n    final String INPUT = \"G&uuml;nther G&uuml;nther is here\";\n\n    // create MappingCharFilter\n    List<String> mappingRules = new ArrayList<>();\n    mappingRules.add( \"\\\"&uuml;\\\" => \\\"ü\\\"\" );\n    NormalizeCharMap.Builder builder = new NormalizeCharMap.Builder();\n    builder.add(\"&uuml;\", \"ü\");\n    NormalizeCharMap normMap = builder.build();\n    CharFilter charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n\n    // create PatternTokenizer\n    Tokenizer stream = new PatternTokenizer(Pattern.compile(\"[,;/\\\\s]+\"), -1);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\", \"is\", \"here\" },\n        new int[] { 0, 13, 26, 29 },\n        new int[] { 12, 25, 28, 33 },\n        INPUT.length());\n    \n    charStream = new MappingCharFilter( normMap, new StringReader( INPUT ) );\n    stream = new PatternTokenizer(Pattern.compile(\"Günther\"), 0);\n    stream.setReader(charStream);\n    assertTokenStreamContents(stream,\n        new String[] { \"Günther\", \"Günther\" },\n        new int[] { 0, 13 },\n        new int[] { 12, 25 },\n        INPUT.length());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"aba371508186796cc6151d8223a5b4e16d02e26e":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545","fc706b1e03a539d44d99998108feb684bb44cbb2"],"fc706b1e03a539d44d99998108feb684bb44cbb2":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["fc706b1e03a539d44d99998108feb684bb44cbb2"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545","fc706b1e03a539d44d99998108feb684bb44cbb2"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["923f36bb0db6f793cf62dbb68723ae3bfbaf1d75"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["b4e5bbc7f726dbcc466cb9b3c029d539a06f6545"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"fc706b1e03a539d44d99998108feb684bb44cbb2":["aba371508186796cc6151d8223a5b4e16d02e26e","ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"fe33227f6805edab2036cbb80645cc4e2d1fa424":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b4e5bbc7f726dbcc466cb9b3c029d539a06f6545":["aba371508186796cc6151d8223a5b4e16d02e26e","fc706b1e03a539d44d99998108feb684bb44cbb2","fe33227f6805edab2036cbb80645cc4e2d1fa424"],"923f36bb0db6f793cf62dbb68723ae3bfbaf1d75":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["aba371508186796cc6151d8223a5b4e16d02e26e","fe33227f6805edab2036cbb80645cc4e2d1fa424","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}