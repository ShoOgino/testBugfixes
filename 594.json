{"path":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","commits":[{"id":"be20f9fed1d3edcb1c84abcc39df87a90fab22df","date":1275590285,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), false);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5dd1f022589b14944b0bb3a6bcf8b03e79fda2b0","date":1275676756,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), false);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b3db5e0b3f68fa60d974ea225303686872186aa","date":1275754578,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          bytes.copyUsingLengthPrefix(term);\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"87697f76828e4cfc6dc898262de8898e5010703f","date":1276817002,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7","date":1277380640,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n          if (numUniqueTerms > Integer.MAX_VALUE-1) {\n            throw new IllegalStateException(\"this field has too many (\" + numUniqueTerms + \") unique terms\");\n          }\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"163fe85a71d778fd2b7747f65ca27b54829e2e57","date":1279898785,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eee9156bf08d7eaae5e8d8ab5f855ba61012e257","date":1285443157,"type":4,"author":"Ryan McKinley","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":4,"author":"Michael Busch","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(IndexReader,Entry).mjava","sourceNew":null,"sourceOld":"    @Override\n    protected Object createValue(IndexReader reader, Entry entryKey)\n        throws IOException {\n\n      String field = StringHelper.intern(entryKey.field);\n      Terms terms = MultiFields.getTerms(reader, field);\n\n      final boolean fasterButMoreRAM = ((Boolean) entryKey.custom).booleanValue();\n\n      final PagedBytes bytes = new PagedBytes(15);\n\n      int startBytesBPV;\n      int startTermsBPV;\n      int startNumUniqueTerms;\n\n      int maxDoc = reader.maxDoc();\n      final int termCountHardLimit;\n      if (maxDoc == Integer.MAX_VALUE) {\n        termCountHardLimit = Integer.MAX_VALUE;\n      } else {\n        termCountHardLimit = maxDoc+1;\n      }\n\n      if (terms != null) {\n        // Try for coarse estimate for number of bits; this\n        // should be an underestimate most of the time, which\n        // is fine -- GrowableWriter will reallocate as needed\n        long numUniqueTerms = 0;\n        try {\n          numUniqueTerms = terms.getUniqueTermCount();\n        } catch (UnsupportedOperationException uoe) {\n          numUniqueTerms = -1;\n        }\n        if (numUniqueTerms != -1) {\n\n          if (numUniqueTerms > termCountHardLimit) {\n            // app is misusing the API (there is more than\n            // one term per doc); in this case we make best\n            // effort to load what we can (see LUCENE-2142)\n            numUniqueTerms = termCountHardLimit;\n          }\n\n          startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n          startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n          startNumUniqueTerms = (int) numUniqueTerms;\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n      } else {\n        startBytesBPV = 1;\n        startTermsBPV = 1;\n        startNumUniqueTerms = 1;\n      }\n\n      GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, fasterButMoreRAM);\n      final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, reader.maxDoc(), fasterButMoreRAM);\n\n      // 0 is reserved for \"unset\"\n      bytes.copyUsingLengthPrefix(new BytesRef());\n      int termOrd = 1;\n\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        DocsEnum docs = null;\n\n        while(true) {\n          final BytesRef term = termsEnum.next();\n          if (term == null) {\n            break;\n          }\n          if (termOrd >= termCountHardLimit) {\n            break;\n          }\n\n          if (termOrd == termOrdToBytesOffset.size()) {\n            // NOTE: this code only runs if the incoming\n            // reader impl doesn't implement\n            // getUniqueTermCount (which should be uncommon)\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n          }\n          termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n          docs = termsEnum.docs(delDocs, docs);\n          while (true) {\n            final int docID = docs.nextDoc();\n            if (docID == DocsEnum.NO_MORE_DOCS) {\n              break;\n            }\n            docToTermOrd.set(docID, termOrd);\n          }\n          termOrd++;\n        }\n\n        if (termOrdToBytesOffset.size() > termOrd) {\n          termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n        }\n      }\n\n      // maybe an int-only impl?\n      return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7"],"87697f76828e4cfc6dc898262de8898e5010703f":["6b3db5e0b3f68fa60d974ea225303686872186aa"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":["03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7"],"5dd1f022589b14944b0bb3a6bcf8b03e79fda2b0":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5f4e87790277826a2aea119328600dfb07761f32":["6b3db5e0b3f68fa60d974ea225303686872186aa","03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7"],"03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7":["87697f76828e4cfc6dc898262de8898e5010703f"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["5f4e87790277826a2aea119328600dfb07761f32","eee9156bf08d7eaae5e8d8ab5f855ba61012e257"],"6b3db5e0b3f68fa60d974ea225303686872186aa":["5dd1f022589b14944b0bb3a6bcf8b03e79fda2b0"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257"]},"commit2Childs":{"eee9156bf08d7eaae5e8d8ab5f855ba61012e257":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"87697f76828e4cfc6dc898262de8898e5010703f":["03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7"],"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["5dd1f022589b14944b0bb3a6bcf8b03e79fda2b0"],"163fe85a71d778fd2b7747f65ca27b54829e2e57":[],"5dd1f022589b14944b0bb3a6bcf8b03e79fda2b0":["6b3db5e0b3f68fa60d974ea225303686872186aa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"5f4e87790277826a2aea119328600dfb07761f32":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"03b0f4eb3bc61d48f2910ae15fd1457a3f907ff7":["eee9156bf08d7eaae5e8d8ab5f855ba61012e257","163fe85a71d778fd2b7747f65ca27b54829e2e57","5f4e87790277826a2aea119328600dfb07761f32"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"6b3db5e0b3f68fa60d974ea225303686872186aa":["87697f76828e4cfc6dc898262de8898e5010703f","5f4e87790277826a2aea119328600dfb07761f32"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["163fe85a71d778fd2b7747f65ca27b54829e2e57","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}