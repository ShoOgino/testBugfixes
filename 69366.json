{"path":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    TermAttribute termAtt = tokenStream.addAttribute(TermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.term()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    TermAttribute termAtt = tokenStream.addAttribute(TermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.term()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a7347509fad0711ac30cb15a746e9a3830a38ebd","date":1275388513,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    TermAttribute termAtt = tokenStream.addAttribute(TermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.term()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c45bbf26db88631f7a389cbff0f4eab70f55ec64","date":1303271007,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d493718201f0d0c54c773fb323d87bbd2fbffe41","date":1303546048,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","date":1305207152,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c700f8d0842d3e52bb2bdfbfdc046a137e836edb","date":1305285499,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","pathOld":"lucene/contrib/highlighter/src/java/org/apache/lucene/search/highlight/Highlighter#getBestTextFragments(TokenStream,String,boolean,int).mjava","sourceNew":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","sourceOld":"\t/**\n\t * Low level api to get the most relevant (formatted) sections of the document.\n\t * This method has been made public to allow visibility of score information held in TextFragment objects.\n\t * Thanks to Jason Calabrese for help in redefining the interface.\n\t * @param tokenStream\n\t * @param text\n\t * @param maxNumFragments\n\t * @param mergeContiguousFragments\n\t * @throws IOException\n\t * @throws InvalidTokenOffsetsException thrown if any token's endOffset exceeds the provided text's length\n\t */\n\tpublic final TextFragment[] getBestTextFragments(\n\t\tTokenStream tokenStream,\n\t\tString text,\n\t\tboolean mergeContiguousFragments,\n\t\tint maxNumFragments)\n\t\tthrows IOException, InvalidTokenOffsetsException\n\t{\n\t\tArrayList<TextFragment> docFrags = new ArrayList<TextFragment>();\n\t\tStringBuilder newText=new StringBuilder();\n\t\t\n\t    CharTermAttribute termAtt = tokenStream.addAttribute(CharTermAttribute.class);\n\t    OffsetAttribute offsetAtt = tokenStream.addAttribute(OffsetAttribute.class);\n\t    tokenStream.addAttribute(PositionIncrementAttribute.class);\n\t    tokenStream.reset();\n\t    \n\t\tTextFragment currentFrag =\tnew TextFragment(newText,newText.length(), docFrags.size());\n\t\t\n    if (fragmentScorer instanceof QueryScorer) {\n      ((QueryScorer) fragmentScorer).setMaxDocCharsToAnalyze(maxDocCharsToAnalyze);\n    }\n    \n\t\tTokenStream newStream = fragmentScorer.init(tokenStream);\n\t\tif(newStream != null) {\n\t\t  tokenStream = newStream;\n\t\t}\n\t\tfragmentScorer.startFragment(currentFrag);\n\t\tdocFrags.add(currentFrag);\n\n\t\tFragmentQueue fragQueue = new FragmentQueue(maxNumFragments);\n\n\t\ttry\n\t\t{\n\n\t\t\tString tokenText;\n\t\t\tint startOffset;\n\t\t\tint endOffset;\n\t\t\tint lastEndOffset = 0;\n\t\t\ttextFragmenter.start(text, tokenStream);\n\n\t\t\tTokenGroup tokenGroup=new TokenGroup(tokenStream);\n\n\t\t\tfor (boolean next = tokenStream.incrementToken(); next && (offsetAtt.startOffset()< maxDocCharsToAnalyze);\n\t\t\t      next = tokenStream.incrementToken())\n\t\t\t{\n\t\t\t\tif(\t(offsetAtt.endOffset()>text.length())\n\t\t\t\t\t||\n\t\t\t\t\t(offsetAtt.startOffset()>text.length())\n\t\t\t\t\t)\t\t\t\t\t\t\n\t\t\t\t{\n\t\t\t\t\tthrow new InvalidTokenOffsetsException(\"Token \"+ termAtt.toString()\n\t\t\t\t\t\t\t+\" exceeds length of provided text sized \"+text.length());\n\t\t\t\t}\n\t\t\t\tif((tokenGroup.numTokens>0)&&(tokenGroup.isDistinct()))\n\t\t\t\t{\n\t\t\t\t\t//the current token is distinct from previous tokens -\n\t\t\t\t\t// markup the cached token group info\n\t\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\t\tnewText.append(markedUpText);\n\t\t\t\t\tlastEndOffset=Math.max(endOffset, lastEndOffset);\n\t\t\t\t\ttokenGroup.clear();\n\n\t\t\t\t\t//check if current token marks the start of a new fragment\n\t\t\t\t\tif(textFragmenter.isNewFragment())\n\t\t\t\t\t{\n\t\t\t\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\t\t\t\t\t\t//record stats for a new fragment\n\t\t\t\t\t\tcurrentFrag.textEndPos = newText.length();\n\t\t\t\t\t\tcurrentFrag =new TextFragment(newText, newText.length(), docFrags.size());\n\t\t\t\t\t\tfragmentScorer.startFragment(currentFrag);\n\t\t\t\t\t\tdocFrags.add(currentFrag);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\ttokenGroup.addToken(fragmentScorer.getTokenScore());\n\n//\t\t\t\tif(lastEndOffset>maxDocBytesToAnalyze)\n//\t\t\t\t{\n//\t\t\t\t\tbreak;\n//\t\t\t\t}\n\t\t\t}\n\t\t\tcurrentFrag.setScore(fragmentScorer.getFragmentScore());\n\n\t\t\tif(tokenGroup.numTokens>0)\n\t\t\t{\n\t\t\t\t//flush the accumulated text (same code as in above loop)\n\t\t\t\tstartOffset = tokenGroup.matchStartOffset;\n\t\t\t\tendOffset = tokenGroup.matchEndOffset;\n\t\t\t\ttokenText = text.substring(startOffset, endOffset);\n\t\t\t\tString markedUpText=formatter.highlightTerm(encoder.encodeText(tokenText), tokenGroup);\n\t\t\t\t//store any whitespace etc from between this and last group\n\t\t\t\tif (startOffset > lastEndOffset)\n\t\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset, startOffset)));\n\t\t\t\tnewText.append(markedUpText);\n\t\t\t\tlastEndOffset=Math.max(lastEndOffset,endOffset);\n\t\t\t}\n\n\t\t\t//Test what remains of the original text beyond the point where we stopped analyzing \n\t\t\tif (\n//\t\t\t\t\tif there is text beyond the last token considered..\n\t\t\t\t\t(lastEndOffset < text.length()) \n\t\t\t\t\t&&\n//\t\t\t\t\tand that text is not too large...\n\t\t\t\t\t(text.length()<= maxDocCharsToAnalyze)\n\t\t\t\t)\t\t\t\t\n\t\t\t{\n\t\t\t\t//append it to the last fragment\n\t\t\t\tnewText.append(encoder.encodeText(text.substring(lastEndOffset)));\n\t\t\t}\n\n\t\t\tcurrentFrag.textEndPos = newText.length();\n\n\t\t\t//sort the most relevant sections of the text\n\t\t\tfor (Iterator<TextFragment> i = docFrags.iterator(); i.hasNext();)\n\t\t\t{\n\t\t\t\tcurrentFrag = i.next();\n\n\t\t\t\t//If you are running with a version of Lucene before 11th Sept 03\n\t\t\t\t// you do not have PriorityQueue.insert() - so uncomment the code below\n\t\t\t\t/*\n\t\t\t\t\t\t\t\t\tif (currentFrag.getScore() >= minScore)\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tfragQueue.put(currentFrag);\n\t\t\t\t\t\t\t\t\t\tif (fragQueue.size() > maxNumFragments)\n\t\t\t\t\t\t\t\t\t\t{ // if hit queue overfull\n\t\t\t\t\t\t\t\t\t\t\tfragQueue.pop(); // remove lowest in hit queue\n\t\t\t\t\t\t\t\t\t\t\tminScore = ((TextFragment) fragQueue.top()).getScore(); // reset minScore\n\t\t\t\t\t\t\t\t\t\t}\n\n\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t*/\n\t\t\t\t//The above code caused a problem as a result of Christoph Goller's 11th Sept 03\n\t\t\t\t//fix to PriorityQueue. The correct method to use here is the new \"insert\" method\n\t\t\t\t// USE ABOVE CODE IF THIS DOES NOT COMPILE!\n\t\t\t\tfragQueue.insertWithOverflow(currentFrag);\n\t\t\t}\n\n\t\t\t//return the most relevant fragments\n\t\t\tTextFragment frag[] = new TextFragment[fragQueue.size()];\n\t\t\tfor (int i = frag.length - 1; i >= 0; i--)\n\t\t\t{\n\t\t\t\tfrag[i] = fragQueue.pop();\n\t\t\t}\n\n\t\t\t//merge any contiguous fragments to improve readability\n\t\t\tif(mergeContiguousFragments)\n\t\t\t{\n\t\t\t\tmergeContiguousFragments(frag);\n\t\t\t\tArrayList<TextFragment> fragTexts = new ArrayList<TextFragment>();\n\t\t\t\tfor (int i = 0; i < frag.length; i++)\n\t\t\t\t{\n\t\t\t\t\tif ((frag[i] != null) && (frag[i].getScore() > 0))\n\t\t\t\t\t{\n\t\t\t\t\t\tfragTexts.add(frag[i]);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfrag= fragTexts.toArray(new TextFragment[0]);\n\t\t\t}\n\n\t\t\treturn frag;\n\n\t\t}\n\t\tfinally\n\t\t{\n\t\t\tif (tokenStream != null)\n\t\t\t{\n\t\t\t\ttry\n\t\t\t\t{\n\t\t\t\t  tokenStream.end();\n\t\t\t\t\ttokenStream.close();\n\t\t\t\t}\n\t\t\t\tcatch (Exception e)\n\t\t\t\t{\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"a3776dccca01c11e7046323cfad46a3b4a471233":["a7347509fad0711ac30cb15a746e9a3830a38ebd","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["a7347509fad0711ac30cb15a746e9a3830a38ebd","c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":["135621f3a0670a9394eb563224a3b76cc4dddc0f","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"d493718201f0d0c54c773fb323d87bbd2fbffe41":["a7347509fad0711ac30cb15a746e9a3830a38ebd","c45bbf26db88631f7a389cbff0f4eab70f55ec64"],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c45bbf26db88631f7a389cbff0f4eab70f55ec64":["e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4","135621f3a0670a9394eb563224a3b76cc4dddc0f","d493718201f0d0c54c773fb323d87bbd2fbffe41"],"e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4":["b89678825b68eccaf09e6ab71675fc0b0af1e099","a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"a3776dccca01c11e7046323cfad46a3b4a471233":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["c700f8d0842d3e52bb2bdfbfdc046a137e836edb"],"c700f8d0842d3e52bb2bdfbfdc046a137e836edb":[],"d493718201f0d0c54c773fb323d87bbd2fbffe41":[],"a7347509fad0711ac30cb15a746e9a3830a38ebd":["c45bbf26db88631f7a389cbff0f4eab70f55ec64","a3776dccca01c11e7046323cfad46a3b4a471233","135621f3a0670a9394eb563224a3b76cc4dddc0f","d493718201f0d0c54c773fb323d87bbd2fbffe41"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a7347509fad0711ac30cb15a746e9a3830a38ebd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a3776dccca01c11e7046323cfad46a3b4a471233","c700f8d0842d3e52bb2bdfbfdc046a137e836edb","d493718201f0d0c54c773fb323d87bbd2fbffe41","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}