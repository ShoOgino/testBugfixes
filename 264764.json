{"path":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","commits":[{"id":"41c5beb1570c380eda38c433d43949de2fb7e000","date":1357425858,"type":0,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieType) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0549c25bebe09ca9d5adff1ae344576618c627bf","c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0549c25bebe09ca9d5adff1ae344576618c627bf","date":1357427031,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieType) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","bugFix":["41c5beb1570c380eda38c433d43949de2fb7e000"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"/dev/null","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08970e5b8411182a29412c177eff67ec1110095b","date":1366640815,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", value);\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", \"\");\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","bugFix":["41c5beb1570c380eda38c433d43949de2fb7e000"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","pathOld":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","sourceNew":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", value);\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", \"\");\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","sourceOld":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(value));\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", new StringReader(\"\"));\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"99c9d8533c954f661481ae44273622957dbf572f","date":1380991288,"type":4,"author":"Robert Muir","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/TestTrie#testTokenizer().mjava","sourceNew":null,"sourceOld":"  @Test\n  public void testTokenizer() throws Exception {\n    FieldType type = h.getCore().getLatestSchema().getFieldType(\"tint\");\n    assertTrue(type instanceof TrieField);\n    \n    String value = String.valueOf(random().nextInt());\n    TokenStream ts = type.getAnalyzer().tokenStream(\"dummy\", value);\n    OffsetAttribute ofsAtt = ts.addAttribute(OffsetAttribute.class);\n    ts.reset();\n    int count = 0;\n    while (ts.incrementToken()) {\n      count++;\n      assertEquals(0, ofsAtt.startOffset());\n      assertEquals(value.length(), ofsAtt.endOffset());\n    }\n    final int precStep = ((TrieField) type).getPrecisionStep();\n    assertEquals( (32 + precStep - 1) / precStep, count);\n    ts.end();\n    assertEquals(value.length(), ofsAtt.startOffset());\n    assertEquals(value.length(), ofsAtt.endOffset());\n    ts.close();\n    \n    // Test empty one:\n    ts = type.getAnalyzer().tokenStream(\"dummy\", \"\");\n    ts.reset();\n    assertFalse(ts.incrementToken());\n    ts.end();\n    assertEquals(0, ofsAtt.startOffset());\n    assertEquals(0, ofsAtt.endOffset());\n    ts.close();    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"99c9d8533c954f661481ae44273622957dbf572f":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0549c25bebe09ca9d5adff1ae344576618c627bf"],"0549c25bebe09ca9d5adff1ae344576618c627bf":["41c5beb1570c380eda38c433d43949de2fb7e000"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["08970e5b8411182a29412c177eff67ec1110095b","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["08970e5b8411182a29412c177eff67ec1110095b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"41c5beb1570c380eda38c433d43949de2fb7e000":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["99c9d8533c954f661481ae44273622957dbf572f"],"08970e5b8411182a29412c177eff67ec1110095b":["0549c25bebe09ca9d5adff1ae344576618c627bf"]},"commit2Childs":{"99c9d8533c954f661481ae44273622957dbf572f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":[],"0549c25bebe09ca9d5adff1ae344576618c627bf":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","08970e5b8411182a29412c177eff67ec1110095b"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["99c9d8533c954f661481ae44273622957dbf572f","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","41c5beb1570c380eda38c433d43949de2fb7e000"],"41c5beb1570c380eda38c433d43949de2fb7e000":["0549c25bebe09ca9d5adff1ae344576618c627bf"],"08970e5b8411182a29412c177eff67ec1110095b":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}