{"path":"lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping#testDocSortOptimizationWithAfter().mjava","commits":[{"id":"e3d1c24d2bbe79dcf77ffcb104706e42ae3c9241","date":1599588987,"type":0,"author":"Mayya Sharipova","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping#testDocSortOptimizationWithAfter().mjava","pathOld":"/dev/null","sourceNew":"  public void testDocSortOptimizationWithAfter() throws IOException {\n    final Directory dir = newDirectory();\n    final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig());\n    final int numDocs = atLeast(150);\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = new Document();\n      writer.addDocument(doc);\n      if ((i > 0) && (i % 50 == 0)) {\n        writer.commit();\n      }\n    }\n    final IndexReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    final int numHits = 3;\n    final int totalHitsThreshold = 3;\n    final int[] searchAfters = {10, 140, numDocs - 4};\n    for (int searchAfter : searchAfters) {\n      // sort by _doc with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // check that very few docs were collected\n        assertTrue(topDocs.totalHits.value < 10);\n      }\n\n      // sort by _doc + _score with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC, FIELD_SCORE);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Object[]{searchAfter, 1.0f});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // assert that very few docs were collected\n        assertTrue(topDocs.totalHits.value < 10);\n      }\n\n      // sort by _doc desc should not trigger optimization\n      {\n        final Sort sort = new Sort(new SortField(null, SortField.Type.DOC, true));\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter - 1 - i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        // assert that all documents were collected\n        assertEquals(numDocs, topDocs.totalHits.value);\n      }\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e977a403f93a917f75266c88727eadb89e4f64fc","date":1600866583,"type":3,"author":"Mayya Sharipova","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping#testDocSortOptimizationWithAfter().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping#testDocSortOptimizationWithAfter().mjava","sourceNew":"  public void testDocSortOptimizationWithAfter() throws IOException {\n    final Directory dir = newDirectory();\n    final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig());\n    final int numDocs = atLeast(150);\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = new Document();\n      writer.addDocument(doc);\n      if ((i > 0) && (i % 50 == 0)) {\n        writer.flush();\n      }\n    }\n\n    final IndexReader reader = DirectoryReader.open(writer);\n    writer.close();\n    IndexSearcher searcher = newSearcher(reader);\n    final int numHits = 10;\n    final int totalHitsThreshold = 10;\n    final int[] searchAfters = {3, 10, numDocs - 10};\n    for (int searchAfter : searchAfters) {\n      // sort by _doc with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        int expNumHits = (searchAfter >= (numDocs - numHits)) ? (numDocs - searchAfter - 1) : numHits;\n        assertEquals(expNumHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < topDocs.scoreDocs.length; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // check that very few docs were collected\n        assertTrue(topDocs.totalHits.value < numDocs);\n      }\n\n      // sort by _doc + _score with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC, FIELD_SCORE);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Object[]{searchAfter, 1.0f});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        int expNumHits = (searchAfter >= (numDocs - numHits)) ? (numDocs - searchAfter - 1) : numHits;\n        assertEquals(expNumHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < topDocs.scoreDocs.length; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // assert that very few docs were collected\n        assertTrue(topDocs.totalHits.value < numDocs);\n      }\n\n      // sort by _doc desc should not trigger optimization\n      {\n        final Sort sort = new Sort(new SortField(null, SortField.Type.DOC, true));\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        int expNumHits = (searchAfter < numHits) ? searchAfter : numHits;\n        assertEquals(expNumHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < topDocs.scoreDocs.length; i++) {\n          int expectedDocID = searchAfter - 1 - i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        // assert that all documents were collected\n        assertEquals(numDocs, topDocs.totalHits.value);\n      }\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testDocSortOptimizationWithAfter() throws IOException {\n    final Directory dir = newDirectory();\n    final IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig());\n    final int numDocs = atLeast(150);\n    for (int i = 0; i < numDocs; ++i) {\n      final Document doc = new Document();\n      writer.addDocument(doc);\n      if ((i > 0) && (i % 50 == 0)) {\n        writer.commit();\n      }\n    }\n    final IndexReader reader = DirectoryReader.open(writer);\n    IndexSearcher searcher = new IndexSearcher(reader);\n    final int numHits = 3;\n    final int totalHitsThreshold = 3;\n    final int[] searchAfters = {10, 140, numDocs - 4};\n    for (int searchAfter : searchAfters) {\n      // sort by _doc with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // check that very few docs were collected\n        assertTrue(topDocs.totalHits.value < 10);\n      }\n\n      // sort by _doc + _score with search after should trigger optimization\n      {\n        final Sort sort = new Sort(FIELD_DOC, FIELD_SCORE);\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Object[]{searchAfter, 1.0f});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter + 1 + i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        assertTrue(collector.isEarlyTerminated());\n        // assert that very few docs were collected\n        assertTrue(topDocs.totalHits.value < 10);\n      }\n\n      // sort by _doc desc should not trigger optimization\n      {\n        final Sort sort = new Sort(new SortField(null, SortField.Type.DOC, true));\n        FieldDoc after = new FieldDoc(searchAfter, Float.NaN, new Integer[]{searchAfter});\n        final TopFieldCollector collector = TopFieldCollector.create(sort, numHits, after, totalHitsThreshold);\n        searcher.search(new MatchAllDocsQuery(), collector);\n        TopDocs topDocs = collector.topDocs();\n        assertEquals(numHits, topDocs.scoreDocs.length);\n        for (int i = 0; i < numHits; i++) {\n          int expectedDocID = searchAfter - 1 - i;\n          assertEquals(expectedDocID, topDocs.scoreDocs[i].doc);\n        }\n        // assert that all documents were collected\n        assertEquals(numDocs, topDocs.totalHits.value);\n      }\n    }\n\n    writer.close();\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e977a403f93a917f75266c88727eadb89e4f64fc":["e3d1c24d2bbe79dcf77ffcb104706e42ae3c9241"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e977a403f93a917f75266c88727eadb89e4f64fc"],"e3d1c24d2bbe79dcf77ffcb104706e42ae3c9241":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"e977a403f93a917f75266c88727eadb89e4f64fc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e3d1c24d2bbe79dcf77ffcb104706e42ae3c9241"],"e3d1c24d2bbe79dcf77ffcb104706e42ae3c9241":["e977a403f93a917f75266c88727eadb89e4f64fc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}