{"path":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","commits":[{"id":"849494cf2f3a96af5c8c84995108ddd8456fcd04","date":1372277913,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef","7c88c33fee958027b9192ef2c6bb54836618b165","70505a4870244b22d4d0f1a98951b08b197d5bb0","26c232b6682b30734da1eec2869ff14f0e065ab0","f635d9be9c2f00fd5eaa3ca437adb33f661e2aac","f1fd096d09854ad0876c8474505b917b7fc807ee"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":0,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f796095dfe542c48b2e9d89c8bfb9e6b64c70d50","date":1376481440,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":["70505a4870244b22d4d0f1a98951b08b197d5bb0","f1fd096d09854ad0876c8474505b917b7fc807ee"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c88c33fee958027b9192ef2c6bb54836618b165","date":1390430350,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    \n    try {\n      if (!fs.exists(tlogDir)) {\n        boolean success = fs.mkdirs(tlogDir);\n        if (!success) {\n          throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n        }\n      }\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal shutdown both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f1fd096d09854ad0876c8474505b917b7fc807ee","date":1408630567,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    FileSystem oldFs = fs;\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      if (oldFs != null) {\n        oldFs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    try {\n      if (fs != null) {\n        fs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04","f796095dfe542c48b2e9d89c8bfb9e6b64c70d50"],"bugIntro":["70505a4870244b22d4d0f1a98951b08b197d5bb0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c6d82c04c0bc088fae82f28ef47cb25a164f47fd","date":1422552161,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    FileSystem oldFs = fs;\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      if (oldFs != null) {\n        oldFs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    FileSystem oldFs = fs;\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      if (oldFs != null) {\n        oldFs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"70505a4870244b22d4d0f1a98951b08b197d5bb0","date":1424906348,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    FileSystem oldFs = fs;\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      if (oldFs != null) {\n        oldFs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04","f1fd096d09854ad0876c8474505b917b7fc807ee","f796095dfe542c48b2e9d89c8bfb9e6b64c70d50"],"bugIntro":["6afb0ba86024b96e8b34cfc2e15562239dc36360"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6f3c1f22c5fe0011e187dac3151422365ae857f3","date":1425728437,"type":3,"author":"Ramkumar Aiyengar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    if (ulogDir != null) {\n      dataDir = ulogDir;\n    }\n    if (dataDir == null || dataDir.length()==0) {\n      dataDir = core.getDataDir();\n    }\n    \n    if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n      try {\n        dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR, e);\n      }\n    }\n    \n    FileSystem oldFs = fs;\n    \n    try {\n      fs = FileSystem.newInstance(new Path(dataDir).toUri(), getConf());\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    try {\n      if (oldFs != null) {\n        oldFs.close();\n      }\n    } catch (IOException e) {\n      throw new SolrException(ErrorCode.SERVER_ERROR, e);\n    }\n    \n    this.uhandler = uhandler;\n    \n    if (dataDir.equals(lastDataDir)) {\n      if (debug) {\n        log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n            \" this is a reopen... nothing else to do.\");\n      }\n      \n      versionInfo.reload();\n      \n      // on a normal reopen, we currently shouldn't have to do anything\n      return;\n    }\n    lastDataDir = dataDir;\n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(numRecordsToKeep);\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2b82485108ad24cfd45d88a7465e68000f54055c","date":1430225324,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53","date":1449051812,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    HdfsUpdateLog.RecentUpdates startingUpdates = getRecentUpdates();\n    try {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n      \n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n      \n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n      \n    } finally {\n      startingUpdates.close();\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f635d9be9c2f00fd5eaa3ca437adb33f661e2aac","date":1458583817,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, 256);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d9da6af0d6e0b9ee92e3adbfd2796792453afbef","date":1472233199,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      newestLogsOnStartup.addFirst(ll);\n      if (newestLogsOnStartup.size() >= 2) break;\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"26c232b6682b30734da1eec2869ff14f0e065ab0","date":1487173758,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoMBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n    \n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n    \n  }\n\n","bugFix":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"816521ebaad5add9cb96bb88c577394e2938c40b","date":1491931343,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoMBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54ca69905c5d9d1529286f06ab1d12c68f6c13cb","date":1492683554,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoMBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1455c941cc4ce652efc776fc23471b0e499246f6","date":1528086751,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n      startingOperation = startingUpdates.getLatestOperation();\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"870bbea2a1d8085b48b52a1480ac95db389476c1","date":1553970360,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          fs = FileSystem.get(new Path(dataDir).toUri(), getConf());\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6afb0ba86024b96e8b34cfc2e15562239dc36360","date":1579768208,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id +\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id,\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":["70505a4870244b22d4d0f1a98951b08b197d5bb0"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"740d649f013f07efbeb73ca854f106c60166e7c0","date":1587431295,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir={}, next id={}  this is a reopen or double init ... nothing else to do.\"\n              , tlogDir, id);\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir={}, existing tlogs={}, next id={}\"\n          , tlogDir, Arrays.asList(tlogFiles), id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log {}\", ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: {}\", e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", next id=\" + id +\n              \" this is a reopen or double init ... nothing else to do.\");\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir=\" + tlogDir + \", existing tlogs=\"\n          + Arrays.asList(tlogFiles) + \", next id=\" + id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log \" + ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \" + e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"018a36ff4088cb91ab12cbe44f696d81d1fadd77","date":1591657414,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir={}, next id={}  this is a reopen or double init ... nothing else to do.\"\n              , tlogDir, id);\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir={}, existing tlogs={}, next id={}\"\n          , tlogDir, Arrays.asList(tlogFiles), id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log {}\", ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: {}\", e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        @SuppressWarnings({\"unchecked\"})\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir={}, next id={}  this is a reopen or double init ... nothing else to do.\"\n              , tlogDir, id);\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir={}, existing tlogs={}, next id={}\"\n          , tlogDir, Arrays.asList(tlogFiles), id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log {}\", ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: {}\", e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57c238f5fb83803b49b37b3a1a12224a64d47542","date":1593655679,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","pathOld":"solr/core/src/java/org/apache/solr/update/HdfsUpdateLog#init(UpdateHandler,SolrCore).mjava","sourceNew":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir={}, next id={}  this is a reopen or double init ... nothing else to do.\"\n              , tlogDir, id);\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir={}, existing tlogs={}, next id={}\"\n          , tlogDir, Arrays.asList(tlogFiles), id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log {}\", ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: \", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        @SuppressWarnings({\"unchecked\"})\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","sourceOld":"  @Override\n  public void init(UpdateHandler uhandler, SolrCore core) {\n    \n    // ulogDir from CoreDescriptor overrides\n    String ulogDir = core.getCoreDescriptor().getUlogDir();\n\n    this.uhandler = uhandler;\n\n    synchronized (fsLock) {\n      // just like dataDir, we do not allow\n      // moving the tlog dir on reload\n      if (fs == null) {\n        if (ulogDir != null) {\n          dataDir = ulogDir;\n        }\n        if (dataDir == null || dataDir.length() == 0) {\n          dataDir = core.getDataDir();\n        }\n        \n        if (!core.getDirectoryFactory().isAbsolute(dataDir)) {\n          try {\n            dataDir = core.getDirectoryFactory().getDataHome(core.getCoreDescriptor());\n          } catch (IOException e) {\n            throw new SolrException(ErrorCode.SERVER_ERROR, e);\n          }\n        }\n        \n        try {\n          Path dataDirPath = new Path(dataDir);\n          fs = FileSystem.get(dataDirPath.toUri(), getConf(dataDirPath));\n        } catch (IOException e) {\n          throw new SolrException(ErrorCode.SERVER_ERROR, e);\n        }\n      } else {\n        if (debug) {\n          log.debug(\"UpdateHandler init: tlogDir={}, next id={}  this is a reopen or double init ... nothing else to do.\"\n              , tlogDir, id);\n        }\n        versionInfo.reload();\n        return;\n      }\n    }\n    \n    tlogDir = new Path(dataDir, TLOG_NAME);\n    while (true) {\n      try {\n        if (!fs.exists(tlogDir)) {\n          boolean success = fs.mkdirs(tlogDir);\n          if (!success) {\n            throw new RuntimeException(\"Could not create directory:\" + tlogDir);\n          }\n        } else {\n          fs.mkdirs(tlogDir); // To check for safe mode\n        }\n        break;\n      } catch (RemoteException e) {\n        if (e.getClassName().equals(\n            \"org.apache.hadoop.hdfs.server.namenode.SafeModeException\")) {\n          log.warn(\"The NameNode is in SafeMode - Solr will wait 5 seconds and try again.\");\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException e1) {\n            Thread.interrupted();\n          }\n          continue;\n        }\n        throw new RuntimeException(\n            \"Problem creating directory: \" + tlogDir, e);\n      } catch (IOException e) {\n        throw new RuntimeException(\"Problem creating directory: \" + tlogDir, e);\n      }\n    }\n\n    String[] oldBufferTlog = getBufferLogList(fs, tlogDir);\n    if (oldBufferTlog != null && oldBufferTlog.length != 0) {\n      existOldBufferLog = true;\n    }\n    \n    tlogFiles = getLogList(fs, tlogDir);\n    id = getLastLogId() + 1; // add 1 since we will create a new log for the\n                             // next update\n    \n    if (debug) {\n      log.debug(\"UpdateHandler init: tlogDir={}, existing tlogs={}, next id={}\"\n          , tlogDir, Arrays.asList(tlogFiles), id);\n    }\n    \n    TransactionLog oldLog = null;\n    for (String oldLogName : tlogFiles) {\n      Path f = new Path(tlogDir, oldLogName);\n      try {\n        oldLog = new HdfsTransactionLog(fs, f, null, true, tlogDfsReplication);\n        addOldLog(oldLog, false); // don't remove old logs on startup since more\n                                  // than one may be uncapped.\n      } catch (Exception e) {\n        INIT_FAILED_LOGS_COUNT.incrementAndGet();\n        SolrException.log(log, \"Failure to open existing log file (non fatal) \"\n            + f, e);\n        try {\n          fs.delete(f, false);\n        } catch (IOException e1) {\n          throw new RuntimeException(e1);\n        }\n      }\n    }\n    \n    // Record first two logs (oldest first) at startup for potential tlog\n    // recovery.\n    // It's possible that at abnormal close both \"tlog\" and \"prevTlog\" were\n    // uncapped.\n    for (TransactionLog ll : logs) {\n      if (newestLogsOnStartup.size() < 2) {\n        newestLogsOnStartup.addFirst(ll);\n      } else {\n        // We're never going to modify old non-recovery logs - no need to hold their output open\n        log.info(\"Closing output for old non-recovery log {}\", ll);\n        ll.closeOutput();\n      }\n    }\n    \n    try {\n      versionInfo = new VersionInfo(this, numVersionBuckets);\n    } catch (SolrException e) {\n      log.error(\"Unable to use updateLog: {}\", e.getMessage(), e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Unable to use updateLog: \" + e.getMessage(), e);\n    }\n    \n    // TODO: these startingVersions assume that we successfully recover from all\n    // non-complete tlogs.\n    try (RecentUpdates startingUpdates = getRecentUpdates()) {\n      startingVersions = startingUpdates.getVersions(getNumRecordsToKeep());\n\n      // populate recent deletes list (since we can't get that info from the\n      // index)\n      for (int i = startingUpdates.deleteList.size() - 1; i >= 0; i--) {\n        DeleteUpdate du = startingUpdates.deleteList.get(i);\n        oldDeletes.put(new BytesRef(du.id), new LogPtr(-1, du.version));\n      }\n\n      // populate recent deleteByQuery commands\n      for (int i = startingUpdates.deleteByQueryList.size() - 1; i >= 0; i--) {\n        Update update = startingUpdates.deleteByQueryList.get(i);\n        @SuppressWarnings({\"unchecked\"})\n        List<Object> dbq = (List<Object>) update.log.lookup(update.pointer);\n        long version = (Long) dbq.get(1);\n        String q = (String) dbq.get(2);\n        trackDeleteByQuery(q, version);\n      }\n\n    }\n\n    // initialize metrics\n    core.getCoreMetricManager().registerMetricProducer(SolrInfoBean.Category.TLOG.toString(), this);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"d9da6af0d6e0b9ee92e3adbfd2796792453afbef":["f635d9be9c2f00fd5eaa3ca437adb33f661e2aac"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":["26c232b6682b30734da1eec2869ff14f0e065ab0"],"018a36ff4088cb91ab12cbe44f696d81d1fadd77":["740d649f013f07efbeb73ca854f106c60166e7c0"],"740d649f013f07efbeb73ca854f106c60166e7c0":["6afb0ba86024b96e8b34cfc2e15562239dc36360"],"870bbea2a1d8085b48b52a1480ac95db389476c1":["1455c941cc4ce652efc776fc23471b0e499246f6"],"2b82485108ad24cfd45d88a7465e68000f54055c":["6f3c1f22c5fe0011e187dac3151422365ae857f3"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53":["2b82485108ad24cfd45d88a7465e68000f54055c"],"1455c941cc4ce652efc776fc23471b0e499246f6":["816521ebaad5add9cb96bb88c577394e2938c40b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd","6f3c1f22c5fe0011e187dac3151422365ae857f3"],"26c232b6682b30734da1eec2869ff14f0e065ab0":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"f796095dfe542c48b2e9d89c8bfb9e6b64c70d50":["849494cf2f3a96af5c8c84995108ddd8456fcd04"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["f635d9be9c2f00fd5eaa3ca437adb33f661e2aac","d9da6af0d6e0b9ee92e3adbfd2796792453afbef"],"b70042a8a492f7054d480ccdd2be9796510d4327":["816521ebaad5add9cb96bb88c577394e2938c40b","1455c941cc4ce652efc776fc23471b0e499246f6"],"f1fd096d09854ad0876c8474505b917b7fc807ee":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"c6d82c04c0bc088fae82f28ef47cb25a164f47fd":["f1fd096d09854ad0876c8474505b917b7fc807ee"],"7c88c33fee958027b9192ef2c6bb54836618b165":["f796095dfe542c48b2e9d89c8bfb9e6b64c70d50"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["37a0f60745e53927c4c876cfe5b5a58170f0646c","f796095dfe542c48b2e9d89c8bfb9e6b64c70d50"],"57c238f5fb83803b49b37b3a1a12224a64d47542":["018a36ff4088cb91ab12cbe44f696d81d1fadd77"],"f635d9be9c2f00fd5eaa3ca437adb33f661e2aac":["a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53"],"6f3c1f22c5fe0011e187dac3151422365ae857f3":["70505a4870244b22d4d0f1a98951b08b197d5bb0"],"816521ebaad5add9cb96bb88c577394e2938c40b":["26c232b6682b30734da1eec2869ff14f0e065ab0"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f635d9be9c2f00fd5eaa3ca437adb33f661e2aac","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"6afb0ba86024b96e8b34cfc2e15562239dc36360":["870bbea2a1d8085b48b52a1480ac95db389476c1"],"70505a4870244b22d4d0f1a98951b08b197d5bb0":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["7c88c33fee958027b9192ef2c6bb54836618b165"],"f592209545c71895260367152601e9200399776d":["816521ebaad5add9cb96bb88c577394e2938c40b","1455c941cc4ce652efc776fc23471b0e499246f6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["57c238f5fb83803b49b37b3a1a12224a64d47542"]},"commit2Childs":{"d9da6af0d6e0b9ee92e3adbfd2796792453afbef":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"54ca69905c5d9d1529286f06ab1d12c68f6c13cb":[],"018a36ff4088cb91ab12cbe44f696d81d1fadd77":["57c238f5fb83803b49b37b3a1a12224a64d47542"],"740d649f013f07efbeb73ca854f106c60166e7c0":["018a36ff4088cb91ab12cbe44f696d81d1fadd77"],"870bbea2a1d8085b48b52a1480ac95db389476c1":["6afb0ba86024b96e8b34cfc2e15562239dc36360"],"2b82485108ad24cfd45d88a7465e68000f54055c":["a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"a9186bf60d7c6f504d4d5b01cfee95dc4bd15e53":["f635d9be9c2f00fd5eaa3ca437adb33f661e2aac"],"1455c941cc4ce652efc776fc23471b0e499246f6":["870bbea2a1d8085b48b52a1480ac95db389476c1","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"26c232b6682b30734da1eec2869ff14f0e065ab0":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","816521ebaad5add9cb96bb88c577394e2938c40b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["37a0f60745e53927c4c876cfe5b5a58170f0646c","849494cf2f3a96af5c8c84995108ddd8456fcd04"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["26c232b6682b30734da1eec2869ff14f0e065ab0","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"f796095dfe542c48b2e9d89c8bfb9e6b64c70d50":["7c88c33fee958027b9192ef2c6bb54836618b165","3dffec77fb8f7d0e9ca4869dddd6af94528b4576"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"f1fd096d09854ad0876c8474505b917b7fc807ee":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"c6d82c04c0bc088fae82f28ef47cb25a164f47fd":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","70505a4870244b22d4d0f1a98951b08b197d5bb0"],"7c88c33fee958027b9192ef2c6bb54836618b165":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"f635d9be9c2f00fd5eaa3ca437adb33f661e2aac":["d9da6af0d6e0b9ee92e3adbfd2796792453afbef","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"57c238f5fb83803b49b37b3a1a12224a64d47542":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"6f3c1f22c5fe0011e187dac3151422365ae857f3":["2b82485108ad24cfd45d88a7465e68000f54055c","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"816521ebaad5add9cb96bb88c577394e2938c40b":["1455c941cc4ce652efc776fc23471b0e499246f6","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"849494cf2f3a96af5c8c84995108ddd8456fcd04":["37a0f60745e53927c4c876cfe5b5a58170f0646c","f796095dfe542c48b2e9d89c8bfb9e6b64c70d50"],"6afb0ba86024b96e8b34cfc2e15562239dc36360":["740d649f013f07efbeb73ca854f106c60166e7c0"],"70505a4870244b22d4d0f1a98951b08b197d5bb0":["6f3c1f22c5fe0011e187dac3151422365ae857f3"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["f1fd096d09854ad0876c8474505b917b7fc807ee"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["54ca69905c5d9d1529286f06ab1d12c68f6c13cb","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b70042a8a492f7054d480ccdd2be9796510d4327","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}