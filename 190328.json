{"path":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","commits":[{"id":"eda61b1e90b490cc5837200e04c02639a0d272c7","date":1358795519,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"07155cdd910937cdf6877e48884d5782845c8b8b","date":1358796205,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"/dev/null","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b88448324d3a96c5842455dabea63450b697b58f","date":1421779050,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9","date":1481155163,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.getBytes(), 0, termSuffixes.getPosition(), vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.getBytes(), 0, termSuffixes.getPosition(), vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.bytes, 0, termSuffixes.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.length = 0;\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"409da428f28953cf35fddd5c9ff5c7e4f5439863","date":1547556145,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      //\n      // TODO: We could compress in the slices we already have in the buffer (min/max slice\n      // can be set on the buffer itself).\n      byte[] content = termSuffixes.toArrayCopy();\n      compressor.compress(content, 0, content.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n    numChunks++;\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      compressor.compress(termSuffixes.getBytes(), 0, termSuffixes.getPosition(), vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45264aed0cfa8a8a55ae1292b0e336d29cd88401","date":1600361948,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","pathOld":"lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter#flush().mjava","sourceNew":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      //\n      // TODO: We could compress in the slices we already have in the buffer (min/max slice\n      // can be set on the buffer itself).\n      byte[] content = termSuffixes.toArrayCopy();\n      compressor.compress(content, 0, content.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n  }\n\n","sourceOld":"  private void flush() throws IOException {\n    final int chunkDocs = pendingDocs.size();\n    assert chunkDocs > 0 : chunkDocs;\n\n    // write the index file\n    indexWriter.writeIndex(chunkDocs, vectorsStream.getFilePointer());\n\n    final int docBase = numDocs - chunkDocs;\n    vectorsStream.writeVInt(docBase);\n    vectorsStream.writeVInt(chunkDocs);\n\n    // total number of fields of the chunk\n    final int totalFields = flushNumFields(chunkDocs);\n\n    if (totalFields > 0) {\n      // unique field numbers (sorted)\n      final int[] fieldNums = flushFieldNums();\n      // offsets in the array of unique field numbers\n      flushFields(totalFields, fieldNums);\n      // flags (does the field have positions, offsets, payloads?)\n      flushFlags(totalFields, fieldNums);\n      // number of terms of each field\n      flushNumTerms(totalFields);\n      // prefix and suffix lengths for each field\n      flushTermLengths();\n      // term freqs - 1 (because termFreq is always >=1) for each term\n      flushTermFreqs();\n      // positions for all terms, when enabled\n      flushPositions();\n      // offsets for all terms, when enabled\n      flushOffsets(fieldNums);\n      // payload lengths for all terms, when enabled\n      flushPayloadLengths();\n\n      // compress terms and payloads and write them to the output\n      //\n      // TODO: We could compress in the slices we already have in the buffer (min/max slice\n      // can be set on the buffer itself).\n      byte[] content = termSuffixes.toArrayCopy();\n      compressor.compress(content, 0, content.length, vectorsStream);\n    }\n\n    // reset\n    pendingDocs.clear();\n    curDoc = null;\n    curField = null;\n    termSuffixes.reset();\n    numChunks++;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"409da428f28953cf35fddd5c9ff5c7e4f5439863":["c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"eda61b1e90b490cc5837200e04c02639a0d272c7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"b88448324d3a96c5842455dabea63450b697b58f":["eda61b1e90b490cc5837200e04c02639a0d272c7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["409da428f28953cf35fddd5c9ff5c7e4f5439863"],"9856095f7afb5a607bf5e65077615ed91273508c":["b88448324d3a96c5842455dabea63450b697b58f","c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"07155cdd910937cdf6877e48884d5782845c8b8b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","eda61b1e90b490cc5837200e04c02639a0d272c7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["b88448324d3a96c5842455dabea63450b697b58f"]},"commit2Childs":{"409da428f28953cf35fddd5c9ff5c7e4f5439863":["45264aed0cfa8a8a55ae1292b0e336d29cd88401"],"eda61b1e90b490cc5837200e04c02639a0d272c7":["b88448324d3a96c5842455dabea63450b697b58f","07155cdd910937cdf6877e48884d5782845c8b8b"],"b88448324d3a96c5842455dabea63450b697b58f":["9856095f7afb5a607bf5e65077615ed91273508c","c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["eda61b1e90b490cc5837200e04c02639a0d272c7","07155cdd910937cdf6877e48884d5782845c8b8b"],"45264aed0cfa8a8a55ae1292b0e336d29cd88401":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"07155cdd910937cdf6877e48884d5782845c8b8b":[],"c4ad863d796f4e72a3a1ef4bacd2e19c3e9258c9":["409da428f28953cf35fddd5c9ff5c7e4f5439863","9856095f7afb5a607bf5e65077615ed91273508c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","07155cdd910937cdf6877e48884d5782845c8b8b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}