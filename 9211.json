{"path":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","commits":[{"id":"44ca189138a5b6e1989d12ab992fab60e235ddc7","date":1549051496,"type":0,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Create a blook pool slice\n   * @param bpid Block pool Id\n   * @param volume {@link FsVolumeImpl} to which this BlockPool belongs to\n   * @param bpDir directory corresponding to the BlockPool\n   * @param conf configuration\n   * @param timer include methods for getting time\n   * @throws IOException Error making directories\n   */\n  BlockPoolSlice(String bpid, FsVolumeImpl volume, File bpDir,\n                 Configuration conf, Timer timer) throws IOException {\n    this.bpid = bpid;\n    this.volume = volume;\n    this.fileIoProvider = volume.getFileIoProvider();\n    this.currentDir = new File(bpDir, DataStorage.STORAGE_DIR_CURRENT);\n    this.finalizedDir = new File(\n        currentDir, DataStorage.STORAGE_DIR_FINALIZED);\n    this.lazypersistDir = new File(currentDir, DataStorage.STORAGE_DIR_LAZY_PERSIST);\n    if (!this.finalizedDir.exists()) {\n      if (!this.finalizedDir.mkdirs()) {\n        throw new IOException(\"Failed to mkdirs \" + this.finalizedDir);\n      }\n    }\n\n    this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(conf);\n\n    this.deleteDuplicateReplicas = conf.getBoolean(\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION_DEFAULT);\n\n    this.cachedDfsUsedCheckTime =\n        conf.getLong(\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_MS,\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_DEFAULT_MS);\n\n    this.maxDataLength = conf.getInt(\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);\n\n    this.timer = timer;\n\n    // Files that were being written when the datanode was last shutdown\n    // are now moved back to the data directory. It is possible that\n    // in the future, we might want to do some sort of datanode-local\n    // recovery for these blocks. For example, crc validation.\n    //\n    this.tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);\n    if (tmpDir.exists()) {\n      fileIoProvider.fullyDelete(volume, tmpDir);\n    }\n    this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);\n\n    // create the rbw and tmp directories if they don't exist.\n    fileIoProvider.mkdirs(volume, rbwDir);\n    fileIoProvider.mkdirs(volume, tmpDir);\n\n    // Use cached value initially if available. Or the following call will\n    // block until the initial du command completes.\n    this.dfsUsage = new CachingGetSpaceUsed.Builder().setPath(bpDir)\n        .setConf(conf)\n        .setInitialUsed(loadDfsUsed())\n        .build();\n    if (addReplicaThreadPool == null) {\n      // initialize add replica fork join pool\n      initializeAddReplicaPool(conf);\n    }\n    // Make the dfs usage to be saved during shutdown.\n    shutdownHook = new Runnable() {\n      @Override\n      public void run() {\n        if (!dfsUsedSaved) {\n          saveDfsUsed();\n          addReplicaThreadPool.shutdownNow();\n        }\n      }\n    };\n    ShutdownHookManager.get().addShutdownHook(shutdownHook,\n        SHUTDOWN_HOOK_PRIORITY);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bdf107cf16be0f22504ae184fed81596665a244","date":1576012524,"type":3,"author":"Kevin Risden","isMerge":false,"pathNew":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","pathOld":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","sourceNew":"  /**\n   * Create a blook pool slice\n   * @param bpid Block pool Id\n   * @param volume {@link FsVolumeImpl} to which this BlockPool belongs to\n   * @param bpDir directory corresponding to the BlockPool\n   * @param conf configuration\n   * @param timer include methods for getting time\n   * @throws IOException Error making directories\n   */\n  BlockPoolSlice(String bpid, FsVolumeImpl volume, File bpDir,\n                 Configuration conf, Timer timer) throws IOException {\n    this.bpid = bpid;\n    this.volume = volume;\n    this.fileIoProvider = volume.getFileIoProvider();\n    this.currentDir = new File(bpDir, DataStorage.STORAGE_DIR_CURRENT);\n    this.finalizedDir = new File(\n        currentDir, DataStorage.STORAGE_DIR_FINALIZED);\n    this.lazypersistDir = new File(currentDir, DataStorage.STORAGE_DIR_LAZY_PERSIST);\n    if (!this.finalizedDir.exists()) {\n      if (!this.finalizedDir.mkdirs()) {\n        throw new IOException(\"Failed to mkdirs \" + this.finalizedDir);\n      }\n    }\n\n    this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(conf);\n\n    this.deleteDuplicateReplicas = conf.getBoolean(\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION_DEFAULT);\n\n    this.cachedDfsUsedCheckTime =\n        conf.getLong(\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_MS,\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_DEFAULT_MS);\n\n    this.maxDataLength = conf.getInt(\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);\n\n    this.timer = timer;\n\n    // Files that were being written when the datanode was last shutdown\n    // are now moved back to the data directory. It is possible that\n    // in the future, we might want to do some sort of datanode-local\n    // recovery for these blocks. For example, crc validation.\n    //\n    this.tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);\n    if (tmpDir.exists()) {\n      fileIoProvider.fullyDelete(volume, tmpDir);\n    }\n    this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);\n\n    // create the rbw and tmp directories if they don't exist.\n    fileIoProvider.mkdirs(volume, rbwDir);\n    fileIoProvider.mkdirs(volume, tmpDir);\n\n    if (addReplicaThreadPool == null) {\n      // initialize add replica fork join pool\n      initializeAddReplicaPool(conf);\n    }\n    // Make the dfs usage to be saved during shutdown.\n    shutdownHook = new Runnable() {\n      @Override\n      public void run() {\n        addReplicaThreadPool.shutdownNow();\n      }\n    };\n    ShutdownHookManager.get().addShutdownHook(shutdownHook,\n        SHUTDOWN_HOOK_PRIORITY);\n  }\n\n","sourceOld":"  /**\n   * Create a blook pool slice\n   * @param bpid Block pool Id\n   * @param volume {@link FsVolumeImpl} to which this BlockPool belongs to\n   * @param bpDir directory corresponding to the BlockPool\n   * @param conf configuration\n   * @param timer include methods for getting time\n   * @throws IOException Error making directories\n   */\n  BlockPoolSlice(String bpid, FsVolumeImpl volume, File bpDir,\n                 Configuration conf, Timer timer) throws IOException {\n    this.bpid = bpid;\n    this.volume = volume;\n    this.fileIoProvider = volume.getFileIoProvider();\n    this.currentDir = new File(bpDir, DataStorage.STORAGE_DIR_CURRENT);\n    this.finalizedDir = new File(\n        currentDir, DataStorage.STORAGE_DIR_FINALIZED);\n    this.lazypersistDir = new File(currentDir, DataStorage.STORAGE_DIR_LAZY_PERSIST);\n    if (!this.finalizedDir.exists()) {\n      if (!this.finalizedDir.mkdirs()) {\n        throw new IOException(\"Failed to mkdirs \" + this.finalizedDir);\n      }\n    }\n\n    this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(conf);\n\n    this.deleteDuplicateReplicas = conf.getBoolean(\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION_DEFAULT);\n\n    this.cachedDfsUsedCheckTime =\n        conf.getLong(\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_MS,\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_DEFAULT_MS);\n\n    this.maxDataLength = conf.getInt(\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);\n\n    this.timer = timer;\n\n    // Files that were being written when the datanode was last shutdown\n    // are now moved back to the data directory. It is possible that\n    // in the future, we might want to do some sort of datanode-local\n    // recovery for these blocks. For example, crc validation.\n    //\n    this.tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);\n    if (tmpDir.exists()) {\n      fileIoProvider.fullyDelete(volume, tmpDir);\n    }\n    this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);\n\n    // create the rbw and tmp directories if they don't exist.\n    fileIoProvider.mkdirs(volume, rbwDir);\n    fileIoProvider.mkdirs(volume, tmpDir);\n\n    // Use cached value initially if available. Or the following call will\n    // block until the initial du command completes.\n    this.dfsUsage = new CachingGetSpaceUsed.Builder().setPath(bpDir)\n        .setConf(conf)\n        .setInitialUsed(loadDfsUsed())\n        .build();\n    if (addReplicaThreadPool == null) {\n      // initialize add replica fork join pool\n      initializeAddReplicaPool(conf);\n    }\n    // Make the dfs usage to be saved during shutdown.\n    shutdownHook = new Runnable() {\n      @Override\n      public void run() {\n        if (!dfsUsedSaved) {\n          saveDfsUsed();\n          addReplicaThreadPool.shutdownNow();\n        }\n      }\n    };\n    ShutdownHookManager.get().addShutdownHook(shutdownHook,\n        SHUTDOWN_HOOK_PRIORITY);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a229cb50768e988c50a2106bdae3a92154f428bf","date":1576051038,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","pathOld":"solr/core/src/test/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice#BlockPoolSlice(String,FsVolumeImpl,File,Configuration,Timer).mjava","sourceNew":"  /**\n   * Create a blook pool slice\n   * @param bpid Block pool Id\n   * @param volume {@link FsVolumeImpl} to which this BlockPool belongs to\n   * @param bpDir directory corresponding to the BlockPool\n   * @param conf configuration\n   * @param timer include methods for getting time\n   * @throws IOException Error making directories\n   */\n  BlockPoolSlice(String bpid, FsVolumeImpl volume, File bpDir,\n                 Configuration conf, Timer timer) throws IOException {\n    this.bpid = bpid;\n    this.volume = volume;\n    this.fileIoProvider = volume.getFileIoProvider();\n    this.currentDir = new File(bpDir, DataStorage.STORAGE_DIR_CURRENT);\n    this.finalizedDir = new File(\n        currentDir, DataStorage.STORAGE_DIR_FINALIZED);\n    this.lazypersistDir = new File(currentDir, DataStorage.STORAGE_DIR_LAZY_PERSIST);\n    if (!this.finalizedDir.exists()) {\n      if (!this.finalizedDir.mkdirs()) {\n        throw new IOException(\"Failed to mkdirs \" + this.finalizedDir);\n      }\n    }\n\n    this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(conf);\n\n    this.deleteDuplicateReplicas = conf.getBoolean(\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION_DEFAULT);\n\n    this.cachedDfsUsedCheckTime =\n        conf.getLong(\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_MS,\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_DEFAULT_MS);\n\n    this.maxDataLength = conf.getInt(\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);\n\n    this.timer = timer;\n\n    // Files that were being written when the datanode was last shutdown\n    // are now moved back to the data directory. It is possible that\n    // in the future, we might want to do some sort of datanode-local\n    // recovery for these blocks. For example, crc validation.\n    //\n    this.tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);\n    if (tmpDir.exists()) {\n      fileIoProvider.fullyDelete(volume, tmpDir);\n    }\n    this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);\n\n    // create the rbw and tmp directories if they don't exist.\n    fileIoProvider.mkdirs(volume, rbwDir);\n    fileIoProvider.mkdirs(volume, tmpDir);\n\n    if (addReplicaThreadPool == null) {\n      // initialize add replica fork join pool\n      initializeAddReplicaPool(conf);\n    }\n    // Make the dfs usage to be saved during shutdown.\n    shutdownHook = new Runnable() {\n      @Override\n      public void run() {\n        addReplicaThreadPool.shutdownNow();\n      }\n    };\n    ShutdownHookManager.get().addShutdownHook(shutdownHook,\n        SHUTDOWN_HOOK_PRIORITY);\n  }\n\n","sourceOld":"  /**\n   * Create a blook pool slice\n   * @param bpid Block pool Id\n   * @param volume {@link FsVolumeImpl} to which this BlockPool belongs to\n   * @param bpDir directory corresponding to the BlockPool\n   * @param conf configuration\n   * @param timer include methods for getting time\n   * @throws IOException Error making directories\n   */\n  BlockPoolSlice(String bpid, FsVolumeImpl volume, File bpDir,\n                 Configuration conf, Timer timer) throws IOException {\n    this.bpid = bpid;\n    this.volume = volume;\n    this.fileIoProvider = volume.getFileIoProvider();\n    this.currentDir = new File(bpDir, DataStorage.STORAGE_DIR_CURRENT);\n    this.finalizedDir = new File(\n        currentDir, DataStorage.STORAGE_DIR_FINALIZED);\n    this.lazypersistDir = new File(currentDir, DataStorage.STORAGE_DIR_LAZY_PERSIST);\n    if (!this.finalizedDir.exists()) {\n      if (!this.finalizedDir.mkdirs()) {\n        throw new IOException(\"Failed to mkdirs \" + this.finalizedDir);\n      }\n    }\n\n    this.ioFileBufferSize = DFSUtilClient.getIoFileBufferSize(conf);\n\n    this.deleteDuplicateReplicas = conf.getBoolean(\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,\n        DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION_DEFAULT);\n\n    this.cachedDfsUsedCheckTime =\n        conf.getLong(\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_MS,\n            DFSConfigKeys.DFS_DN_CACHED_DFSUSED_CHECK_INTERVAL_DEFAULT_MS);\n\n    this.maxDataLength = conf.getInt(\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH,\n        CommonConfigurationKeys.IPC_MAXIMUM_DATA_LENGTH_DEFAULT);\n\n    this.timer = timer;\n\n    // Files that were being written when the datanode was last shutdown\n    // are now moved back to the data directory. It is possible that\n    // in the future, we might want to do some sort of datanode-local\n    // recovery for these blocks. For example, crc validation.\n    //\n    this.tmpDir = new File(bpDir, DataStorage.STORAGE_DIR_TMP);\n    if (tmpDir.exists()) {\n      fileIoProvider.fullyDelete(volume, tmpDir);\n    }\n    this.rbwDir = new File(currentDir, DataStorage.STORAGE_DIR_RBW);\n\n    // create the rbw and tmp directories if they don't exist.\n    fileIoProvider.mkdirs(volume, rbwDir);\n    fileIoProvider.mkdirs(volume, tmpDir);\n\n    // Use cached value initially if available. Or the following call will\n    // block until the initial du command completes.\n    this.dfsUsage = new CachingGetSpaceUsed.Builder().setPath(bpDir)\n        .setConf(conf)\n        .setInitialUsed(loadDfsUsed())\n        .build();\n    if (addReplicaThreadPool == null) {\n      // initialize add replica fork join pool\n      initializeAddReplicaPool(conf);\n    }\n    // Make the dfs usage to be saved during shutdown.\n    shutdownHook = new Runnable() {\n      @Override\n      public void run() {\n        if (!dfsUsedSaved) {\n          saveDfsUsed();\n          addReplicaThreadPool.shutdownNow();\n        }\n      }\n    };\n    ShutdownHookManager.get().addShutdownHook(shutdownHook,\n        SHUTDOWN_HOOK_PRIORITY);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a229cb50768e988c50a2106bdae3a92154f428bf":["44ca189138a5b6e1989d12ab992fab60e235ddc7","6bdf107cf16be0f22504ae184fed81596665a244"],"6bdf107cf16be0f22504ae184fed81596665a244":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["6bdf107cf16be0f22504ae184fed81596665a244"]},"commit2Childs":{"a229cb50768e988c50a2106bdae3a92154f428bf":[],"6bdf107cf16be0f22504ae184fed81596665a244":["a229cb50768e988c50a2106bdae3a92154f428bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["44ca189138a5b6e1989d12ab992fab60e235ddc7"],"44ca189138a5b6e1989d12ab992fab60e235ddc7":["a229cb50768e988c50a2106bdae3a92154f428bf","6bdf107cf16be0f22504ae184fed81596665a244"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a229cb50768e988c50a2106bdae3a92154f428bf","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}