{"path":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","commits":[{"id":"7a02003eb48495b52b8483703e1b6b48c099ec7d","date":1438767858,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"/dev/null","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                final long hash = sdv.valueAt(i);\n                final double lon = GeoUtils.mortonUnhashLon(hash);\n                final double lat = GeoUtils.mortonUnhashLat(hash);\n                if (termsEnum.postFilter(lon, lat)) {\n                  builder.add(docId);\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["0f01ec580f0443437a320e1e34902e33f38c5720","0f01ec580f0443437a320e1e34902e33f38c5720"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"404bbb5c1692276fefc358d0d4a9ccb74ed2518e","date":1451595423,"type":3,"author":"Nick Knize","isMerge":false,"pathNew":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                final long hash = sdv.valueAt(i);\n                final double lon = GeoUtils.mortonUnhashLon(hash);\n                final double lat = GeoUtils.mortonUnhashLat(hash);\n                if (termsEnum.postFilter(lon, lat)) {\n                  builder.add(docId);\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":["0f01ec580f0443437a320e1e34902e33f38c5720","0f01ec580f0443437a320e1e34902e33f38c5720"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"937923083e4d137932336fc80f3d78758ff698a6","date":1454691519,"type":5,"author":"nknize","isMerge":false,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":["0f01ec580f0443437a320e1e34902e33f38c5720","0f01ec580f0443437a320e1e34902e33f38c5720"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1e6acbaae7af722f17204ceccf0f7db5753eccf3","date":1454775255,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/spatial/src/java/org/apache/lucene/spatial/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","pathOld":"lucene/sandbox/src/java/org/apache/lucene/search/GeoPointTermQueryConstantScoreWrapper#createWeight(IndexSearcher,boolean).mjava","sourceNew":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.getField());\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms, null));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.getField());\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(mortonUnhashLon(hash), mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","sourceOld":"  @Override\n  public Weight createWeight(IndexSearcher searcher, boolean needsScores) throws IOException {\n    return new ConstantScoreWeight(this) {\n\n      private DocIdSet getDocIDs(LeafReaderContext context) throws IOException {\n        final Terms terms = context.reader().terms(query.field);\n        if (terms == null) {\n          return DocIdSet.EMPTY;\n        }\n\n        final GeoPointTermsEnum termsEnum = (GeoPointTermsEnum)(query.getTermsEnum(terms));\n        assert termsEnum != null;\n\n        LeafReader reader = context.reader();\n        DocIdSetBuilder builder = new DocIdSetBuilder(reader.maxDoc());\n        PostingsEnum docs = null;\n        SortedNumericDocValues sdv = reader.getSortedNumericDocValues(query.field);\n\n        while (termsEnum.next() != null) {\n          docs = termsEnum.postings(docs, PostingsEnum.NONE);\n          // boundary terms need post filtering by\n          if (termsEnum.boundaryTerm()) {\n            int docId = docs.nextDoc();\n            long hash;\n            do {\n              sdv.setDocument(docId);\n              for (int i=0; i<sdv.count(); ++i) {\n                hash = sdv.valueAt(i);\n                if (termsEnum.postFilter(GeoUtils.mortonUnhashLon(hash), GeoUtils.mortonUnhashLat(hash))) {\n                  builder.add(docId);\n                  break;\n                }\n              }\n            } while ((docId = docs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS);\n          } else {\n            builder.add(docs);\n          }\n        }\n\n        return builder.build();\n      }\n\n      private Scorer scorer(DocIdSet set) throws IOException {\n        if (set == null) {\n          return null;\n        }\n        final DocIdSetIterator disi = set.iterator();\n        if (disi == null) {\n          return null;\n        }\n        return new ConstantScoreScorer(this, score(), disi);\n      }\n\n      @Override\n      public BulkScorer bulkScorer(LeafReaderContext context) throws IOException {\n        final Scorer scorer = scorer(getDocIDs(context));\n        if (scorer == null) {\n          return null;\n        }\n        return new DefaultBulkScorer(scorer);\n      }\n\n      @Override\n      public Scorer scorer(LeafReaderContext context) throws IOException {\n        return scorer(getDocIDs(context));\n      }\n    };\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"404bbb5c1692276fefc358d0d4a9ccb74ed2518e":["7a02003eb48495b52b8483703e1b6b48c099ec7d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":["404bbb5c1692276fefc358d0d4a9ccb74ed2518e","937923083e4d137932336fc80f3d78758ff698a6"],"937923083e4d137932336fc80f3d78758ff698a6":["404bbb5c1692276fefc358d0d4a9ccb74ed2518e"],"7a02003eb48495b52b8483703e1b6b48c099ec7d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["937923083e4d137932336fc80f3d78758ff698a6"]},"commit2Childs":{"404bbb5c1692276fefc358d0d4a9ccb74ed2518e":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","937923083e4d137932336fc80f3d78758ff698a6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7a02003eb48495b52b8483703e1b6b48c099ec7d"],"1e6acbaae7af722f17204ceccf0f7db5753eccf3":[],"937923083e4d137932336fc80f3d78758ff698a6":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"7a02003eb48495b52b8483703e1b6b48c099ec7d":["404bbb5c1692276fefc358d0d4a9ccb74ed2518e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["1e6acbaae7af722f17204ceccf0f7db5753eccf3","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}