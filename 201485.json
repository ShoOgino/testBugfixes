{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","commits":[{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"/dev/null","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":0,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"/dev/null","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cca56866c19997e28ef073622656669c15210540","date":1307449014,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.NO, Index.NOT_ANALYZED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"/dev/null","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.NO, Index.NOT_ANALYZED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.NO, Index.NOT_ANALYZED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.YES, Index.ANALYZED, TermVector.WITH_POSITIONS_OFFSETS));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","sourceNew":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", StringField.TYPE_UNSTORED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", Store.NO, Index.NOT_ANALYZED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterForceMerge#testBackgroundForceMerge().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterOptimize#testBackgroundOptimize().mjava","sourceNew":"  // Test calling forceMerge(1, false) whereby forceMerge is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundForceMerge() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", StringField.TYPE_UNSTORED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.forceMerge(1, false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertEquals(1, reader.getSequentialSubReaders().length);\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the merging\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.getSequentialSubReaders().length > 1);\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","sourceOld":"  // Test calling optimize(false) whereby optimize is kicked\n  // off but we don't wait for it to finish (but\n  // writer.close()) does wait\n  public void testBackgroundOptimize() throws IOException {\n\n    Directory dir = newDirectory();\n    for(int pass=0;pass<2;pass++) {\n      IndexWriter writer = new IndexWriter(\n          dir,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.CREATE).\n              setMaxBufferedDocs(2).\n              setMergePolicy(newLogMergePolicy(51))\n      );\n      Document doc = new Document();\n      doc.add(newField(\"field\", \"aaa\", StringField.TYPE_UNSTORED));\n      for(int i=0;i<100;i++)\n        writer.addDocument(doc);\n      writer.optimize(false);\n\n      if (0 == pass) {\n        writer.close();\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(reader.isOptimized());\n        reader.close();\n      } else {\n        // Get another segment to flush so we can verify it is\n        // NOT included in the optimization\n        writer.addDocument(doc);\n        writer.addDocument(doc);\n        writer.close();\n\n        IndexReader reader = IndexReader.open(dir, true);\n        assertTrue(!reader.isOptimized());\n        reader.close();\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(dir);\n        assertEquals(2, infos.size());\n      }\n    }\n\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["79c2cb24929f2649a8875fb629086171f914d5ce","cca56866c19997e28ef073622656669c15210540"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","cca56866c19997e28ef073622656669c15210540"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["cca56866c19997e28ef073622656669c15210540"],"cca56866c19997e28ef073622656669c15210540":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"]},"commit2Childs":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":[],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["79c2cb24929f2649a8875fb629086171f914d5ce","cca56866c19997e28ef073622656669c15210540"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"],"cca56866c19997e28ef073622656669c15210540":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}