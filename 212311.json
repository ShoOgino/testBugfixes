{"path":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","commits":[{"id":"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","date":1399816179,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\");\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    FieldCache.Doubles doubles = cache.getDoubles(reader, \"theDouble\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getDoubles(reader, \"theDouble\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", doubles, cache.getDoubles(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(doubles.get(i) + \" does not equal: \" + (Double.MAX_VALUE - i), doubles.get(i) == (Double.MAX_VALUE - i));\n    }\n    \n    FieldCache.Longs longs = cache.getLongs(reader, \"theLong\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getLongs(reader, \"theLong\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", longs, cache.getLongs(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(longs.get(i) + \" does not equal: \" + (Long.MAX_VALUE - i) + \" i=\" + i, longs.get(i) == (Long.MAX_VALUE - i));\n    }\n\n    FieldCache.Ints ints = cache.getInts(reader, \"theInt\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getInts(reader, \"theInt\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", ints, cache.getInts(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(ints.get(i) + \" does not equal: \" + (Integer.MAX_VALUE - i), ints.get(i) == (Integer.MAX_VALUE - i));\n    }\n    \n    FieldCache.Floats floats = cache.getFloats(reader, \"theFloat\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getFloats(reader, \"theFloat\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", floats, cache.getFloats(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(floats.get(i) + \" does not equal: \" + (Float.MAX_VALUE - i), floats.get(i) == (Float.MAX_VALUE - i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\");\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"36a4a25c2f02edc0c96bc51a298c45b1764e662d","date":1399923529,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\");\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":1,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    FieldCache.Doubles doubles = cache.getDoubles(reader, \"theDouble\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getDoubles(reader, \"theDouble\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", doubles, cache.getDoubles(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(doubles.get(i) + \" does not equal: \" + (Double.MAX_VALUE - i), doubles.get(i) == (Double.MAX_VALUE - i));\n    }\n    \n    FieldCache.Longs longs = cache.getLongs(reader, \"theLong\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getLongs(reader, \"theLong\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", longs, cache.getLongs(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(longs.get(i) + \" does not equal: \" + (Long.MAX_VALUE - i) + \" i=\" + i, longs.get(i) == (Long.MAX_VALUE - i));\n    }\n\n    FieldCache.Ints ints = cache.getInts(reader, \"theInt\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getInts(reader, \"theInt\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", ints, cache.getInts(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(ints.get(i) + \" does not equal: \" + (Integer.MAX_VALUE - i), ints.get(i) == (Integer.MAX_VALUE - i));\n    }\n    \n    FieldCache.Floats floats = cache.getFloats(reader, \"theFloat\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getFloats(reader, \"theFloat\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", floats, cache.getFloats(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(floats.get(i) + \" does not equal: \" + (Float.MAX_VALUE - i), floats.get(i) == (Float.MAX_VALUE - i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\");\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":1,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    FieldCache.Doubles doubles = cache.getDoubles(reader, \"theDouble\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getDoubles(reader, \"theDouble\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", doubles, cache.getDoubles(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(doubles.get(i) + \" does not equal: \" + (Double.MAX_VALUE - i), doubles.get(i) == (Double.MAX_VALUE - i));\n    }\n    \n    FieldCache.Longs longs = cache.getLongs(reader, \"theLong\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getLongs(reader, \"theLong\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", longs, cache.getLongs(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(longs.get(i) + \" does not equal: \" + (Long.MAX_VALUE - i) + \" i=\" + i, longs.get(i) == (Long.MAX_VALUE - i));\n    }\n\n    FieldCache.Ints ints = cache.getInts(reader, \"theInt\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getInts(reader, \"theInt\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", ints, cache.getInts(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(ints.get(i) + \" does not equal: \" + (Integer.MAX_VALUE - i), ints.get(i) == (Integer.MAX_VALUE - i));\n    }\n    \n    FieldCache.Floats floats = cache.getFloats(reader, \"theFloat\", random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getFloats(reader, \"theFloat\", random().nextBoolean()));\n    assertSame(\"Second request with explicit parser return same array\", floats, cache.getFloats(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertTrue(floats.get(i) + \" does not equal: \" + (Float.MAX_VALUE - i), floats.get(i) == (Float.MAX_VALUE - i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\");\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\");\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    assertSame(\"Second request to cache return same array\", termsIndex, cache.getTermsIndex(reader, \"theRandomUnicodeString\"));\n    final BytesRef br = new BytesRef();\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final BytesRef term;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        term = null;\n      } else {\n        termsIndex.lookupOrd(ord, br);\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    BytesRef val = new BytesRef();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = tenum.next();\n      termsIndex.lookupOrd(i, val);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      termsIndex.lookupOrd(k, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      termsIndex.lookupOrd(i, val);\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      terms.get(i, br);\n      final BytesRef term;\n      if (!bits.get(i)) {\n        term = null;\n      } else {\n        term = br;\n      }\n      final String s = term == null ? null : term.utf8ToString();\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = new BytesRef();\n        termOrds.lookupOrd(ord, scratch);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31ba6411a5c9d154ee6ccb3cecdc8ad6acfceac5","date":1401991635,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    assertSame(\"Second request to cache return same array\", terms, cache.getTerms(reader, \"theRandomUnicodeString\", true));\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8c146731a64debc22c115bbf11ee1a060aa7ea02","date":1457616596,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.NUMERIC_UTILS_DOUBLE_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.NUMERIC_UTILS_LONG_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.NUMERIC_UTILS_INT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.NUMERIC_UTILS_FLOAT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\"));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\");\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\"));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","87d6f9603307ae2ad642fb01deedf031320fd0c3","382fe3a6ca9745891afebda9b9a57cc158305545"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a076c3c721f685b7559308fdc2cd72d91bba67e5","date":1464168992,"type":5,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0e121d43b5a10f2df530f406f935102656e9c4e8","date":1464198131,"type":5,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83870855d82aba6819217abeff5a40779dbb28b4","date":1464291012,"type":5,"author":"Mike McCandless","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/uninverting/TestFieldCache#test().mjava","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":4,"author":"Kevin Risden","isMerge":true,"pathNew":"/dev/null","pathOld":"lucene/misc/src/test/org/apache/lucene/uninverting/TestFieldCache#test().mjava","sourceNew":null,"sourceOld":"  public void test() throws IOException {\n    FieldCache cache = FieldCache.DEFAULT;\n    NumericDocValues doubles = cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", doubles, cache.getNumerics(reader, \"theDouble\", FieldCache.DOUBLE_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Double.doubleToLongBits(Double.MAX_VALUE - i), doubles.get(i));\n    }\n    \n    NumericDocValues longs = cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", longs, cache.getNumerics(reader, \"theLong\", FieldCache.LONG_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Long.MAX_VALUE - i, longs.get(i));\n    }\n\n    NumericDocValues ints = cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", ints, cache.getNumerics(reader, \"theInt\", FieldCache.INT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Integer.MAX_VALUE - i, ints.get(i));\n    }\n    \n    NumericDocValues floats = cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean());\n    assertSame(\"Second request to cache return same array\", floats, cache.getNumerics(reader, \"theFloat\", FieldCache.FLOAT_POINT_PARSER, random().nextBoolean()));\n    for (int i = 0; i < NUM_DOCS; i++) {\n      assertEquals(Float.floatToIntBits(Float.MAX_VALUE - i), floats.get(i));\n    }\n\n    Bits docsWithField = cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"theLong\", FieldCache.LONG_POINT_PARSER));\n    assertTrue(\"docsWithField(theLong) must be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(theLong) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertTrue(docsWithField.get(i));\n    }\n    \n    docsWithField = cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER);\n    assertSame(\"Second request to cache return same array\", docsWithField, cache.getDocsWithField(reader, \"sparse\", FieldCache.INT_POINT_PARSER));\n    assertFalse(\"docsWithField(sparse) must not be class Bits.MatchAllBits\", docsWithField instanceof Bits.MatchAllBits);\n    assertTrue(\"docsWithField(sparse) Size: \" + docsWithField.length() + \" is not: \" + NUM_DOCS, docsWithField.length() == NUM_DOCS);\n    for (int i = 0; i < docsWithField.length(); i++) {\n      assertEquals(i%2 == 0, docsWithField.get(i));\n    }\n\n    // getTermsIndex\n    SortedDocValues termsIndex = cache.getTermsIndex(reader, \"theRandomUnicodeString\");\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      final int ord = termsIndex.getOrd(i);\n      if (ord == -1) {\n        s = null;\n      } else {\n        s = termsIndex.lookupOrd(ord).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    int nTerms = termsIndex.getValueCount();\n\n    TermsEnum tenum = termsIndex.termsEnum();\n    for (int i=0; i<nTerms; i++) {\n      BytesRef val1 = BytesRef.deepCopyOf(tenum.next());\n      final BytesRef val = termsIndex.lookupOrd(i);\n      // System.out.println(\"i=\"+i);\n      assertEquals(val, val1);\n    }\n\n    // seek the enum around (note this isn't a great test here)\n    int num = atLeast(100);\n    for (int i = 0; i < num; i++) {\n      int k = random().nextInt(nTerms);\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(k));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    for(int i=0;i<nTerms;i++) {\n      final BytesRef val = BytesRef.deepCopyOf(termsIndex.lookupOrd(i));\n      assertEquals(TermsEnum.SeekStatus.FOUND, tenum.seekCeil(val));\n      assertEquals(val, tenum.term());\n    }\n\n    // test bad field\n    termsIndex = cache.getTermsIndex(reader, \"bogusfield\");\n\n    // getTerms\n    BinaryDocValues terms = cache.getTerms(reader, \"theRandomUnicodeString\", true);\n    Bits bits = cache.getDocsWithField(reader, \"theRandomUnicodeString\", null);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      final String s;\n      if (!bits.get(i)) {\n        s = null;\n      } else {\n        s = terms.get(i).utf8ToString();\n      }\n      assertTrue(\"for doc \" + i + \": \" + s + \" does not equal: \" + unicodeStrings[i], unicodeStrings[i] == null || unicodeStrings[i].equals(s));\n    }\n\n    // test bad field\n    terms = cache.getTerms(reader, \"bogusfield\", false);\n\n    // getDocTermOrds\n    SortedSetDocValues termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    int numEntries = cache.getCacheEntries().length;\n    // ask for it again, and check that we didnt create any additional entries:\n    termOrds = cache.getDocTermOrds(reader, \"theRandomUnicodeMultiValuedField\", null);\n    assertEquals(numEntries, cache.getCacheEntries().length);\n\n    for (int i = 0; i < NUM_DOCS; i++) {\n      termOrds.setDocument(i);\n      // This will remove identical terms. A DocTermOrds doesn't return duplicate ords for a docId\n      List<BytesRef> values = new ArrayList<>(new LinkedHashSet<>(Arrays.asList(multiValued[i])));\n      for (BytesRef v : values) {\n        if (v == null) {\n          // why does this test use null values... instead of an empty list: confusing\n          break;\n        }\n        long ord = termOrds.nextOrd();\n        assert ord != SortedSetDocValues.NO_MORE_ORDS;\n        BytesRef scratch = termOrds.lookupOrd(ord);\n        assertEquals(v, scratch);\n      }\n      assertEquals(SortedSetDocValues.NO_MORE_ORDS, termOrds.nextOrd());\n    }\n\n    // test bad field\n    termOrds = cache.getDocTermOrds(reader, \"bogusfield\", null);\n    assertTrue(termOrds.getValueCount() == 0);\n\n    FieldCache.DEFAULT.purgeByCacheKey(reader.getCoreCacheKey());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["8c146731a64debc22c115bbf11ee1a060aa7ea02","a076c3c721f685b7559308fdc2cd72d91bba67e5"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"83870855d82aba6819217abeff5a40779dbb28b4":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"56572ec06f1407c066d6b7399413178b33176cd8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","93dd449115a9247533e44bab47e8429e5dccbc6d"],"31ba6411a5c9d154ee6ccb3cecdc8ad6acfceac5":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["8c146731a64debc22c115bbf11ee1a060aa7ea02","0e121d43b5a10f2df530f406f935102656e9c4e8"],"36a4a25c2f02edc0c96bc51a298c45b1764e662d":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","36a4a25c2f02edc0c96bc51a298c45b1764e662d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["31ba6411a5c9d154ee6ccb3cecdc8ad6acfceac5"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0e121d43b5a10f2df530f406f935102656e9c4e8"]},"commit2Childs":{"a076c3c721f685b7559308fdc2cd72d91bba67e5":["0e121d43b5a10f2df530f406f935102656e9c4e8"],"0e121d43b5a10f2df530f406f935102656e9c4e8":["83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["31ba6411a5c9d154ee6ccb3cecdc8ad6acfceac5"],"b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c":["36a4a25c2f02edc0c96bc51a298c45b1764e662d"],"83870855d82aba6819217abeff5a40779dbb28b4":[],"56572ec06f1407c066d6b7399413178b33176cd8":[],"31ba6411a5c9d154ee6ccb3cecdc8ad6acfceac5":["8c146731a64debc22c115bbf11ee1a060aa7ea02"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"36a4a25c2f02edc0c96bc51a298c45b1764e662d":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","56572ec06f1407c066d6b7399413178b33176cd8"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b70a13d2b73512ad6b204e9ad8fe09ffeeda3c2c","56572ec06f1407c066d6b7399413178b33176cd8","93dd449115a9247533e44bab47e8429e5dccbc6d"],"8c146731a64debc22c115bbf11ee1a060aa7ea02":["a076c3c721f685b7559308fdc2cd72d91bba67e5","0e121d43b5a10f2df530f406f935102656e9c4e8","83870855d82aba6819217abeff5a40779dbb28b4","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["83870855d82aba6819217abeff5a40779dbb28b4","56572ec06f1407c066d6b7399413178b33176cd8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}