{"path":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","commits":[{"id":"7b2c4e17100e207bc842e56d016b9f91f411304e","date":1434336696,"type":0,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","pathOld":"/dev/null","sourceNew":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      Comparator<Tuple> comp = bucketSortComp(buckets, sortDirection);\n      tupleStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        Comparator<Tuple> comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0891a6931fc352fc7e61f2752ef9add758d3fb89","date":1435151229,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","sourceNew":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      tupleStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        StreamComparator comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      Comparator<Tuple> comp = bucketSortComp(buckets, sortDirection);\n      tupleStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        Comparator<Tuple> comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3078cad1008b796c6d573b743c586fdf9ef5660a","date":1436019875,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","sourceNew":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n\n      StreamFactory factory = new StreamFactory()\n          .withFunctionName(\"search\", CloudSolrStream.class)\n          .withFunctionName(\"parallel\", ParallelStream.class)\n          .withFunctionName(\"rollup\", RollupStream.class)\n          .withFunctionName(\"sum\", SumMetric.class)\n          .withFunctionName(\"min\", MinMetric.class)\n          .withFunctionName(\"max\", MaxMetric.class)\n          .withFunctionName(\"avg\", MeanMetric.class)\n          .withFunctionName(\"count\", CountMetric.class);\n\n      parallelStream.setStreamFactory(factory);\n      parallelStream.setObjectSerialize(false);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        StreamComparator comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      tupleStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        StreamComparator comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8da9a71da64ce12a97dcfcdd912893aeb1fa2981","date":1437510515,"type":5,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupByWithAggregates(SQLVisitor,int,String,String).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/SQLHandler#doGroupBy(SQLVisitor,Map[String,TableSpec],int,String,String).mjava","sourceNew":"  private static TupleStream doGroupByWithAggregates(SQLVisitor sqlVisitor,\n                                                     int numWorkers,\n                                                     String workerCollection,\n                                                     String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n    if(metrics.length == 0) {\n      throw new IOException(\"Group by queries must include atleast one aggregate function.\");\n    }\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = new TableSpec(sqlVisitor.table, defaultZkhost);\n\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n\n      StreamFactory factory = new StreamFactory()\n          .withFunctionName(\"search\", CloudSolrStream.class)\n          .withFunctionName(\"parallel\", ParallelStream.class)\n          .withFunctionName(\"rollup\", RollupStream.class)\n          .withFunctionName(\"sum\", SumMetric.class)\n          .withFunctionName(\"min\", MinMetric.class)\n          .withFunctionName(\"max\", MaxMetric.class)\n          .withFunctionName(\"avg\", MeanMetric.class)\n          .withFunctionName(\"count\", CountMetric.class);\n\n      parallelStream.setStreamFactory(factory);\n      parallelStream.setObjectSerialize(false);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        StreamComparator comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","sourceOld":"  private static TupleStream doGroupBy(SQLVisitor sqlVisitor,\n                                       Map<String, TableSpec> tableMap,\n                                       int numWorkers,\n                                       String workerCollection,\n                                       String workerZkHost) throws IOException {\n\n    Set<String> fieldSet = new HashSet();\n    Bucket[] buckets = getBuckets(sqlVisitor.groupBy, fieldSet);\n    Metric[] metrics = getMetrics(sqlVisitor.fields, fieldSet);\n\n    String fl = fields(fieldSet);\n    String sortDirection = getSortDirection(sqlVisitor.sorts);\n    String sort = bucketSort(buckets, sortDirection);\n\n    TableSpec tableSpec = tableMap.get(sqlVisitor.table);\n    String zkHost = tableSpec.zkHost;\n    String collection = tableSpec.collection;\n    Map<String, String> params = new HashMap();\n\n    params.put(CommonParams.FL, fl);\n    params.put(CommonParams.Q, sqlVisitor.query);\n    //Always use the /export handler for Group By Queries because it requires exporting full result sets.\n    params.put(CommonParams.QT, \"/export\");\n\n    if(numWorkers > 1) {\n      params.put(\"partitionKeys\", getPartitionKeys(buckets));\n    }\n\n    params.put(\"sort\", sort);\n\n    TupleStream tupleStream = null;\n\n    CloudSolrStream cstream = new CloudSolrStream(zkHost, collection, params);\n    tupleStream = new RollupStream(cstream, buckets, metrics);\n\n    if(numWorkers > 1) {\n      // Do the rollups in parallel\n      // Maintain the sort of the Tuples coming from the workers.\n      StreamComparator comp = bucketSortComp(buckets, sortDirection);\n      ParallelStream parallelStream = new ParallelStream(workerZkHost, workerCollection, tupleStream, numWorkers, comp);\n\n      StreamFactory factory = new StreamFactory()\n          .withFunctionName(\"search\", CloudSolrStream.class)\n          .withFunctionName(\"parallel\", ParallelStream.class)\n          .withFunctionName(\"rollup\", RollupStream.class)\n          .withFunctionName(\"sum\", SumMetric.class)\n          .withFunctionName(\"min\", MinMetric.class)\n          .withFunctionName(\"max\", MaxMetric.class)\n          .withFunctionName(\"avg\", MeanMetric.class)\n          .withFunctionName(\"count\", CountMetric.class);\n\n      parallelStream.setStreamFactory(factory);\n      parallelStream.setObjectSerialize(false);\n      tupleStream = parallelStream;\n    }\n\n    //TODO: This should be done on the workers, but it won't serialize because it relies on Presto classes.\n    // Once we make this a Expressionable the problem will be solved.\n\n    if(sqlVisitor.havingExpression != null) {\n      tupleStream = new HavingStream(tupleStream, sqlVisitor.havingExpression);\n    }\n\n    if(sqlVisitor.sorts != null && sqlVisitor.sorts.size() > 0) {\n      if(!sortsEqual(buckets, sortDirection, sqlVisitor.sorts)) {\n        int limit = sqlVisitor.limit == -1 ? 100 : sqlVisitor.limit;\n        StreamComparator comp = getComp(sqlVisitor.sorts);\n        //Rank the Tuples\n        //If parallel stream is used ALL the Rolled up tuples from the workers will be ranked\n        //Providing a true Top or Bottom.\n        tupleStream = new RankStream(tupleStream, limit, comp);\n      } else {\n        // Sort is the same as the same as the underlying stream\n        // Only need to limit the result, not Rank the result\n        if(sqlVisitor.limit > -1) {\n          tupleStream = new LimitStream(tupleStream, sqlVisitor.limit);\n        }\n      }\n    }\n\n    return tupleStream;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3078cad1008b796c6d573b743c586fdf9ef5660a":["0891a6931fc352fc7e61f2752ef9add758d3fb89"],"0891a6931fc352fc7e61f2752ef9add758d3fb89":["7b2c4e17100e207bc842e56d016b9f91f411304e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7b2c4e17100e207bc842e56d016b9f91f411304e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8da9a71da64ce12a97dcfcdd912893aeb1fa2981":["3078cad1008b796c6d573b743c586fdf9ef5660a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8da9a71da64ce12a97dcfcdd912893aeb1fa2981"]},"commit2Childs":{"3078cad1008b796c6d573b743c586fdf9ef5660a":["8da9a71da64ce12a97dcfcdd912893aeb1fa2981"],"0891a6931fc352fc7e61f2752ef9add758d3fb89":["3078cad1008b796c6d573b743c586fdf9ef5660a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7b2c4e17100e207bc842e56d016b9f91f411304e"],"7b2c4e17100e207bc842e56d016b9f91f411304e":["0891a6931fc352fc7e61f2752ef9add758d3fb89"],"8da9a71da64ce12a97dcfcdd912893aeb1fa2981":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}