{"path":"lucene/core/src/test/org/apache/lucene/util/graph/TestGraphTokenStreamFiniteStrings#assertTokenStream(TokenStream,String[],int[]).mjava","commits":[{"id":"e52c30617998e1bc2a946cc226f52e9b35162b1e","date":1483438353,"type":0,"author":"Matt Weber","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/util/graph/TestGraphTokenStreamFiniteStrings#assertTokenStream(TokenStream,String[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void assertTokenStream(TokenStream ts, String[] terms, int[] increments) throws Exception {\n    // verify no nulls and arrays same length\n    assertNotNull(ts);\n    assertNotNull(terms);\n    assertNotNull(increments);\n    assertEquals(terms.length, increments.length);\n    BytesTermAttribute termAtt = ts.getAttribute(BytesTermAttribute.class);\n    PositionIncrementAttribute incrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    int offset = 0;\n    while (ts.incrementToken()) {\n      // verify term and increment\n      assert offset < terms.length;\n      assertEquals(terms[offset], termAtt.getBytesRef().utf8ToString());\n      assertEquals(increments[offset], incrAtt.getPositionIncrement());\n      offset++;\n    }\n\n    // make sure we processed all items\n    assertEquals(offset, terms.length);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/util/graph/TestGraphTokenStreamFiniteStrings#assertTokenStream(TokenStream,String[],int[]).mjava","pathOld":"/dev/null","sourceNew":"  private void assertTokenStream(TokenStream ts, String[] terms, int[] increments) throws Exception {\n    // verify no nulls and arrays same length\n    assertNotNull(ts);\n    assertNotNull(terms);\n    assertNotNull(increments);\n    assertEquals(terms.length, increments.length);\n    BytesTermAttribute termAtt = ts.getAttribute(BytesTermAttribute.class);\n    PositionIncrementAttribute incrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    int offset = 0;\n    while (ts.incrementToken()) {\n      // verify term and increment\n      assert offset < terms.length;\n      assertEquals(terms[offset], termAtt.getBytesRef().utf8ToString());\n      assertEquals(increments[offset], incrAtt.getPositionIncrement());\n      offset++;\n    }\n\n    // make sure we processed all items\n    assertEquals(offset, terms.length);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2a1183e4d02ec8307ffca25b9879752bea1822f1","date":1563533185,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/util/graph/TestGraphTokenStreamFiniteStrings#assertTokenStream(TokenStream,String[],int[]).mjava","pathOld":"lucene/core/src/test/org/apache/lucene/util/graph/TestGraphTokenStreamFiniteStrings#assertTokenStream(TokenStream,String[],int[]).mjava","sourceNew":"  private void assertTokenStream(TokenStream ts, String[] terms, int[] increments) throws Exception {\n    // verify no nulls and arrays same length\n    assertNotNull(ts);\n    assertNotNull(terms);\n    assertNotNull(increments);\n    assertEquals(terms.length, increments.length);\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    PositionIncrementAttribute incrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    PositionLengthAttribute lenAtt = ts.getAttribute(PositionLengthAttribute.class);\n    int offset = 0;\n    while (ts.incrementToken()) {\n      // verify term and increment\n      assert offset < terms.length;\n      assertEquals(terms[offset], termAtt.toString());\n      assertEquals(increments[offset], incrAtt.getPositionIncrement());\n      assertEquals(1, lenAtt.getPositionLength());  // we always output linear token streams\n      offset++;\n    }\n\n    // make sure we processed all items\n    assertEquals(offset, terms.length);\n  }\n\n","sourceOld":"  private void assertTokenStream(TokenStream ts, String[] terms, int[] increments) throws Exception {\n    // verify no nulls and arrays same length\n    assertNotNull(ts);\n    assertNotNull(terms);\n    assertNotNull(increments);\n    assertEquals(terms.length, increments.length);\n    BytesTermAttribute termAtt = ts.getAttribute(BytesTermAttribute.class);\n    PositionIncrementAttribute incrAtt = ts.getAttribute(PositionIncrementAttribute.class);\n    int offset = 0;\n    while (ts.incrementToken()) {\n      // verify term and increment\n      assert offset < terms.length;\n      assertEquals(terms[offset], termAtt.getBytesRef().utf8ToString());\n      assertEquals(increments[offset], incrAtt.getPositionIncrement());\n      offset++;\n    }\n\n    // make sure we processed all items\n    assertEquals(offset, terms.length);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"e52c30617998e1bc2a946cc226f52e9b35162b1e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2a1183e4d02ec8307ffca25b9879752bea1822f1":["e52c30617998e1bc2a946cc226f52e9b35162b1e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2a1183e4d02ec8307ffca25b9879752bea1822f1"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","e52c30617998e1bc2a946cc226f52e9b35162b1e"]},"commit2Childs":{"e52c30617998e1bc2a946cc226f52e9b35162b1e":["2a1183e4d02ec8307ffca25b9879752bea1822f1","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e52c30617998e1bc2a946cc226f52e9b35162b1e","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"2a1183e4d02ec8307ffca25b9879752bea1822f1":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}