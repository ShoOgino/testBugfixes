{"path":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","commits":[{"id":"893269407f5c988a4b2ee13c7ffc01ec43136c1d","date":1268598046,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","date":1268599006,"type":4,"author":"Mark Robert Miller","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":null,"sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ad94625fb8d088209f46650c8097196fec67f00c","date":1453508319,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1da8d55113b689b06716246649de6f62430f15c0","date":1453508340,"type":0,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"be20f9fed1d3edcb1c84abcc39df87a90fab22df","date":1275590285,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.StringIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4","d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term.utf8ToString());\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term.utf8ToString());\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(tt.term), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(tt.term)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      Double value = Double.parseDouble(ft.indexedToReadable(getTermText(te, i)));\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c220849f876de24a79f756f65b3eb045db59f63f","date":1294902803,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"868da859b43505d9d2a023bfeae6dd0c795f5295","date":1294948401,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"95ae76773bf2b95987d5f9c8f566ab3738953fb4","date":1301758351,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    NumberedTermsEnum te = ti.getEnumerator(searcher.getIndexReader());\n\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(ti.field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n    te.close();\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a1b3a24d5d9b47345473ff564f5cc127a7b526b4","date":1306277076,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","date":1306767085,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2e10cb22a8bdb44339e282925a29182bb2f3174d","date":1306841137,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n\n    // add results in index order\n    CharArr spare = new CharArr();\n\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, spare);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","pathOld":"solr/src/java/org/apache/solr/request/UnInvertedField#getStats(SolrIndexSearcher,DocSet,String[]).mjava","sourceNew":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","sourceOld":"  /**\n   * Collect statistics about the UninvertedField.  Code is very similar to {@link #getCounts(org.apache.solr.search.SolrIndexSearcher, org.apache.solr.search.DocSet, int, int, Integer, boolean, String, String)}\n   * It can be used to calculate stats on multivalued fields.\n   * <p/>\n   * This method is mainly used by the {@link org.apache.solr.handler.component.StatsComponent}.\n   *\n   * @param searcher The Searcher to use to gather the statistics\n   * @param baseDocs The {@link org.apache.solr.search.DocSet} to gather the stats on\n   * @param facet One or more fields to facet on.\n   * @return The {@link org.apache.solr.handler.component.StatsValues} collected\n   * @throws IOException\n   */\n  public StatsValues getStats(SolrIndexSearcher searcher, DocSet baseDocs, String[] facet) throws IOException {\n    //this function is ripped off nearly wholesale from the getCounts function to use\n    //for multiValued fields within the StatsComponent.  may be useful to find common\n    //functionality between the two and refactor code somewhat\n    use.incrementAndGet();\n\n    StatsValues allstats = new StatsValues();\n\n\n    DocSet docs = baseDocs;\n    int baseSize = docs.size();\n    int maxDoc = searcher.maxDoc();\n\n    if (baseSize <= 0) return allstats;\n\n    FieldType ft = searcher.getSchema().getFieldType(field);\n\n    DocSet missing = docs.andNot( searcher.getDocSet(new TermRangeQuery(field, null, null, false, false)) );\n\n    int i = 0;\n    final FieldFacetStats[] finfo = new FieldFacetStats[facet.length];\n    //Initialize facetstats, if facets have been passed in\n    FieldCache.DocTermsIndex si;\n    for (String f : facet) {\n      FieldType facet_ft = searcher.getSchema().getFieldType(f);\n      try {\n        si = FieldCache.DEFAULT.getTermsIndex(searcher.getIndexReader(), f);\n      }\n      catch (IOException e) {\n        throw new RuntimeException(\"failed to open field cache for: \" + f, e);\n      }\n      finfo[i] = new FieldFacetStats(f, si, facet_ft, numTermsInField);\n      i++;\n    }\n\n    final int[] index = this.index;\n    final int[] counts = new int[numTermsInField];//keep track of the number of times we see each word in the field for all the documents in the docset\n\n    TermsEnum te = getOrdTermsEnum(searcher.getIndexReader());\n\n    boolean doNegative = false;\n    if (finfo.length == 0) {\n      //if we're collecting statistics with a facet field, can't do inverted counting\n      doNegative = baseSize > maxDoc >> 1 && termInstances > 0\n              && docs instanceof BitDocSet;\n    }\n\n    if (doNegative) {\n      OpenBitSet bs = (OpenBitSet) ((BitDocSet) docs).getBits().clone();\n      bs.flip(0, maxDoc);\n      // TODO: when iterator across negative elements is available, use that\n      // instead of creating a new bitset and inverting.\n      docs = new BitDocSet(bs, maxDoc - baseSize);\n      // simply negating will mean that we have deleted docs in the set.\n      // that should be OK, as their entries in our table should be empty.\n    }\n\n    // For the biggest terms, do straight set intersections\n    for (TopTerm tt : bigTerms.values()) {\n      // TODO: counts could be deferred if sorted==false\n      if (tt.termNum >= 0 && tt.termNum < numTermsInField) {\n        final Term t = new Term(field, tt.term);\n        if (finfo.length == 0) {\n          counts[tt.termNum] = searcher.numDocs(new TermQuery(t), docs);\n        } else {\n          //COULD BE VERY SLOW\n          //if we're collecting stats for facet fields, we need to iterate on all matching documents\n          DocSet bigTermDocSet = searcher.getDocSet(new TermQuery(t)).intersection(docs);\n          DocIterator iter = bigTermDocSet.iterator();\n          while (iter.hasNext()) {\n            int doc = iter.nextDoc();\n            counts[tt.termNum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tt.termNum);\n            }\n          }\n        }\n      }\n    }\n\n\n    if (termInstances > 0) {\n      DocIterator iter = docs.iterator();\n      while (iter.hasNext()) {\n        int doc = iter.nextDoc();\n        int code = index[doc];\n\n        if ((code & 0xff) == 1) {\n          int pos = code >>> 8;\n          int whichArray = (doc >>> 16) & 0xff;\n          byte[] arr = tnums[whichArray];\n          int tnum = 0;\n          for (; ;) {\n            int delta = 0;\n            for (; ;) {\n              byte b = arr[pos++];\n              delta = (delta << 7) | (b & 0x7f);\n              if ((b & 0x80) == 0) break;\n            }\n            if (delta == 0) break;\n            tnum += delta - TNUM_OFFSET;\n            counts[tnum]++;\n            for (FieldFacetStats f : finfo) {\n              f.facetTermNum(doc, tnum);\n            }\n          }\n        } else {\n          int tnum = 0;\n          int delta = 0;\n          for (; ;) {\n            delta = (delta << 7) | (code & 0x7f);\n            if ((code & 0x80) == 0) {\n              if (delta == 0) break;\n              tnum += delta - TNUM_OFFSET;\n              counts[tnum]++;\n              for (FieldFacetStats f : finfo) {\n                f.facetTermNum(doc, tnum);\n              }\n              delta = 0;\n            }\n            code >>>= 8;\n          }\n        }\n      }\n    }\n    final CharsRef charsRef = new CharsRef();\n    // add results in index order\n    for (i = 0; i < numTermsInField; i++) {\n      int c = doNegative ? maxTermCounts[i] - counts[i] : counts[i];\n      if (c == 0) continue;\n      String label = getReadableValue(getTermValue(te, i), ft, charsRef);\n      // TODO: we should avoid this re-parse\n      Double value = Double.parseDouble(label);\n\n      allstats.accumulate(value, c);\n      //as we've parsed the termnum into a value, lets also accumulate fieldfacet statistics\n      for (FieldFacetStats f : finfo) {\n        f.accumulateTermNum(i, value);\n      }\n    }\n\n    int c = missing.size();\n    allstats.addMissing(c);\n\n    if (finfo.length > 0) {\n      allstats.facets = new HashMap<String, Map<String, StatsValues>>();\n      for (FieldFacetStats f : finfo) {\n        Map<String, StatsValues> facetStatsValues = f.facetStatsValues;\n        FieldType facetType = searcher.getSchema().getFieldType(f.name);\n        for (Map.Entry<String,StatsValues> entry : facetStatsValues.entrySet()) {\n          String termLabel = entry.getKey();\n          int missingCount = searcher.numDocs(new TermQuery(new Term(f.name, facetType.toInternal(termLabel))), missing);\n          entry.getValue().addMissing(missingCount);\n        }\n        allstats.facets.put(f.name, facetStatsValues);\n      }\n    }\n\n    return allstats;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["1da8d55113b689b06716246649de6f62430f15c0"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"c26f00b574427b55127e869b935845554afde1fa":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":[],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"5f4e87790277826a2aea119328600dfb07761f32":["be20f9fed1d3edcb1c84abcc39df87a90fab22df","4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"1da8d55113b689b06716246649de6f62430f15c0":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8","ad94625fb8d088209f46650c8097196fec67f00c"],"c220849f876de24a79f756f65b3eb045db59f63f":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a3776dccca01c11e7046323cfad46a3b4a471233":["c220849f876de24a79f756f65b3eb045db59f63f","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["4f29ba80b723649f5feb7e37afe1a558dd2c1304","c220849f876de24a79f756f65b3eb045db59f63f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a1b3a24d5d9b47345473ff564f5cc127a7b526b4"],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"ad94625fb8d088209f46650c8097196fec67f00c":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["c220849f876de24a79f756f65b3eb045db59f63f"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["5f4e87790277826a2aea119328600dfb07761f32","c220849f876de24a79f756f65b3eb045db59f63f"],"45669a651c970812a680841b97a77cce06af559f":["868da859b43505d9d2a023bfeae6dd0c795f5295","95ae76773bf2b95987d5f9c8f566ab3738953fb4"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"],"2e10cb22a8bdb44339e282925a29182bb2f3174d":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a1b3a24d5d9b47345473ff564f5cc127a7b526b4"]},"commit2Childs":{"be20f9fed1d3edcb1c84abcc39df87a90fab22df":["5f4e87790277826a2aea119328600dfb07761f32","28427ef110c4c5bf5b4057731b83110bd1e13724"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["5f4e87790277826a2aea119328600dfb07761f32","c220849f876de24a79f756f65b3eb045db59f63f","29ef99d61cda9641b6250bf9567329a6e65f901d"],"893269407f5c988a4b2ee13c7ffc01ec43136c1d":["a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8"],"a89acb2b4321b599bbfa1e802c00c4dbbc8ee6b8":["1da8d55113b689b06716246649de6f62430f15c0"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b":["ad94625fb8d088209f46650c8097196fec67f00c"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["2e10cb22a8bdb44339e282925a29182bb2f3174d"],"5f4e87790277826a2aea119328600dfb07761f32":["868da859b43505d9d2a023bfeae6dd0c795f5295"],"1da8d55113b689b06716246649de6f62430f15c0":["be20f9fed1d3edcb1c84abcc39df87a90fab22df"],"c220849f876de24a79f756f65b3eb045db59f63f":["a3776dccca01c11e7046323cfad46a3b4a471233","29ef99d61cda9641b6250bf9567329a6e65f901d","95ae76773bf2b95987d5f9c8f566ab3738953fb4","868da859b43505d9d2a023bfeae6dd0c795f5295"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"5128b7b3b73fedff05fdc5ea2e6be53c1020bb91":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["5128b7b3b73fedff05fdc5ea2e6be53c1020bb91"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["893269407f5c988a4b2ee13c7ffc01ec43136c1d"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"a1b3a24d5d9b47345473ff564f5cc127a7b526b4":["c26f00b574427b55127e869b935845554afde1fa","5128b7b3b73fedff05fdc5ea2e6be53c1020bb91","a258fbb26824fd104ed795e5d9033d2d040049ee","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"ad94625fb8d088209f46650c8097196fec67f00c":["1da8d55113b689b06716246649de6f62430f15c0"],"95ae76773bf2b95987d5f9c8f566ab3738953fb4":["135621f3a0670a9394eb563224a3b76cc4dddc0f","a3776dccca01c11e7046323cfad46a3b4a471233","a1b3a24d5d9b47345473ff564f5cc127a7b526b4","45669a651c970812a680841b97a77cce06af559f"],"868da859b43505d9d2a023bfeae6dd0c795f5295":["45669a651c970812a680841b97a77cce06af559f"],"45669a651c970812a680841b97a77cce06af559f":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"2e10cb22a8bdb44339e282925a29182bb2f3174d":[]},"heads":["a258fbb26824fd104ed795e5d9033d2d040049ee","45669a651c970812a680841b97a77cce06af559f","cd5edd1f2b162a5cfa08efd17851a07373a96817","2e10cb22a8bdb44339e282925a29182bb2f3174d"],"roots":["3cdb369a6112bacd5f5fc1d4e022bed2f8bffb9b","a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}