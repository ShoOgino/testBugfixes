{"path":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","commits":[{"id":"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5","date":1556572478,"type":0,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","pathOld":"/dev/null","sourceNew":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      if (verbose) {\n        log.info(\"- calculating suggestions...\");\n      }\n      long start = TimeSource.NANO_TIME.getTimeNs();\n      List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n      long end = TimeSource.NANO_TIME.getTimeNs();\n      if (verbose) {\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n      }\n      start = TimeSource.NANO_TIME.getTimeNs();\n      MapWriter mw = PolicyHelper.getDiagnostics(session);\n      Map<String, Object> diagnostics = new LinkedHashMap<>();\n      mw.toMap(diagnostics);\n      end = TimeSource.NANO_TIME.getTimeNs();\n      if (verbose) {\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        Map<String, Map<String, Number>> collStats = new TreeMap<>();\n        clusterState.forEachCollection(coll -> {\n          Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n          AtomicInteger numCores = new AtomicInteger();\n          HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n          coll.getSlices().forEach(s -> {\n            numCores.addAndGet(s.getReplicas().size());\n            s.getReplicas().forEach(r -> {\n              nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                  .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n            });\n          });\n          int maxCoresPerNode = 0;\n          int minCoresPerNode = 0;\n          int maxActualShardsPerNode = 0;\n          int minActualShardsPerNode = 0;\n          int maxShardReplicasPerNode = 0;\n          int minShardReplicasPerNode = 0;\n          if (!nodes.isEmpty()) {\n            minCoresPerNode = Integer.MAX_VALUE;\n            minActualShardsPerNode = Integer.MAX_VALUE;\n            minShardReplicasPerNode = Integer.MAX_VALUE;\n            for (Map<String, AtomicInteger> counts : nodes.values()) {\n              int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n              for (AtomicInteger count : counts.values()) {\n                if (count.get() > maxShardReplicasPerNode) {\n                  maxShardReplicasPerNode = count.get();\n                }\n                if (count.get() < minShardReplicasPerNode) {\n                  minShardReplicasPerNode = count.get();\n                }\n              }\n              if (total > maxCoresPerNode) {\n                maxCoresPerNode = total;\n              }\n              if (total < minCoresPerNode) {\n                minCoresPerNode = total;\n              }\n              if (counts.size() > maxActualShardsPerNode) {\n                maxActualShardsPerNode = counts.size();\n              }\n              if (counts.size() < minActualShardsPerNode) {\n                minActualShardsPerNode = counts.size();\n              }\n            }\n          }\n          perColl.put(\"activeShards\", coll.getActiveSlices().size());\n          perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n          perColl.put(\"rf\", coll.getReplicationFactor());\n          perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n          perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n          perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n          perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n          perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n          perColl.put(\"numCores\", numCores.get());\n          perColl.put(\"numNodes\", nodes.size());\n          perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n          perColl.put(\"minCoresPerNode\", minCoresPerNode);\n        });\n        Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n        Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n        for (Row row : session.getSortedNodes()) {\n          Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n          nodeStat.put(\"isLive\", row.isLive());\n          nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n          nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n          int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n          nodeStat.put(\"cores\", cores);\n          coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n          Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n          row.forEachReplica(ri -> {\n            Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n            if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n              perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n            }\n            perReplica.put(\"coreNode\", ri.getName());\n            if (ri.getBool(\"leader\", false)) {\n              perReplica.put(\"leader\", true);\n              Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                  .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n              Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n              if (riSize != null) {\n                totalSize += riSize.doubleValue();\n                collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                if (max == null) max = 0.0;\n                if (riSize.doubleValue() > max) {\n                  collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                }\n                Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                if (min == null) min = Double.MAX_VALUE;\n                if (riSize.doubleValue() < min) {\n                  collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                }\n              }\n            }\n            nodeStat.put(\"replicas\", collReplicas);\n          });\n        }\n\n        // calculate average per shard and convert the units\n        for (Map<String, Number> perColl : collStats.values()) {\n          Number avg = perColl.get(\"avgShardSize\");\n          if (avg != null) {\n            avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n            perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n          }\n          Number num = perColl.get(\"maxShardSize\");\n          if (num != null) {\n            perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n          }\n          num = perColl.get(\"minShardSize\");\n          if (num != null) {\n            perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n          }\n        }\n        Map<String, Object> stats = new LinkedHashMap<>();\n        results.put(\"STATISTICS\", stats);\n        stats.put(\"coresPerNodes\", coreStats);\n        stats.put(\"nodeStats\", nodeStats);\n        stats.put(\"collectionStats\", collStats);\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"edf5b262a72d10530eb2f01dc8f19060355b213e","date":1557765866,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","sourceNew":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      List<Suggester.SuggestionInfo> suggestions = Collections.emptyList();\n      long start, end;\n      if (withSuggestions) {\n        CLIO.err(\"- calculating suggestions...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> diagnostics = Collections.emptyMap();\n      if (withDiagnostics) {\n        CLIO.err(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        results.put(\"STATISTICS\", SimUtils.calculateStats(clientCloudManager, config, verbose));\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","sourceOld":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      if (verbose) {\n        log.info(\"- calculating suggestions...\");\n      }\n      long start = TimeSource.NANO_TIME.getTimeNs();\n      List<Suggester.SuggestionInfo> suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n      long end = TimeSource.NANO_TIME.getTimeNs();\n      if (verbose) {\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n        log.info(\"- calculating diagnostics...\");\n      }\n      start = TimeSource.NANO_TIME.getTimeNs();\n      MapWriter mw = PolicyHelper.getDiagnostics(session);\n      Map<String, Object> diagnostics = new LinkedHashMap<>();\n      mw.toMap(diagnostics);\n      end = TimeSource.NANO_TIME.getTimeNs();\n      if (verbose) {\n        log.info(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        Map<String, Map<String, Number>> collStats = new TreeMap<>();\n        clusterState.forEachCollection(coll -> {\n          Map<String, Number> perColl = collStats.computeIfAbsent(coll.getName(), n -> new LinkedHashMap<>());\n          AtomicInteger numCores = new AtomicInteger();\n          HashMap<String, Map<String, AtomicInteger>> nodes = new HashMap<>();\n          coll.getSlices().forEach(s -> {\n            numCores.addAndGet(s.getReplicas().size());\n            s.getReplicas().forEach(r -> {\n              nodes.computeIfAbsent(r.getNodeName(), n -> new HashMap<>())\n                  .computeIfAbsent(s.getName(), slice -> new AtomicInteger()).incrementAndGet();\n            });\n          });\n          int maxCoresPerNode = 0;\n          int minCoresPerNode = 0;\n          int maxActualShardsPerNode = 0;\n          int minActualShardsPerNode = 0;\n          int maxShardReplicasPerNode = 0;\n          int minShardReplicasPerNode = 0;\n          if (!nodes.isEmpty()) {\n            minCoresPerNode = Integer.MAX_VALUE;\n            minActualShardsPerNode = Integer.MAX_VALUE;\n            minShardReplicasPerNode = Integer.MAX_VALUE;\n            for (Map<String, AtomicInteger> counts : nodes.values()) {\n              int total = counts.values().stream().mapToInt(c -> c.get()).sum();\n              for (AtomicInteger count : counts.values()) {\n                if (count.get() > maxShardReplicasPerNode) {\n                  maxShardReplicasPerNode = count.get();\n                }\n                if (count.get() < minShardReplicasPerNode) {\n                  minShardReplicasPerNode = count.get();\n                }\n              }\n              if (total > maxCoresPerNode) {\n                maxCoresPerNode = total;\n              }\n              if (total < minCoresPerNode) {\n                minCoresPerNode = total;\n              }\n              if (counts.size() > maxActualShardsPerNode) {\n                maxActualShardsPerNode = counts.size();\n              }\n              if (counts.size() < minActualShardsPerNode) {\n                minActualShardsPerNode = counts.size();\n              }\n            }\n          }\n          perColl.put(\"activeShards\", coll.getActiveSlices().size());\n          perColl.put(\"inactiveShards\", coll.getSlices().size() - coll.getActiveSlices().size());\n          perColl.put(\"rf\", coll.getReplicationFactor());\n          perColl.put(\"maxShardsPerNode\", coll.getMaxShardsPerNode());\n          perColl.put(\"maxActualShardsPerNode\", maxActualShardsPerNode);\n          perColl.put(\"minActualShardsPerNode\", minActualShardsPerNode);\n          perColl.put(\"maxShardReplicasPerNode\", maxShardReplicasPerNode);\n          perColl.put(\"minShardReplicasPerNode\", minShardReplicasPerNode);\n          perColl.put(\"numCores\", numCores.get());\n          perColl.put(\"numNodes\", nodes.size());\n          perColl.put(\"maxCoresPerNode\", maxCoresPerNode);\n          perColl.put(\"minCoresPerNode\", minCoresPerNode);\n        });\n        Map<String, Map<String, Object>> nodeStats = new TreeMap<>();\n        Map<Integer, AtomicInteger> coreStats = new TreeMap<>();\n        for (Row row : session.getSortedNodes()) {\n          Map<String, Object> nodeStat = nodeStats.computeIfAbsent(row.node, n -> new LinkedHashMap<>());\n          nodeStat.put(\"isLive\", row.isLive());\n          nodeStat.put(\"freedisk\", row.getVal(\"freedisk\", 0));\n          nodeStat.put(\"totaldisk\", row.getVal(\"totaldisk\", 0));\n          int cores = ((Number)row.getVal(\"cores\", 0)).intValue();\n          nodeStat.put(\"cores\", cores);\n          coreStats.computeIfAbsent(cores, num -> new AtomicInteger()).incrementAndGet();\n          Map<String, Map<String, Map<String, Object>>> collReplicas = new TreeMap<>();\n          row.forEachReplica(ri -> {\n            Map<String, Object> perReplica = collReplicas.computeIfAbsent(ri.getCollection(), c -> new TreeMap<>())\n                .computeIfAbsent(ri.getCore().substring(ri.getCollection().length() + 1), core -> new LinkedHashMap<>());\n//            if (ri.getVariable(Variable.Type.CORE_IDX.tagName) != null) {\n//              perReplica.put(Variable.Type.CORE_IDX.tagName, ri.getVariable(Variable.Type.CORE_IDX.tagName));\n//            }\n            if (ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute) != null) {\n              perReplica.put(Variable.Type.CORE_IDX.metricsAttribute, ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute));\n            }\n            perReplica.put(\"coreNode\", ri.getName());\n            if (ri.getBool(\"leader\", false)) {\n              perReplica.put(\"leader\", true);\n              Double totalSize = (Double)collStats.computeIfAbsent(ri.getCollection(), c -> new HashMap<>())\n                  .computeIfAbsent(\"avgShardSize\", size -> 0.0);\n              Number riSize = (Number)ri.getVariable(Variable.Type.CORE_IDX.metricsAttribute);\n              if (riSize != null) {\n                totalSize += riSize.doubleValue();\n                collStats.get(ri.getCollection()).put(\"avgShardSize\", totalSize);\n                Double max = (Double)collStats.get(ri.getCollection()).get(\"maxShardSize\");\n                if (max == null) max = 0.0;\n                if (riSize.doubleValue() > max) {\n                  collStats.get(ri.getCollection()).put(\"maxShardSize\", riSize.doubleValue());\n                }\n                Double min = (Double)collStats.get(ri.getCollection()).get(\"minShardSize\");\n                if (min == null) min = Double.MAX_VALUE;\n                if (riSize.doubleValue() < min) {\n                  collStats.get(ri.getCollection()).put(\"minShardSize\", riSize.doubleValue());\n                }\n              }\n            }\n            nodeStat.put(\"replicas\", collReplicas);\n          });\n        }\n\n        // calculate average per shard and convert the units\n        for (Map<String, Number> perColl : collStats.values()) {\n          Number avg = perColl.get(\"avgShardSize\");\n          if (avg != null) {\n            avg = avg.doubleValue() / perColl.get(\"activeShards\").doubleValue();\n            perColl.put(\"avgShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(avg));\n          }\n          Number num = perColl.get(\"maxShardSize\");\n          if (num != null) {\n            perColl.put(\"maxShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n          }\n          num = perColl.get(\"minShardSize\");\n          if (num != null) {\n            perColl.put(\"minShardSize\", (Number)Variable.Type.CORE_IDX.convertVal(num));\n          }\n        }\n        Map<String, Object> stats = new LinkedHashMap<>();\n        results.put(\"STATISTICS\", stats);\n        stats.put(\"coresPerNodes\", coreStats);\n        stats.put(\"nodeStats\", nodeStats);\n        stats.put(\"collectionStats\", collStats);\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5ad9c35f926b4bf8da0336d1300efc709c8d5a56","date":1591729157,"type":3,"author":"murblanc","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","sourceNew":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      List<Suggester.SuggestionInfo> suggestions = Collections.emptyList();\n      long start, end;\n      if (withSuggestions) {\n        CLIO.err(\"- calculating suggestions...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> diagnostics = Collections.emptyMap();\n      if (withDiagnostics) {\n        CLIO.err(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        results.put(\"STATISTICS\", SimUtils.calculateStats(clientCloudManager, config, verbose));\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","sourceOld":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      List<Suggester.SuggestionInfo> suggestions = Collections.emptyList();\n      long start, end;\n      if (withSuggestions) {\n        CLIO.err(\"- calculating suggestions...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> diagnostics = Collections.emptyMap();\n      if (withDiagnostics) {\n        CLIO.err(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"znodeVersion\", clusterState.getZNodeVersion());\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        results.put(\"STATISTICS\", SimUtils.calculateStats(clientCloudManager, config, verbose));\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","bugFix":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/util/SolrCLI.AutoscalingTool#prepareResults(SolrCloudManager,AutoScalingConfig,boolean,boolean,boolean,boolean,boolean).mjava","sourceNew":null,"sourceOld":"    private Map<String, Object> prepareResults(SolrCloudManager clientCloudManager,\n                                               AutoScalingConfig config,\n                                               boolean withClusterState,\n                                               boolean withStats,\n                                               boolean withSuggestions,\n                                               boolean withSortedNodes,\n                                               boolean withDiagnostics) throws Exception {\n      Policy.Session session = config.getPolicy().createSession(clientCloudManager);\n      ClusterState clusterState = clientCloudManager.getClusterStateProvider().getClusterState();\n      List<Suggester.SuggestionInfo> suggestions = Collections.emptyList();\n      long start, end;\n      if (withSuggestions) {\n        CLIO.err(\"- calculating suggestions...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        suggestions = PolicyHelper.getSuggestions(config, clientCloudManager);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> diagnostics = Collections.emptyMap();\n      if (withDiagnostics) {\n        CLIO.err(\"- calculating diagnostics...\");\n        start = TimeSource.NANO_TIME.getTimeNs();\n        MapWriter mw = PolicyHelper.getDiagnostics(session);\n        diagnostics = new LinkedHashMap<>();\n        mw.toMap(diagnostics);\n        end = TimeSource.NANO_TIME.getTimeNs();\n        CLIO.err(\"  (took \" + TimeUnit.NANOSECONDS.toMillis(end - start) + \" ms)\");\n      }\n      Map<String, Object> results = new LinkedHashMap<>();\n      if (withClusterState) {\n        Map<String, Object> map = new LinkedHashMap<>();\n        map.put(\"liveNodes\", new TreeSet<>(clusterState.getLiveNodes()));\n        map.put(\"collections\", clusterState.getCollectionsMap());\n        results.put(\"CLUSTERSTATE\", map);\n      }\n      if (withStats) {\n        results.put(\"STATISTICS\", SimUtils.calculateStats(clientCloudManager, config, verbose));\n      }\n      if (withSuggestions) {\n        results.put(\"SUGGESTIONS\", suggestions);\n      }\n      if (!withSortedNodes) {\n        diagnostics.remove(\"sortedNodes\");\n      }\n      if (withDiagnostics) {\n        results.put(\"DIAGNOSTICS\", diagnostics);\n      }\n      return results;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"3f504512a03d978990cbff30db0522b354e846db":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"edf5b262a72d10530eb2f01dc8f19060355b213e":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"]},"commit2Childs":{"5ad9c35f926b4bf8da0336d1300efc709c8d5a56":["3f504512a03d978990cbff30db0522b354e846db"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5"],"edf5b262a72d10530eb2f01dc8f19060355b213e":["5ad9c35f926b4bf8da0336d1300efc709c8d5a56"],"9ef8d00dbfbeb534eba8a219a5df9d99b2de6ab5":["edf5b262a72d10530eb2f01dc8f19060355b213e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}