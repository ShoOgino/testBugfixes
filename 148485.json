{"path":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","commits":[{"id":"469904587e47b677b125b4bb303304e1ef04e961","date":1393861559,"type":0,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"/dev/null","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = null;\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = rb.getQuery();\n    List<Query> filters = rb.getFilters();\n    List<Query> newFilters = new ArrayList();\n    for(Query q : filters) {\n      if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n        newFilters.add(q);\n      } else {\n        CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n        field = cp.getField();\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["4cd5fb664641d53f375acae565b2358e6cd309a2","97473b7232d88686879e8d72210e5e2ad71bfcd1","7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9c47a0a06ff84bb4ff326818729665c58bc8e691","date":1393868567,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = null;\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = rb.getQuery();\n    List<Query> filters = rb.getFilters();\n    List<Query> newFilters = new ArrayList();\n    for(Query q : filters) {\n      if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n        newFilters.add(q);\n      } else {\n        CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n        field = cp.getField();\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = null;\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = rb.getQuery();\n    List<Query> filters = rb.getFilters();\n    List<Query> newFilters = new ArrayList();\n    for(Query q : filters) {\n      if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n        newFilters.add(q);\n      } else {\n        CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n        field = cp.getField();\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c92f8a8bfce9513f91cce0590834ca5333c64e6","date":1396388225,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = null;\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = rb.getQuery();\n    List<Query> filters = rb.getFilters();\n    List<Query> newFilters = new ArrayList();\n    for(Query q : filters) {\n      if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n        newFilters.add(q);\n      } else {\n        CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n        field = cp.getField();\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["97473b7232d88686879e8d72210e5e2ad71bfcd1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5eb2511ababf862ea11e10761c70ee560cd84510","date":1396607225,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = null;\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = rb.getQuery();\n    List<Query> filters = rb.getFilters();\n    List<Query> newFilters = new ArrayList();\n    for(Query q : filters) {\n      if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n        newFilters.add(q);\n      } else {\n        CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n        field = cp.getField();\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae73da626f97850c922c42736f808d0378e165f0","date":1396625460,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"15e323346eac5e4685c0a9f2df85eb96b4239bbb","date":1396688577,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectOpenHashMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e28b14e7783d24ca69089f13ddadadbd2afdcb29","date":1399840701,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"93dd449115a9247533e44bab47e8429e5dccbc6d","date":1400258396,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"56572ec06f1407c066d6b7399413178b33176cd8","date":1400495675,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = FieldCache.DEFAULT.getTermsIndex(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eb518e3b5dddd136bbd532fe2e2890ac3f48e682","date":1401310344,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53c4b316b8c254e1bdf99a58a6b30856012f995d","date":1401311743,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"        @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if(!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if(ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if(field == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n              CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter)q;\n              field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if(field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if(sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query = null;\n    if(qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList();\n\n    if(fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if(filters != null) {\n        for(Query q : filters) {\n          if(!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length()!=0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch(Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size()*2);\n\n    DocIterator idit = docList.iterator();\n\n    while(idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if(ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector = null;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if(pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Iterator<IntObjectCursor> it = groups.iterator();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    while(it.hasNext()) {\n      IntObjectCursor cursor = it.next();\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector)cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if(scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for(int i=0; i<docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["4cd5fb664641d53f375acae565b2358e6cd309a2","97473b7232d88686879e8d72210e5e2ad71bfcd1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"866d433cdd93efc97c64c1291fd1e49710711590","date":1401312177,"type":3,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf","date":1401983689,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    BytesRef bytesRef = new BytesRef();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        values.lookupOrd(ord, bytesRef);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50","date":1407854805,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRef charsRef = new CharsRef();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["469904587e47b677b125b4bb303304e1ef04e961"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    AtomicReader reader = searcher.getAtomicReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cd5fb664641d53f375acae565b2358e6cd309a2","date":1418429100,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    boolean isShard = params.getBool(ShardParams.IS_SHARD, false);\n    String ids = params.get(ShardParams.IDS);\n\n    if (ids == null && isShard) {\n      return;\n    }\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["469904587e47b677b125b4bb303304e1ef04e961","53c4b316b8c254e1bdf99a58a6b30856012f995d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8ced52befdc8d88d90766aadbaac496f3fd0e4ee","date":1421070034,"type":3,"author":"Joel Bernstein","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.put(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.put(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.put(Float.toString(Float.intBitsToFloat((int)groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.put(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n    SortedDocValues values = DocValues.getSorted(reader, field);\n    FixedBitSet groupBits = new FixedBitSet(values.getValueCount());\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    DocIterator idit = docList.iterator();\n\n    while (idit.hasNext()) {\n      int doc = idit.nextDoc();\n      int ord = values.getOrd(doc);\n      if (ord > -1) {\n        groupBits.set(ord);\n        collapsedSet.add(doc);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n    GroupExpandCollector groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    IntObjectMap groups = groupExpandCollector.getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n    for (IntObjectCursor cursor : (Iterable<IntObjectCursor>) groups) {\n      int ord = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n        final BytesRef bytesRef = values.lookupOrd(ord);\n        fieldType.indexedToReadable(bytesRef, charsRef);\n        String group = charsRef.toString();\n        outMap.put(group, slice);\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["97473b7232d88686879e8d72210e5e2ad71bfcd1","1c295155afd43c4478881cf6d7826a66eca4f549"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","date":1424799790,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.put(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.put(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.put(Float.toString(Float.intBitsToFloat((int)groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.put(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(query, pfilter.filter, collector);\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.put(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.put(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.put(Float.toString(Float.intBitsToFloat((int)groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.put(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["6b4ef0126a26f5c29fbbdf5438ec0ba49294115b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"84bc0ab6faa849a8e6667f3bce45874b858d44c9","date":1427871306,"type":3,"author":"Varun Thacker","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.put(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.put(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.put(Float.toString(Float.intBitsToFloat((int)groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.put(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c924d4069ef5a5bc479a493befe0121aada6896","date":1427901860,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    Map<String, DocSlice> outMap = new HashMap<>();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.put(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.put(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.put(Float.toString(Float.intBitsToFloat((int)groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.put(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1db68e96dd908fcd79ef809095822736aa601d08","date":1434630596,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      searcher.search(new FilteredQuery(query, pfilter.filter), collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["6b4ef0126a26f5c29fbbdf5438ec0ba49294115b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87","date":1440409984,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongOpenHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntOpenHashSet collapsedSet = new IntOpenHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectOpenHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectOpenHashMap();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongOpenHashSet((int)(docList.size()*1.25));\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap groups = ((GroupCollector)groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor cursor : (Iterable<LongObjectCursor>) groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector topDocsCollector = (TopDocsCollector) cursor.value;\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6c8dc56dbae186505066de8a78986cba31525ef0","date":1447876120,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = QueryParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"cc5ee54a993d26579e3fe1f8f2a696b46b61c48f","date":1470751445,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8bca25eefa1f2205e2b0ef713701dc3a0fecd702","date":1470810578,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3b013574eedcdbac35dc7e35b0ee616ffc38895d","date":1470897818,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["1c295155afd43c4478881cf6d7826a66eca4f549"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e07c409cff8701e4dc3d45934b021a949a5a8822","date":1475694629,"type":3,"author":"yonik","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, null, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, null, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getLeafReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getLeafReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof  MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          int ord = currentValues.getOrd(contextDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          int ord = values.getOrd(globalDoc);\n          if(ord > -1) {\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        long value = collapseValues.get(contextDoc);\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"148d526aa0af441c75f301699e50e61890f37797","date":1480533418,"type":3,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"33fd39ca1330aa2ab1dc6bfc1b46ca450f71d905","date":1490770371,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ||\n            fieldType instanceof IntPointField || fieldType instanceof LongPointField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"eeb4c36addd0851d25aaa43e38e9beac5a8197d0","date":1490873947,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ||\n            fieldType instanceof IntPointField || fieldType instanceof LongPointField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    FieldType fieldType = searcher.getSchema().getField(field).getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ) {\n            outMap.add(Long.toString(groupValue), slice);\n          } else if(fieldType instanceof TrieFloatField) {\n            outMap.add(Float.toString(Float.intBitsToFloat((int) groupValue)), slice);\n          } else if(fieldType instanceof TrieDoubleField) {\n            outMap.add(Double.toString(Double.longBitsToDouble(groupValue)), slice);\n          }\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4d048016075a0b8589fcfc77fdf8e2a29fc80964","date":1494631326,"type":3,"author":"Steve Rowe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ||\n            fieldType instanceof IntPointField || fieldType instanceof LongPointField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9017cf144952056066919f1ebc7897ff9bd71b1","date":1496757600,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      if(defaultValue != null) {\n        if(fieldType instanceof TrieIntField || fieldType instanceof TrieLongField ||\n            fieldType instanceof IntPointField || fieldType instanceof LongPointField) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(Double.parseDouble(defaultValue));\n        }\n      } else {\n        if(fieldType instanceof TrieFloatField || fieldType instanceof FloatPointField){\n          nullValue = Float.floatToIntBits(0.0f);\n        } else if(fieldType instanceof TrieDoubleField || fieldType instanceof DoublePointField){\n          nullValue = Double.doubleToLongBits(0.0f);\n        }\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16e499b49b40705bfab2e379d16c593ec9117da3","date":1498229244,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        try (UninvertingReader uninvertingReader = new UninvertingReader(\n            new ReaderWrapper(\n                searcher.getSlowAtomicReader(),\n                field),\n            mapping)) {\n          values = uninvertingReader.getSortedDocValues(field);\n        }\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7ae3cdd28940328fa6fcad4309c67fd419f2caab","date":1498242381,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        try (UninvertingReader uninvertingReader = new UninvertingReader(\n            new ReaderWrapper(\n                searcher.getSlowAtomicReader(),\n                field),\n            mapping)) {\n          values = uninvertingReader.getSortedDocValues(field);\n        }\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c","date":1498505582,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","date":1498540685,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"957c610636f393a85a38f1af670540028db13e6b","date":1500044517,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aaf90fc29510e72665ac7934f34c3d1c25efad64","date":1500354819,"type":3,"author":"Cao Manh Dat","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      MultiDocValues.OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"81fff83bdb893c1471efd78f6a9a3ce4f98120b9","date":1531895937,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, topDocs.getMaxScore());\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"83788ad129a5154d5c6562c4e8ce3db48793aada","date":1532961485,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a07eeba66d4090af0095b50cbd1795a3d6182c1d","date":1538797239,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\") LeafReader uninvertingReader = UninvertingReader.wrap(\n            new ReaderWrapper(searcher.getSlowAtomicReader(), field),\n            Collections.singletonMap(field, UninvertingReader.Type.SORTED)::get);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\") LeafReader uninvertingReader = UninvertingReader.wrap(\n            new ReaderWrapper(searcher.getSlowAtomicReader(), field),\n            Collections.singletonMap(field, UninvertingReader.Type.SORTED)::get);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        Map<String, UninvertingReader.Type> mapping = new HashMap();\n        mapping.put(field, UninvertingReader.Type.SORTED);\n        @SuppressWarnings(\"resource\")\n        UninvertingReader uninvertingReader = new UninvertingReader(new ReaderWrapper(searcher.getSlowAtomicReader(), field), mapping);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":["7bc10aeb596994a4cbfa5801932a4dac9fdd2afc"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7bc10aeb596994a4cbfa5801932a4dac9fdd2afc","date":1541533532,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\") LeafReader uninvertingReader = UninvertingReader.wrap(\n            new ReaderWrapper(searcher.getSlowAtomicReader(), field),\n            Collections.singletonMap(field, UninvertingReader.Type.SORTED)::get);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\") LeafReader uninvertingReader = UninvertingReader.wrap(\n            new ReaderWrapper(searcher.getSlowAtomicReader(), field),\n            Collections.singletonMap(field, UninvertingReader.Type.SORTED)::get);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["a07eeba66d4090af0095b50cbd1795a3d6182c1d"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"97473b7232d88686879e8d72210e5e2ad71bfcd1","date":1566274445,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new IOException(\"Expand field is null.\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    if (qs == null) {\n      query = rb.getQuery();\n    } else {\n      try {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    List<Query> newFilters = new ArrayList<>();\n\n    if (fqs == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n            newFilters.add(q);\n          }\n        }\n      }\n    } else {\n      try {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      } catch (Exception e) {\n        throw new IOException(e);\n      }\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["469904587e47b677b125b4bb303304e1ef04e961","53c4b316b8c254e1bdf99a58a6b30856012f995d","8ced52befdc8d88d90766aadbaac496f3fd0e4ee","0c92f8a8bfce9513f91cce0590834ca5333c64e6"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6b4ef0126a26f5c29fbbdf5438ec0ba49294115b","date":1571926274,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter == null) {\n      searcher.search(query, collector);\n    } else {\n      Query q = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n      searcher.search(q, collector);\n    }\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","1db68e96dd908fcd79ef809095822736aa601d08"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86cdf663c5df5189beeeb5e9f6a5a1edcbefea34","date":1572337563,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        try {\n          groupQuery = getGroupQuery(field, count, ordBytes);\n        } catch(Exception e) {\n          throw new IOException(e);\n        }\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1c295155afd43c4478881cf6d7826a66eca4f549","date":1575779373,"type":3,"author":"Michael Gibney","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":["8ced52befdc8d88d90766aadbaac496f3fd0e4ee","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ebdcd7cba625039937ab5b668449544f9dd36927","date":1575885354,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n      int count = 0;\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int ord = currentValues.ordValue();\n            ++count;\n            BytesRef ref = currentValues.lookupOrd(ord);\n            ord = (int)segmentOrdinalMap.get(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            ++count;\n            BytesRef ref = values.lookupOrd(ord);\n            ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            groupBits.set(ord);\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      int count = 0;\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          ++count;\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"93c255ee5a354a24c9d163acebc0457234aed8db","date":1584503584,"type":3,"author":"ameliahenderson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n      TopDocs topDocs = topDocsCollector.topDocs();\n      ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n      if (scoreDocs.length > 0) {\n        if (returnFields.wantsScore() && sort != null) {\n          TopFieldCollector.populateScores(scoreDocs, searcher, query);\n        }\n        int[] docs = new int[scoreDocs.length];\n        float[] scores = new float[scoreDocs.length];\n        for (int i = 0; i < docs.length; i++) {\n          ScoreDoc scoreDoc = scoreDocs[i];\n          docs[i] = scoreDoc.doc;\n          scores[i] = scoreDoc.score;\n        }\n        assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n        DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n\n        if(fieldType instanceof StrField) {\n          final BytesRef bytesRef = ordBytes.get((int)groupValue);\n          fieldType.indexedToReadable(bytesRef, charsRef);\n          String group = charsRef.toString();\n          outMap.add(group, slice);\n        } else {\n          outMap.add(numericToString(fieldType, groupValue), slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"820a47b6a19467558c5c4eb4ff340c4bb224fd71","date":1585374825,"type":3,"author":"Munendra S N","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            field = cp.getField();\n            hint = cp.hint;\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (fq != null && fq.trim().length() != 0 && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5c4d4ec3bca82c8eac712592ab518cd082026f34","date":1585956055,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    query = QueryUtils.combineQueryAndFilter(query, pfilter.filter);\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    if (pfilter.filter != null) {\n      query = new BooleanQuery.Builder()\n          .add(query, Occur.MUST)\n          .add(pfilter.filter, Occur.FILTER)\n          .build();\n    }\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d4d8a0350513120be3cf48c0eafd6b8787fcd3ec","date":1586897159,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    final Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(QueryUtils.combineQueryAndFilter(query, pfilter.filter), collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    query = QueryUtils.combineQueryAndFilter(query, pfilter.filter);\n    searcher.search(query, collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"640ded7811e1b7d29236a5e2934ec3cd266a8199","date":1588973147,"type":3,"author":"Tomas Fernandez Lobbe","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    final Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(QueryUtils.combineQueryAndFilter(query, pfilter.filter), collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    final Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(QueryUtils.combineQueryAndFilter(query, pfilter.filter), collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8582f07e9350eaeb33bf6c4617b8c9895d99c839","date":1591307386,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","pathOld":"solr/core/src/java/org/apache/solr/handler/component/ExpandComponent#process(ResponseBuilder).mjava","sourceNew":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    final Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(QueryUtils.combineQueryAndFilter(query, pfilter.filter), collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","sourceOld":"  @SuppressWarnings(\"unchecked\")\n  @Override\n  public void process(ResponseBuilder rb) throws IOException {\n\n    if (!rb.doExpand) {\n      return;\n    }\n\n    SolrQueryRequest req = rb.req;\n    SolrParams params = req.getParams();\n\n    String field = params.get(ExpandParams.EXPAND_FIELD);\n    String hint = null;\n    if (field == null) {\n      List<Query> filters = rb.getFilters();\n      if (filters != null) {\n        int cost = Integer.MAX_VALUE;\n        for (Query q : filters) {\n          if (q instanceof CollapsingQParserPlugin.CollapsingPostFilter) {\n            CollapsingQParserPlugin.CollapsingPostFilter cp = (CollapsingQParserPlugin.CollapsingPostFilter) q;\n            // if there are multiple collapse pick the low cost one\n            // if cost are equal then first one is picked\n            if (cp.getCost() < cost) {\n              cost = cp.getCost();\n              field = cp.getField();\n              hint = cp.hint;\n            }\n          }\n        }\n      }\n    }\n\n    if (field == null) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"missing expand field\");\n    }\n\n    String sortParam = params.get(ExpandParams.EXPAND_SORT);\n    String[] fqs = params.getParams(ExpandParams.EXPAND_FQ);\n    String qs = params.get(ExpandParams.EXPAND_Q);\n    int limit = params.getInt(ExpandParams.EXPAND_ROWS, 5);\n\n    Sort sort = null;\n\n    if (sortParam != null) {\n      sort = SortSpecParsing.parseSortSpec(sortParam, rb.req).getSort();\n    }\n\n    final Query query;\n    List<Query> newFilters = new ArrayList<>();\n    try {\n      if (qs == null) {\n        query = rb.getQuery();\n      } else {\n        QParser parser = QParser.getParser(qs, req);\n        query = parser.getQuery();\n      }\n\n      if (fqs == null) {\n        List<Query> filters = rb.getFilters();\n        if (filters != null) {\n          for (Query q : filters) {\n            if (!(q instanceof CollapsingQParserPlugin.CollapsingPostFilter)) {\n              newFilters.add(q);\n            }\n          }\n        }\n      } else {\n        for (String fq : fqs) {\n          if (StringUtils.isNotBlank(fq) && !fq.equals(\"*:*\")) {\n            QParser fqp = QParser.getParser(fq, req);\n            newFilters.add(fqp.getQuery());\n          }\n        }\n      }\n    } catch (SyntaxError e) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, e);\n    }\n\n    SolrIndexSearcher searcher = req.getSearcher();\n    LeafReader reader = searcher.getSlowAtomicReader();\n\n    SchemaField schemaField = searcher.getSchema().getField(field);\n    FieldType fieldType = schemaField.getType();\n\n    SortedDocValues values = null;\n    long nullValue = 0L;\n\n    if(fieldType instanceof StrField) {\n      //Get The Top Level SortedDocValues\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n    } else if (fieldType.getNumberType() != null) {\n      //Get the nullValue for the numeric collapse field\n      String defaultValue = searcher.getSchema().getField(field).getDefaultValue();\n      \n      final NumberType numType = fieldType.getNumberType();\n\n      // Since the expand component depends on the operation of the collapse component, \n      // which validates that numeric field types are 32-bit,\n      // we don't need to handle invalid 64-bit field types here.\n      // FIXME: what happens when expand.field specified?\n      //  how would this work for date field?\n      //  SOLR-10400: before this, long and double were explicitly handled\n      if (defaultValue != null) {\n        if (numType == NumberType.INTEGER) {\n          nullValue = Long.parseLong(defaultValue);\n        } else if (numType == NumberType.FLOAT) {\n          nullValue = Float.floatToIntBits(Float.parseFloat(defaultValue));\n        }\n      } else if (NumberType.FLOAT.equals(numType)) { // Integer case already handled by nullValue defaulting to 0\n        nullValue = Float.floatToIntBits(0.0f);\n      }\n    } else {\n      // possible if directly expand.field is specified\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n          \"Expand not supported for fieldType:'\" + fieldType.getTypeName() +\"'\");\n    }\n\n    FixedBitSet groupBits = null;\n    LongHashSet groupSet = null;\n    DocList docList = rb.getResults().docList;\n    IntHashSet collapsedSet = new IntHashSet(docList.size() * 2);\n\n    //Gather the groups for the current page of documents\n    DocIterator idit = docList.iterator();\n    int[] globalDocs = new int[docList.size()];\n    int docsIndex = -1;\n    while (idit.hasNext()) {\n      globalDocs[++docsIndex] = idit.nextDoc();\n    }\n\n    Arrays.sort(globalDocs);\n    Query groupQuery = null;\n\n    /*\n    * This code gathers the group information for the current page.\n    */\n    List<LeafReaderContext> contexts = searcher.getTopReaderContext().leaves();\n\n    if(contexts.size() == 0) {\n      //When no context is available we can skip the expanding\n      return;\n    }\n\n    int currentContext = 0;\n    int currentDocBase = contexts.get(currentContext).docBase;\n    int nextDocBase = (currentContext+1)<contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n    IntObjectHashMap<BytesRef> ordBytes = null;\n    if(values != null) {\n      groupBits = new FixedBitSet(values.getValueCount());\n      OrdinalMap ordinalMap = null;\n      SortedDocValues[] sortedDocValues = null;\n      LongValues segmentOrdinalMap = null;\n      SortedDocValues currentValues = null;\n      if(values instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues)values).mapping;\n        sortedDocValues = ((MultiDocValues.MultiSortedDocValues)values).values;\n        currentValues = sortedDocValues[currentContext];\n        segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n      }\n\n      ordBytes = new IntObjectHashMap<>();\n\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = (currentContext+1) < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          if(ordinalMap != null) {\n            currentValues = sortedDocValues[currentContext];\n            segmentOrdinalMap = ordinalMap.getGlobalOrds(currentContext);\n          }\n        }\n\n        int contextDoc = globalDoc - currentDocBase;\n        if(ordinalMap != null) {\n          if (contextDoc > currentValues.docID()) {\n            currentValues.advance(contextDoc);\n          }\n          if (contextDoc == currentValues.docID()) {\n            int contextOrd = currentValues.ordValue();\n            int ord = (int)segmentOrdinalMap.get(contextOrd);\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = currentValues.lookupOrd(contextOrd);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        } else {\n          if (globalDoc > values.docID()) {\n            values.advance(globalDoc);\n          }\n          if (globalDoc == values.docID()) {\n            int ord = values.ordValue();\n            if (!groupBits.getAndSet(ord)) {\n              BytesRef ref = values.lookupOrd(ord);\n              ordBytes.put(ord, BytesRef.deepCopyOf(ref));\n            }\n            collapsedSet.add(globalDoc);\n          }\n        }\n      }\n\n      int count = ordBytes.size();\n      if(count > 0 && count < 200) {\n        groupQuery = getGroupQuery(field, count, ordBytes);\n      }\n    } else {\n      groupSet = new LongHashSet(docList.size());\n      NumericDocValues collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n      for(int i=0; i<globalDocs.length; i++) {\n        int globalDoc = globalDocs[i];\n        while(globalDoc >= nextDocBase) {\n          currentContext++;\n          currentDocBase = contexts.get(currentContext).docBase;\n          nextDocBase = currentContext+1 < contexts.size() ? contexts.get(currentContext+1).docBase : Integer.MAX_VALUE;\n          collapseValues = contexts.get(currentContext).reader().getNumericDocValues(field);\n        }\n        int contextDoc = globalDoc - currentDocBase;\n        int valueDocID = collapseValues.docID();\n        if (valueDocID < contextDoc) {\n          valueDocID = collapseValues.advance(contextDoc);\n        }\n        long value;\n        if (valueDocID == contextDoc) {\n          value = collapseValues.longValue();\n        } else {\n          value = 0;\n        }\n        if(value != nullValue) {\n          groupSet.add(value);\n          collapsedSet.add(globalDoc);\n        }\n      }\n\n      int count = groupSet.size();\n      if(count > 0 && count < 200) {\n        if (fieldType.isPointField()) {\n          groupQuery = getPointGroupQuery(schemaField, count, groupSet);\n        } else {\n          groupQuery = getGroupQuery(field, fieldType, count, groupSet);\n        }\n      }\n    }\n\n    Collector collector;\n    if (sort != null)\n      sort = sort.rewrite(searcher);\n\n\n    Collector groupExpandCollector = null;\n\n    if(values != null) {\n      //Get The Top Level SortedDocValues again so we can re-iterate:\n      if(CollapsingQParserPlugin.HINT_TOP_FC.equals(hint)) {\n        @SuppressWarnings(\"resource\")\n        LeafReader uninvertingReader = CollapsingQParserPlugin.getTopFieldCacheReader(searcher, field);\n        values = uninvertingReader.getSortedDocValues(field);\n      } else {\n        values = DocValues.getSorted(reader, field);\n      }\n      \n      groupExpandCollector = new GroupExpandCollector(values, groupBits, collapsedSet, limit, sort);\n    } else {\n      groupExpandCollector = new NumericGroupExpandCollector(field, nullValue, groupSet, collapsedSet, limit, sort);\n    }\n\n    if(groupQuery !=  null) {\n      //Limits the results to documents that are in the same group as the documents in the page.\n      newFilters.add(groupQuery);\n    }\n\n    SolrIndexSearcher.ProcessedFilter pfilter = searcher.getProcessedFilter(null, newFilters);\n    if (pfilter.postFilter != null) {\n      pfilter.postFilter.setLastDelegate(groupExpandCollector);\n      collector = pfilter.postFilter;\n    } else {\n      collector = groupExpandCollector;\n    }\n\n    searcher.search(QueryUtils.combineQueryAndFilter(query, pfilter.filter), collector);\n\n    ReturnFields returnFields = rb.rsp.getReturnFields();\n    LongObjectMap<Collector> groups = ((GroupCollector) groupExpandCollector).getGroups();\n    NamedList outMap = new SimpleOrderedMap();\n    CharsRefBuilder charsRef = new CharsRefBuilder();\n    for (LongObjectCursor<Collector> cursor : groups) {\n      long groupValue = cursor.key;\n      if (cursor.value instanceof TopDocsCollector) {\n        TopDocsCollector<?> topDocsCollector = TopDocsCollector.class.cast(cursor.value);\n        TopDocs topDocs = topDocsCollector.topDocs();\n        ScoreDoc[] scoreDocs = topDocs.scoreDocs;\n        if (scoreDocs.length > 0) {\n          if (returnFields.wantsScore() && sort != null) {\n            TopFieldCollector.populateScores(scoreDocs, searcher, query);\n          }\n          int[] docs = new int[scoreDocs.length];\n          float[] scores = new float[scoreDocs.length];\n          for (int i = 0; i < docs.length; i++) {\n            ScoreDoc scoreDoc = scoreDocs[i];\n            docs[i] = scoreDoc.doc;\n            scores[i] = scoreDoc.score;\n          }\n          assert topDocs.totalHits.relation == TotalHits.Relation.EQUAL_TO;\n          DocSlice slice = new DocSlice(0, docs.length, docs, scores, topDocs.totalHits.value, Float.NaN, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      } else {\n        int totalHits = ((TotalHitCountCollector) cursor.value).getTotalHits();\n        if (totalHits > 0) {\n          DocSlice slice = new DocSlice(0, 0, null, null, totalHits, 0, TotalHits.Relation.EQUAL_TO);\n          addGroupSliceToOutputMap(fieldType, ordBytes, outMap, charsRef, groupValue, slice);\n        }\n      }\n    }\n\n    rb.rsp.add(\"expanded\", outMap);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"640ded7811e1b7d29236a5e2934ec3cd266a8199":["d4d8a0350513120be3cf48c0eafd6b8787fcd3ec"],"0c92f8a8bfce9513f91cce0590834ca5333c64e6":["9c47a0a06ff84bb4ff326818729665c58bc8e691"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"33fd39ca1330aa2ab1dc6bfc1b46ca450f71d905":["148d526aa0af441c75f301699e50e61890f37797"],"0c924d4069ef5a5bc479a493befe0121aada6896":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc","84bc0ab6faa849a8e6667f3bce45874b858d44c9"],"4cd5fb664641d53f375acae565b2358e6cd309a2":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["eeb4c36addd0851d25aaa43e38e9beac5a8197d0","4d048016075a0b8589fcfc77fdf8e2a29fc80964"],"1db68e96dd908fcd79ef809095822736aa601d08":["84bc0ab6faa849a8e6667f3bce45874b858d44c9"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["957c610636f393a85a38f1af670540028db13e6b","81fff83bdb893c1471efd78f6a9a3ce4f98120b9"],"8ced52befdc8d88d90766aadbaac496f3fd0e4ee":["4cd5fb664641d53f375acae565b2358e6cd309a2"],"93c255ee5a354a24c9d163acebc0457234aed8db":["1c295155afd43c4478881cf6d7826a66eca4f549"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["8ced52befdc8d88d90766aadbaac496f3fd0e4ee"],"a07eeba66d4090af0095b50cbd1795a3d6182c1d":["83788ad129a5154d5c6562c4e8ce3db48793aada"],"9856095f7afb5a607bf5e65077615ed91273508c":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","148d526aa0af441c75f301699e50e61890f37797"],"5c4d4ec3bca82c8eac712592ab518cd082026f34":["820a47b6a19467558c5c4eb4ff340c4bb224fd71"],"86cdf663c5df5189beeeb5e9f6a5a1edcbefea34":["6b4ef0126a26f5c29fbbdf5438ec0ba49294115b"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["ae73da626f97850c922c42736f808d0378e165f0"],"d4d8a0350513120be3cf48c0eafd6b8787fcd3ec":["5c4d4ec3bca82c8eac712592ab518cd082026f34"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["866d433cdd93efc97c64c1291fd1e49710711590"],"eeb4c36addd0851d25aaa43e38e9beac5a8197d0":["148d526aa0af441c75f301699e50e61890f37797"],"53c4b316b8c254e1bdf99a58a6b30856012f995d":["eb518e3b5dddd136bbd532fe2e2890ac3f48e682"],"8582f07e9350eaeb33bf6c4617b8c9895d99c839":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"eb518e3b5dddd136bbd532fe2e2890ac3f48e682":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","957c610636f393a85a38f1af670540028db13e6b"],"ae73da626f97850c922c42736f808d0378e165f0":["0c92f8a8bfce9513f91cce0590834ca5333c64e6"],"957c610636f393a85a38f1af670540028db13e6b":["28288370235ed02234a64753cdbf0c6ec096304a"],"56572ec06f1407c066d6b7399413178b33176cd8":["ae73da626f97850c922c42736f808d0378e165f0","93dd449115a9247533e44bab47e8429e5dccbc6d"],"ebdcd7cba625039937ab5b668449544f9dd36927":["86cdf663c5df5189beeeb5e9f6a5a1edcbefea34","1c295155afd43c4478881cf6d7826a66eca4f549"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"469904587e47b677b125b4bb303304e1ef04e961":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7ae3cdd28940328fa6fcad4309c67fd419f2caab":["16e499b49b40705bfab2e379d16c593ec9117da3"],"7bc10aeb596994a4cbfa5801932a4dac9fdd2afc":["a07eeba66d4090af0095b50cbd1795a3d6182c1d"],"8bca25eefa1f2205e2b0ef713701dc3a0fecd702":["6c8dc56dbae186505066de8a78986cba31525ef0","cc5ee54a993d26579e3fe1f8f2a696b46b61c48f"],"148d526aa0af441c75f301699e50e61890f37797":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":["5eb2511ababf862ea11e10761c70ee560cd84510","ae73da626f97850c922c42736f808d0378e165f0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["e9017cf144952056066919f1ebc7897ff9bd71b1","4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c"],"866d433cdd93efc97c64c1291fd1e49710711590":["53c4b316b8c254e1bdf99a58a6b30856012f995d"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["81fff83bdb893c1471efd78f6a9a3ce4f98120b9"],"5eb2511ababf862ea11e10761c70ee560cd84510":["9c47a0a06ff84bb4ff326818729665c58bc8e691","0c92f8a8bfce9513f91cce0590834ca5333c64e6"],"84bc0ab6faa849a8e6667f3bce45874b858d44c9":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"820a47b6a19467558c5c4eb4ff340c4bb224fd71":["93c255ee5a354a24c9d163acebc0457234aed8db"],"97473b7232d88686879e8d72210e5e2ad71bfcd1":["7bc10aeb596994a4cbfa5801932a4dac9fdd2afc"],"4d048016075a0b8589fcfc77fdf8e2a29fc80964":["33fd39ca1330aa2ab1dc6bfc1b46ca450f71d905"],"cc5ee54a993d26579e3fe1f8f2a696b46b61c48f":["6c8dc56dbae186505066de8a78986cba31525ef0"],"6b4ef0126a26f5c29fbbdf5438ec0ba49294115b":["97473b7232d88686879e8d72210e5e2ad71bfcd1"],"4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c":["7ae3cdd28940328fa6fcad4309c67fd419f2caab"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"28288370235ed02234a64753cdbf0c6ec096304a":["4d048016075a0b8589fcfc77fdf8e2a29fc80964","4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["6c8dc56dbae186505066de8a78986cba31525ef0","e07c409cff8701e4dc3d45934b021a949a5a8822"],"6c8dc56dbae186505066de8a78986cba31525ef0":["1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["ae73da626f97850c922c42736f808d0378e165f0","e28b14e7783d24ca69089f13ddadadbd2afdcb29"],"16e499b49b40705bfab2e379d16c593ec9117da3":["4d048016075a0b8589fcfc77fdf8e2a29fc80964"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702"],"9c47a0a06ff84bb4ff326818729665c58bc8e691":["469904587e47b677b125b4bb303304e1ef04e961"],"81fff83bdb893c1471efd78f6a9a3ce4f98120b9":["957c610636f393a85a38f1af670540028db13e6b"],"1c295155afd43c4478881cf6d7826a66eca4f549":["86cdf663c5df5189beeeb5e9f6a5a1edcbefea34"],"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87":["1db68e96dd908fcd79ef809095822736aa601d08"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":["6c8dc56dbae186505066de8a78986cba31525ef0","8bca25eefa1f2205e2b0ef713701dc3a0fecd702"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8582f07e9350eaeb33bf6c4617b8c9895d99c839"]},"commit2Childs":{"640ded7811e1b7d29236a5e2934ec3cd266a8199":["8582f07e9350eaeb33bf6c4617b8c9895d99c839"],"0c92f8a8bfce9513f91cce0590834ca5333c64e6":["ae73da626f97850c922c42736f808d0378e165f0","5eb2511ababf862ea11e10761c70ee560cd84510"],"7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"33fd39ca1330aa2ab1dc6bfc1b46ca450f71d905":["4d048016075a0b8589fcfc77fdf8e2a29fc80964"],"0c924d4069ef5a5bc479a493befe0121aada6896":[],"4cd5fb664641d53f375acae565b2358e6cd309a2":["8ced52befdc8d88d90766aadbaac496f3fd0e4ee"],"e9017cf144952056066919f1ebc7897ff9bd71b1":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4"],"1db68e96dd908fcd79ef809095822736aa601d08":["1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"8ced52befdc8d88d90766aadbaac496f3fd0e4ee":["f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc"],"93c255ee5a354a24c9d163acebc0457234aed8db":["820a47b6a19467558c5c4eb4ff340c4bb224fd71"],"f8ec642b0195d666cf3b5a6a6c2a80bdd3b756bc":["0c924d4069ef5a5bc479a493befe0121aada6896","84bc0ab6faa849a8e6667f3bce45874b858d44c9"],"a07eeba66d4090af0095b50cbd1795a3d6182c1d":["7bc10aeb596994a4cbfa5801932a4dac9fdd2afc"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"5c4d4ec3bca82c8eac712592ab518cd082026f34":["d4d8a0350513120be3cf48c0eafd6b8787fcd3ec"],"86cdf663c5df5189beeeb5e9f6a5a1edcbefea34":["ebdcd7cba625039937ab5b668449544f9dd36927","1c295155afd43c4478881cf6d7826a66eca4f549"],"e28b14e7783d24ca69089f13ddadadbd2afdcb29":["93dd449115a9247533e44bab47e8429e5dccbc6d"],"d4d8a0350513120be3cf48c0eafd6b8787fcd3ec":["640ded7811e1b7d29236a5e2934ec3cd266a8199"],"53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf":["7dc6ea5fd38ce7aa8f36b3bac8b757da77f31d50"],"eeb4c36addd0851d25aaa43e38e9beac5a8197d0":["e9017cf144952056066919f1ebc7897ff9bd71b1"],"53c4b316b8c254e1bdf99a58a6b30856012f995d":["866d433cdd93efc97c64c1291fd1e49710711590"],"8582f07e9350eaeb33bf6c4617b8c9895d99c839":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"eb518e3b5dddd136bbd532fe2e2890ac3f48e682":["53c4b316b8c254e1bdf99a58a6b30856012f995d"],"aaf90fc29510e72665ac7934f34c3d1c25efad64":[],"ae73da626f97850c922c42736f808d0378e165f0":["e28b14e7783d24ca69089f13ddadadbd2afdcb29","56572ec06f1407c066d6b7399413178b33176cd8","15e323346eac5e4685c0a9f2df85eb96b4239bbb","93dd449115a9247533e44bab47e8429e5dccbc6d"],"957c610636f393a85a38f1af670540028db13e6b":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","aaf90fc29510e72665ac7934f34c3d1c25efad64","81fff83bdb893c1471efd78f6a9a3ce4f98120b9"],"56572ec06f1407c066d6b7399413178b33176cd8":[],"ebdcd7cba625039937ab5b668449544f9dd36927":[],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["4cd5fb664641d53f375acae565b2358e6cd309a2"],"469904587e47b677b125b4bb303304e1ef04e961":["9c47a0a06ff84bb4ff326818729665c58bc8e691"],"7ae3cdd28940328fa6fcad4309c67fd419f2caab":["4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c"],"7bc10aeb596994a4cbfa5801932a4dac9fdd2afc":["97473b7232d88686879e8d72210e5e2ad71bfcd1"],"8bca25eefa1f2205e2b0ef713701dc3a0fecd702":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"148d526aa0af441c75f301699e50e61890f37797":["33fd39ca1330aa2ab1dc6bfc1b46ca450f71d905","9856095f7afb5a607bf5e65077615ed91273508c","eeb4c36addd0851d25aaa43e38e9beac5a8197d0"],"15e323346eac5e4685c0a9f2df85eb96b4239bbb":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["469904587e47b677b125b4bb303304e1ef04e961"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["e07c409cff8701e4dc3d45934b021a949a5a8822"],"fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4":["aaf90fc29510e72665ac7934f34c3d1c25efad64"],"866d433cdd93efc97c64c1291fd1e49710711590":["53fc2f4c5ce4f2053be3d5f5d14d79129ebb4bbf"],"83788ad129a5154d5c6562c4e8ce3db48793aada":["a07eeba66d4090af0095b50cbd1795a3d6182c1d"],"5eb2511ababf862ea11e10761c70ee560cd84510":["15e323346eac5e4685c0a9f2df85eb96b4239bbb"],"84bc0ab6faa849a8e6667f3bce45874b858d44c9":["0c924d4069ef5a5bc479a493befe0121aada6896","1db68e96dd908fcd79ef809095822736aa601d08"],"820a47b6a19467558c5c4eb4ff340c4bb224fd71":["5c4d4ec3bca82c8eac712592ab518cd082026f34"],"97473b7232d88686879e8d72210e5e2ad71bfcd1":["6b4ef0126a26f5c29fbbdf5438ec0ba49294115b"],"4d048016075a0b8589fcfc77fdf8e2a29fc80964":["e9017cf144952056066919f1ebc7897ff9bd71b1","28288370235ed02234a64753cdbf0c6ec096304a","16e499b49b40705bfab2e379d16c593ec9117da3"],"cc5ee54a993d26579e3fe1f8f2a696b46b61c48f":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702"],"6b4ef0126a26f5c29fbbdf5438ec0ba49294115b":["86cdf663c5df5189beeeb5e9f6a5a1edcbefea34"],"4f4aa48e5f0a311a55232e8de7d7c5d4bd172c4c":["fbedfa79ef95dc2b5b49f7d54d80e0b47867f9b4","28288370235ed02234a64753cdbf0c6ec096304a"],"e07c409cff8701e4dc3d45934b021a949a5a8822":["148d526aa0af441c75f301699e50e61890f37797","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["9856095f7afb5a607bf5e65077615ed91273508c"],"28288370235ed02234a64753cdbf0c6ec096304a":["957c610636f393a85a38f1af670540028db13e6b"],"6c8dc56dbae186505066de8a78986cba31525ef0":["8bca25eefa1f2205e2b0ef713701dc3a0fecd702","cc5ee54a993d26579e3fe1f8f2a696b46b61c48f","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","3b013574eedcdbac35dc7e35b0ee616ffc38895d"],"93dd449115a9247533e44bab47e8429e5dccbc6d":["eb518e3b5dddd136bbd532fe2e2890ac3f48e682","56572ec06f1407c066d6b7399413178b33176cd8"],"16e499b49b40705bfab2e379d16c593ec9117da3":["7ae3cdd28940328fa6fcad4309c67fd419f2caab"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"9c47a0a06ff84bb4ff326818729665c58bc8e691":["0c92f8a8bfce9513f91cce0590834ca5333c64e6","5eb2511ababf862ea11e10761c70ee560cd84510"],"81fff83bdb893c1471efd78f6a9a3ce4f98120b9":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","83788ad129a5154d5c6562c4e8ce3db48793aada"],"1c295155afd43c4478881cf6d7826a66eca4f549":["93c255ee5a354a24c9d163acebc0457234aed8db","ebdcd7cba625039937ab5b668449544f9dd36927"],"1fcee2c3acd9aa64a73a3b441b3f6cb492af0b87":["6c8dc56dbae186505066de8a78986cba31525ef0"],"3b013574eedcdbac35dc7e35b0ee616ffc38895d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0c924d4069ef5a5bc479a493befe0121aada6896","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","9856095f7afb5a607bf5e65077615ed91273508c","aaf90fc29510e72665ac7934f34c3d1c25efad64","56572ec06f1407c066d6b7399413178b33176cd8","ebdcd7cba625039937ab5b668449544f9dd36927","15e323346eac5e4685c0a9f2df85eb96b4239bbb","3b013574eedcdbac35dc7e35b0ee616ffc38895d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}