{"path":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","pathOld":"modules/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","sourceNew":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", new StringReader(\"妹の咲子です。俺と年子で、今受験生です。\"));\n    TokenStream ts = analyzer.tokenStream(\"foo\", new StringReader(\"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\"));\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","sourceOld":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", new StringReader(\"妹の咲子です。俺と年子で、今受験生です。\"));\n    TokenStream ts = analyzer.tokenStream(\"foo\", new StringReader(\"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\"));\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","pathOld":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","sourceNew":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", \"妹の咲子です。俺と年子で、今受験生です。\");\n    TokenStream ts = analyzer.tokenStream(\"foo\", \"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\");\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","sourceOld":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", new StringReader(\"妹の咲子です。俺と年子で、今受験生です。\"));\n    TokenStream ts = analyzer.tokenStream(\"foo\", new StringReader(\"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\"));\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","bugFix":["0984ad47974c2d5d354519ddb2aa8358973a6271"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","pathOld":"lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseTokenizer#testTwoSentences().mjava","sourceNew":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", \"妹の咲子です。俺と年子で、今受験生です。\");\n    TokenStream ts = analyzer.tokenStream(\"foo\", \"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\");\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","sourceOld":"  /** Tests that sentence offset is incorporated into the resulting offsets */\n  public void testTwoSentences() throws Exception {\n    /*\n    //TokenStream ts = a.tokenStream(\"foo\", new StringReader(\"妹の咲子です。俺と年子で、今受験生です。\"));\n    TokenStream ts = analyzer.tokenStream(\"foo\", new StringReader(\"&#x250cdf66<!--\\\"<!--#<!--;?><!--#<!--#><!---->?>-->;\"));\n    ts.reset();\n    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n    while(ts.incrementToken()) {\n      System.out.println(\"  \" + termAtt.toString());\n    }\n    System.out.println(\"DONE PARSE\\n\\n\");\n    */\n\n    assertAnalyzesTo(analyzerNoPunct, \"魔女狩大将マシュー・ホプキンス。 魔女狩大将マシュー・ホプキンス。\",\n      new String[] { \"魔女\", \"狩\", \"大将\", \"マシュー\", \"ホプキンス\",  \"魔女\", \"狩\", \"大将\", \"マシュー\",  \"ホプキンス\"  },\n      new int[] { 0, 2, 3, 5, 10, 17, 19, 20, 22, 27 },\n      new int[] { 2, 3, 5, 9, 15, 19, 20, 22, 26, 32 }\n    );\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["b89678825b68eccaf09e6ab71675fc0b0af1e099","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c83d6c4335f31cae14f625a222bc842f20073dcd"]},"commit2Childs":{"b89678825b68eccaf09e6ab71675fc0b0af1e099":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}