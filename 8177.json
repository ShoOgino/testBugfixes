{"path":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","commits":[{"id":"ff6fd241dc6610f7f81b62e3ba4cedf105939623","date":1307331653,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"79c2cb24929f2649a8875fb629086171f914d5ce","date":1307332717,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f83af14a2a8131b14d7aee6274c740334e0363d3","date":1307579822,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77cf4379b2824f6ea34b091c495d6e95c38ff9e2","date":1307610475,"type":1,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriter#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","date":1307729864,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = TEST_NIGHTLY ? 6 * RANDOM_MULTIPLIER : 3 * RANDOM_MULTIPLIER;\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","date":1308670974,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 3000);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1509f151d7692d84fae414b2b799ac06ba60fcb4","date":1314451621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", Field.Store.NO, Field.Index.ANALYZED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"53ae89cd75b0acbdfb8890710c6742f3fb80e65d","date":1315806626,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new MockTokenizer(reader, MockTokenizer.WHITESPACE, true);\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2f49143da0a5d278a72f741432047fcfa6da996e","date":1316927425,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new ReusableAnalyzerBase() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06afeb024ba6ddf4a0beb6acdc9b123476d2dedf","date":1319454029,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    assumeFalse(\"This test cannot run with Memory codec\", CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", CodecProvider.getDefault().getFieldCodec(\"field\").equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"06584e6e98d592b34e1329b384182f368d2025e8","date":1320850353,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    w.setInfoStream(VERBOSE ? System.out : null);\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestIndexWriterDelete#testIndexingThenDeleting().mjava","sourceNew":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testIndexingThenDeleting() throws Exception {\n    final String fieldFormat = _TestUtil.getPostingsFormat(\"field\");\n    assumeFalse(\"This test cannot run with Memory codec\", fieldFormat.equals(\"Memory\"));\n    assumeFalse(\"This test cannot run with SimpleText codec\", fieldFormat.equals(\"SimpleText\"));\n    final Random r = random;\n    Directory dir = newDirectory();\n    // note this test explicitly disables payloads\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      public TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        return new TokenStreamComponents(new MockTokenizer(reader, MockTokenizer.WHITESPACE, true));\n      }\n    };\n    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, analyzer).setRAMBufferSizeMB(1.0).setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH).setMaxBufferedDeleteTerms(IndexWriterConfig.DISABLE_AUTO_FLUSH));\n    Document doc = new Document();\n    doc.add(newField(\"field\", \"go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\", TextField.TYPE_UNSTORED));\n    int num = atLeast(3);\n    for (int iter = 0; iter < num; iter++) {\n      int count = 0;\n\n      final boolean doIndexing = r.nextBoolean();\n      if (VERBOSE) {\n        System.out.println(\"TEST: iter doIndexing=\" + doIndexing);\n      }\n      if (doIndexing) {\n        // Add docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.addDocument(doc);\n          count++;\n        }\n      } else {\n        // Delete docs until a flush is triggered\n        final int startFlushCount = w.getFlushCount();\n        while(w.getFlushCount() == startFlushCount) {\n          w.deleteDocuments(new Term(\"foo\", \"\"+count));\n          count++;\n        }\n      }\n      assertTrue(\"flush happened too quickly during \" + (doIndexing ? \"indexing\" : \"deleting\") + \" count=\" + count, count > 2500);\n    }\n    w.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["79c2cb24929f2649a8875fb629086171f914d5ce","f83af14a2a8131b14d7aee6274c740334e0363d3"],"f83af14a2a8131b14d7aee6274c740334e0363d3":["ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["06584e6e98d592b34e1329b384182f368d2025e8"],"06584e6e98d592b34e1329b384182f368d2025e8":["7b91922b55d15444d554721b352861d028eb8278"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["f83af14a2a8131b14d7aee6274c740334e0363d3"],"2553b00f699380c64959ccb27991289aae87be2e":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f83af14a2a8131b14d7aee6274c740334e0363d3","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"7b91922b55d15444d554721b352861d028eb8278":["06afeb024ba6ddf4a0beb6acdc9b123476d2dedf"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ff6fd241dc6610f7f81b62e3ba4cedf105939623"],"06afeb024ba6ddf4a0beb6acdc9b123476d2dedf":["2f49143da0a5d278a72f741432047fcfa6da996e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f83af14a2a8131b14d7aee6274c740334e0363d3"],"2f49143da0a5d278a72f741432047fcfa6da996e":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["fafef7c83fe8e0b3ca9298d5d75d6b943dc28153"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"a02058e0eaba4bbd5d05e6b06b9522c0acfd1655":["2553b00f699380c64959ccb27991289aae87be2e"],"f83af14a2a8131b14d7aee6274c740334e0363d3":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655","fafef7c83fe8e0b3ca9298d5d75d6b943dc28153","d083e83f225b11e5fdd900e83d26ddb385b6955c","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"ff6fd241dc6610f7f81b62e3ba4cedf105939623":["f83af14a2a8131b14d7aee6274c740334e0363d3","79c2cb24929f2649a8875fb629086171f914d5ce"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"06584e6e98d592b34e1329b384182f368d2025e8":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"fafef7c83fe8e0b3ca9298d5d75d6b943dc28153":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","1509f151d7692d84fae414b2b799ac06ba60fcb4"],"2553b00f699380c64959ccb27991289aae87be2e":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"53ae89cd75b0acbdfb8890710c6742f3fb80e65d":["2f49143da0a5d278a72f741432047fcfa6da996e"],"7b91922b55d15444d554721b352861d028eb8278":["06584e6e98d592b34e1329b384182f368d2025e8"],"79c2cb24929f2649a8875fb629086171f914d5ce":["a02058e0eaba4bbd5d05e6b06b9522c0acfd1655"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ff6fd241dc6610f7f81b62e3ba4cedf105939623","79c2cb24929f2649a8875fb629086171f914d5ce","77cf4379b2824f6ea34b091c495d6e95c38ff9e2"],"06afeb024ba6ddf4a0beb6acdc9b123476d2dedf":["7b91922b55d15444d554721b352861d028eb8278"],"77cf4379b2824f6ea34b091c495d6e95c38ff9e2":[],"2f49143da0a5d278a72f741432047fcfa6da996e":["06afeb024ba6ddf4a0beb6acdc9b123476d2dedf"],"1509f151d7692d84fae414b2b799ac06ba60fcb4":["53ae89cd75b0acbdfb8890710c6742f3fb80e65d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c","77cf4379b2824f6ea34b091c495d6e95c38ff9e2","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}