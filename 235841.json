{"path":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","commits":[{"id":"4bf528aa2b9571ce1ec892ecf726201ef1e404e3","date":1288732150,"type":0,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"/dev/null","sourceNew":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f73345c0361d1deee1163f07ebd2b3263e97a6a","date":1289004040,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"85a883878c0af761245ab048babc63d099f835f3","date":1289553330,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"/dev/null","sourceNew":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"/dev/null","sourceNew":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9c7cdfe5a1ea9db97faa404b251fa644faa73597","date":1308345959,"type":3,"author":"Martijn van Groningen","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":["a250ecd3e83b1c6595bcae0474f5b258df42021b","a250ecd3e83b1c6595bcae0474f5b258df42021b","a250ecd3e83b1c6595bcae0474f5b258df42021b"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7edfc3f7caa7b49a18fe367692768b33b018e9db","date":1308374217,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter()!=null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0; // doclist needed for debugging or highlighting\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n    \n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc>>6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    searcher.search(query, luceneFilter, allCollectors);\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createNextCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (collectors.size() > 0) {\n      searcher.search(query, luceneFilter, MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()])));\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"090a0320e4de4a3674376aef96b9701f47564f86","date":1308707325,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34","date":1309197122,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2553b00f699380c64959ccb27991289aae87be2e","date":1309290151,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    filter = cmd.getFilter() != null ? cmd.getFilter() : searcher.getDocSet(cmd.getFilterList());\n    luceneFilter = filter == null ? null : filter.getTopFilter();\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"abb50f423bca9ade445c9841a81bb5aedff316eb","date":1310061296,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1291e4568eb7d9463d751627596ef14baf4c1603","date":1310112572,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least on field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":5,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/search/Grouping#execute().mjava","pathOld":"solr/src/java/org/apache/solr/search/Grouping#execute().mjava","sourceNew":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","sourceOld":"  public void execute() throws IOException {\n    if (commands.isEmpty()) {\n      throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"Specify at least one field, function or query to group by.\");\n    }\n\n    DocListAndSet out = new DocListAndSet();\n    qr.setDocListAndSet(out);\n\n    SolrIndexSearcher.ProcessedFilter pf = searcher.getProcessedFilter(cmd.getFilter(), cmd.getFilterList());\n    final Filter luceneFilter = pf.filter;\n    maxDoc = searcher.maxDoc();\n\n    needScores = (cmd.getFlags() & SolrIndexSearcher.GET_SCORES) != 0;\n    boolean cacheScores = false;\n    // NOTE: Change this when groupSort can be specified per group\n    if (!needScores && !commands.isEmpty()) {\n      if (commands.get(0).groupSort == null) {\n        cacheScores = true;\n      } else {\n        for (SortField field : commands.get(0).groupSort.getSort()) {\n          if (field.getType() == SortField.Type.SCORE) {\n            cacheScores = true;\n            break;\n          }\n        }\n      }\n    } else if (needScores) {\n      cacheScores = needScores;\n    }\n    getDocSet = (cmd.getFlags() & SolrIndexSearcher.GET_DOCSET) != 0;\n    getDocList = (cmd.getFlags() & SolrIndexSearcher.GET_DOCLIST) != 0;\n    query = QueryUtils.makeQueryable(cmd.getQuery());\n\n    for (Command cmd : commands) {\n      cmd.prepare();\n    }\n\n    List<Collector> collectors = new ArrayList<Collector>(commands.size());\n    for (Command cmd : commands) {\n      Collector collector = cmd.createFirstPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    Collector allCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n    DocSetCollector setCollector = null;\n    if (getDocSet) {\n      setCollector = new DocSetDelegateCollector(maxDoc >> 6, maxDoc, allCollectors);\n      allCollectors = setCollector;\n    }\n\n    CachingCollector cachedCollector = null;\n    if (cacheSecondPassSearch && allCollectors != null) {\n      int maxDocsToCache = (int) Math.round(maxDoc * (maxDocsPercentageToCache / 100.0d));\n      // Only makes sense to cache if we cache more than zero.\n      // Maybe we should have a minimum and a maximum, that defines the window we would like caching for.\n      if (maxDocsToCache > 0) {\n        allCollectors = cachedCollector = CachingCollector.create(allCollectors, cacheScores, maxDocsToCache);\n      }\n    }\n\n    if (pf.postFilter != null) {\n      pf.postFilter.setLastDelegate(allCollectors);\n      allCollectors = pf.postFilter;\n    }\n\n    if (allCollectors != null) {\n      searcher.search(query, luceneFilter, allCollectors);\n    }\n\n    if (getDocSet) {\n      qr.setDocSet(setCollector.getDocSet());\n    }\n\n    collectors.clear();\n    for (Command cmd : commands) {\n      Collector collector = cmd.createSecondPassCollector();\n      if (collector != null)\n        collectors.add(collector);\n    }\n\n    if (!collectors.isEmpty()) {\n      Collector secondPhaseCollectors = MultiCollector.wrap(collectors.toArray(new Collector[collectors.size()]));\n      if (collectors.size() > 0) {\n        if (cachedCollector != null) {\n          if (cachedCollector.isCached()) {\n            cachedCollector.replay(secondPhaseCollectors);\n          } else {\n            signalCacheWarning = true;\n            logger.warn(String.format(\"The grouping cache is active, but not used because it exceeded the max cache limit of %d percent\", maxDocsPercentageToCache));\n            logger.warn(\"Please increase cache size or disable group caching.\");\n            searcher.search(query, luceneFilter, secondPhaseCollectors);\n          }\n        } else {\n          if (pf.postFilter != null) {\n            pf.postFilter.setLastDelegate(secondPhaseCollectors);\n            secondPhaseCollectors = pf.postFilter;\n          }\n          searcher.search(query, luceneFilter, secondPhaseCollectors);\n        }\n      }\n    }\n\n    for (Command cmd : commands) {\n      cmd.finish();\n    }\n\n    qr.groupedResults = grouped;\n\n    if (getDocList) {\n      int sz = idSet.size();\n      int[] ids = new int[sz];\n      int idx = 0;\n      for (int val : idSet) {\n        ids[idx++] = val;\n      }\n      qr.setDocList(new DocSlice(0, sz, ids, null, maxMatches, maxScore));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"4bf528aa2b9571ce1ec892ecf726201ef1e404e3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"0f73345c0361d1deee1163f07ebd2b3263e97a6a":["4bf528aa2b9571ce1ec892ecf726201ef1e404e3"],"c26f00b574427b55127e869b935845554afde1fa":["abb50f423bca9ade445c9841a81bb5aedff316eb","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"abb50f423bca9ade445c9841a81bb5aedff316eb":["f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34"],"85a883878c0af761245ab048babc63d099f835f3":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0f73345c0361d1deee1163f07ebd2b3263e97a6a"],"2553b00f699380c64959ccb27991289aae87be2e":["7edfc3f7caa7b49a18fe367692768b33b018e9db","f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","0f73345c0361d1deee1163f07ebd2b3263e97a6a"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["9c7cdfe5a1ea9db97faa404b251fa644faa73597","f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34"],"090a0320e4de4a3674376aef96b9701f47564f86":["9c7cdfe5a1ea9db97faa404b251fa644faa73597"],"1291e4568eb7d9463d751627596ef14baf4c1603":["d083e83f225b11e5fdd900e83d26ddb385b6955c","abb50f423bca9ade445c9841a81bb5aedff316eb"],"7edfc3f7caa7b49a18fe367692768b33b018e9db":["0f73345c0361d1deee1163f07ebd2b3263e97a6a","9c7cdfe5a1ea9db97faa404b251fa644faa73597"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["2553b00f699380c64959ccb27991289aae87be2e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9c7cdfe5a1ea9db97faa404b251fa644faa73597":["0f73345c0361d1deee1163f07ebd2b3263e97a6a"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["abb50f423bca9ade445c9841a81bb5aedff316eb"],"f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34":["090a0320e4de4a3674376aef96b9701f47564f86"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c26f00b574427b55127e869b935845554afde1fa"]},"commit2Childs":{"4bf528aa2b9571ce1ec892ecf726201ef1e404e3":["0f73345c0361d1deee1163f07ebd2b3263e97a6a"],"0f73345c0361d1deee1163f07ebd2b3263e97a6a":["85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","7edfc3f7caa7b49a18fe367692768b33b018e9db","9c7cdfe5a1ea9db97faa404b251fa644faa73597"],"c26f00b574427b55127e869b935845554afde1fa":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"abb50f423bca9ade445c9841a81bb5aedff316eb":["c26f00b574427b55127e869b935845554afde1fa","1291e4568eb7d9463d751627596ef14baf4c1603","a258fbb26824fd104ed795e5d9033d2d040049ee"],"85a883878c0af761245ab048babc63d099f835f3":[],"2553b00f699380c64959ccb27991289aae87be2e":["c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["1291e4568eb7d9463d751627596ef14baf4c1603"],"090a0320e4de4a3674376aef96b9701f47564f86":["f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34"],"1291e4568eb7d9463d751627596ef14baf4c1603":[],"7edfc3f7caa7b49a18fe367692768b33b018e9db":["2553b00f699380c64959ccb27991289aae87be2e"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["4bf528aa2b9571ce1ec892ecf726201ef1e404e3","85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"9c7cdfe5a1ea9db97faa404b251fa644faa73597":["d083e83f225b11e5fdd900e83d26ddb385b6955c","090a0320e4de4a3674376aef96b9701f47564f86","7edfc3f7caa7b49a18fe367692768b33b018e9db"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"f8bf47b67b38083a0c4d9d2e3f53b59a48e8db34":["abb50f423bca9ade445c9841a81bb5aedff316eb","2553b00f699380c64959ccb27991289aae87be2e","d083e83f225b11e5fdd900e83d26ddb385b6955c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["85a883878c0af761245ab048babc63d099f835f3","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","1291e4568eb7d9463d751627596ef14baf4c1603","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}