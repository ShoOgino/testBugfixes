{"path":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","commits":[{"id":"8723a3379c08ae0b4ba0cf4f246306f86ad8362d","date":1287582680,"type":0,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"/dev/null","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    // prevent any merges from happening.\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8017ab6544f30f93b106e419e7298173bad77f69","date":1287608126,"type":0,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"/dev/null","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    // prevent any merges from happening.\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c60818c639aa9dceabd6640d1e2fd7c80c8a11a4","date":1287840066,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    // prevent any merges from happening.\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b12d3e81e0f95a4527b5703953c503f71120ffcc","date":1288080933,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    // prevent any merges from happening.\n    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES);\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = new IndexWriterConfig(TEST_VERSION_CURRENT, null);\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d5efdc434c21e20adcb20d316e227be9eaf377d2","date":1292842437,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":0,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"/dev/null","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n\n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70ad682703b8585f5d0a637efec044d57ec05efb","date":1294259117,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes();\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8bcd63cc99c783b21344aeeebc8c04db29770205","date":1295239623,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForOptimize((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae31c2e5298b269d50b60961fe85afc5fbe873c3","date":1295256360,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForOptimize((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n\n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n\n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForOptimize((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMB((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d14e8d18c0e3970c20354dbeeb49da11bd587fbd","date":1321041051,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedForceMerge#testByteSizeLimit().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestSizeBoundedOptimize#testByteSizeLimit().mjava","sourceNew":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during forceMerge.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForForcedMerge((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.forceMerge(1);\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","sourceOld":"  public void testByteSizeLimit() throws Exception {\n    // tests that the max merge size constraint is applied during optimize.\n    Directory dir = new RAMDirectory();\n\n    // Prepare an index w/ several small segments and a large one.\n    IndexWriterConfig conf = newWriterConfig();\n    IndexWriter writer = new IndexWriter(dir, conf);\n    final int numSegments = 15;\n    for (int i = 0; i < numSegments; i++) {\n      int numDocs = i == 7 ? 30 : 1;\n      addDocs(writer, numDocs);\n    }\n    writer.close();\n\n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    double min = sis.info(0).sizeInBytes(true);\n\n    conf = newWriterConfig();\n    LogByteSizeMergePolicy lmp = new LogByteSizeMergePolicy();\n    lmp.setMaxMergeMBForOptimize((min + 1) / (1 << 20));\n    conf.setMergePolicy(lmp);\n    \n    writer = new IndexWriter(dir, conf);\n    writer.optimize();\n    writer.close();\n\n    // Should only be 3 segments in the index, because one of them exceeds the size limit\n    sis = new SegmentInfos();\n    sis.read(dir);\n    assertEquals(3, sis.size());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"c60818c639aa9dceabd6640d1e2fd7c80c8a11a4":["8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"70ad682703b8585f5d0a637efec044d57ec05efb":["b12d3e81e0f95a4527b5703953c503f71120ffcc","d5efdc434c21e20adcb20d316e227be9eaf377d2"],"8bcd63cc99c783b21344aeeebc8c04db29770205":["d5efdc434c21e20adcb20d316e227be9eaf377d2"],"b12d3e81e0f95a4527b5703953c503f71120ffcc":["8017ab6544f30f93b106e419e7298173bad77f69","c60818c639aa9dceabd6640d1e2fd7c80c8a11a4"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["8bcd63cc99c783b21344aeeebc8c04db29770205"],"d5efdc434c21e20adcb20d316e227be9eaf377d2":["c60818c639aa9dceabd6640d1e2fd7c80c8a11a4"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d5efdc434c21e20adcb20d316e227be9eaf377d2"],"ae31c2e5298b269d50b60961fe85afc5fbe873c3":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8bcd63cc99c783b21344aeeebc8c04db29770205"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["70ad682703b8585f5d0a637efec044d57ec05efb","8bcd63cc99c783b21344aeeebc8c04db29770205"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"8017ab6544f30f93b106e419e7298173bad77f69":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"8723a3379c08ae0b4ba0cf4f246306f86ad8362d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd"]},"commit2Childs":{"c60818c639aa9dceabd6640d1e2fd7c80c8a11a4":["b12d3e81e0f95a4527b5703953c503f71120ffcc","d5efdc434c21e20adcb20d316e227be9eaf377d2"],"70ad682703b8585f5d0a637efec044d57ec05efb":["29ef99d61cda9641b6250bf9567329a6e65f901d"],"8bcd63cc99c783b21344aeeebc8c04db29770205":["d14e8d18c0e3970c20354dbeeb49da11bd587fbd","ae31c2e5298b269d50b60961fe85afc5fbe873c3","29ef99d61cda9641b6250bf9567329a6e65f901d"],"b12d3e81e0f95a4527b5703953c503f71120ffcc":["70ad682703b8585f5d0a637efec044d57ec05efb"],"d14e8d18c0e3970c20354dbeeb49da11bd587fbd":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d5efdc434c21e20adcb20d316e227be9eaf377d2":["70ad682703b8585f5d0a637efec044d57ec05efb","8bcd63cc99c783b21344aeeebc8c04db29770205","7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["ae31c2e5298b269d50b60961fe85afc5fbe873c3"],"ae31c2e5298b269d50b60961fe85afc5fbe873c3":[],"29ef99d61cda9641b6250bf9567329a6e65f901d":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","8017ab6544f30f93b106e419e7298173bad77f69","8723a3379c08ae0b4ba0cf4f246306f86ad8362d"],"8017ab6544f30f93b106e419e7298173bad77f69":["b12d3e81e0f95a4527b5703953c503f71120ffcc"],"8723a3379c08ae0b4ba0cf4f246306f86ad8362d":["c60818c639aa9dceabd6640d1e2fd7c80c8a11a4","8017ab6544f30f93b106e419e7298173bad77f69"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["ae31c2e5298b269d50b60961fe85afc5fbe873c3","29ef99d61cda9641b6250bf9567329a6e65f901d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}