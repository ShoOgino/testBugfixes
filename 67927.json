{"path":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","commits":[{"id":"c903c3d15906a3da96b8c0c2fb704491005fdbdb","date":1453508227,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a258fbb26824fd104ed795e5d9033d2d040049ee","date":1453508252,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c26f00b574427b55127e869b935845554afde1fa","date":1310252513,"type":1,"author":"Steven Rowe","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"77d924c3b8deab5881ed0d996d597a4ea5bbc40a","date":1316977817,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only WhitespaceTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertNotNull(\"expecting only WhitespaceTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting WhitespaceTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"efe33d200a98c0d3bb8038d038fd5ed68f8e54a6","date":1316985050,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only KeywordTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(KeywordTokenizer.class.getName());\n    assertNotNull(\"expecting only KeywordTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting KeywordTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"08970e5b8411182a29412c177eff67ec1110095b","date":1366640815,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eafa8c5eabc3dacd34680054e6a33bda024080ac","date":1367691488,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 1, new int[]{2,2,2,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 2, new int[]{3,3,3,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 3, new int[]{4,4,4,3,3}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 4, new int[]{5,5,5,4,4}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 5, new int[]{6,6,6,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 6, new int[]{8,8,8,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 7, new int[]{9,9,9,7,7}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 8, new int[]{10,10,10,8,8}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7043e7411cdd4dbbc872bf9fedd21231168cd5b8","date":1426087141,"type":4,"author":"Ryan McKinley","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":null,"sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9ae131ec9cda430231fcefc081a4c4f5d29238ee","date":1426094638,"type":0,"author":"David Wayne Smiley","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","date":1465936684,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"128099577578723971decd2fd3c3b0a043ff9855","date":1473703890,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"89424def13674ea17829b41c5883c54ecc31a132","date":1473767373,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"205f0e81aafd8115e4cd61788ef9e7a6476aa175","date":1530097523,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardFilter\");\n    assertNotNull(\"Expcting StandardFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e98520789adb1d5ad05afb4956eca0944a929688","date":1592430701,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","pathOld":"solr/core/src/test/org/apache/solr/handler/FieldAnalysisRequestHandlerTest#testHandleAnalysisRequest().mjava","sourceNew":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  @SuppressWarnings({\"unchecked\"})\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    @SuppressWarnings({\"rawtypes\"})\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    @SuppressWarnings({\"rawtypes\"})\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","sourceOld":"  /**\n   * Tests the {@link FieldAnalysisRequestHandler#handleAnalysisRequest(org.apache.solr.client.solrj.request.FieldAnalysisRequest,\n   * org.apache.solr.schema.IndexSchema)}\n   */\n  @Test\n  public void testHandleAnalysisRequest() throws Exception {\n\n    FieldAnalysisRequest request = new FieldAnalysisRequest();\n    request.addFieldName(\"whitetok\");\n    request.addFieldName(\"keywordtok\");\n    request.addFieldType(\"text\");\n    request.addFieldType(\"nametext\");\n    request.setFieldValue(\"the quick red fox jumped over the lazy brown dogs\");\n    request.setQuery(\"fox brown\");\n    request.setShowMatch(true);\n\n    NamedList<NamedList> result = handler.handleAnalysisRequest(request, h.getCore().getLatestSchema());\n    assertTrue(\"result is null and it shouldn't be\", result != null);\n\n    NamedList<NamedList> fieldTypes = result.get(\"field_types\");\n    assertNotNull(\"field_types should never be null\", fieldTypes);\n    NamedList<NamedList> textType = fieldTypes.get(\"text\");\n    assertNotNull(\"expecting result for field type 'text'\", textType);\n\n    NamedList<List<NamedList>> indexPart = textType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'text'\", indexPart);\n\n    List<NamedList> tokenList = indexPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expcting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 10);\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"<ALPHANUM>\", 30, 33, 7, new int[]{7,7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jumped\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazy\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dogs\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10}, null, false));\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(tokenList.size(), 8);\n    assertToken(tokenList.get(0), new TokenInfo(\"quick\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"red\", null, \"<ALPHANUM>\", 10, 13, 3, new int[]{3,3,3,3}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 14, 17, 4, new int[]{4,4,4,4}, null, true));\n    assertToken(tokenList.get(3), new TokenInfo(\"jump\", null, \"<ALPHANUM>\", 18, 24, 5, new int[]{5,5,5,5}, null, false));\n    assertToken(tokenList.get(4), new TokenInfo(\"over\", null, \"<ALPHANUM>\", 25, 29, 6, new int[]{6,6,6,6}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"lazi\", null, \"<ALPHANUM>\", 34, 38, 8, new int[]{8,8,8,8}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 39, 44, 9, new int[]{9,9,9,9}, null, true));\n    assertToken(tokenList.get(7), new TokenInfo(\"dog\", null, \"<ALPHANUM>\", 45, 49, 10, new int[]{10,10,10,10}, null, false));\n\n    NamedList<List<NamedList>> queryPart = textType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'text'\", queryPart);\n\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.standard.StandardTokenizer\");\n    assertNotNull(\"Expecting StandardTokenizer analysis breakdown\", tokenList);\n    assertEquals(\"Expecting StandardTokenizer to produce 2 tokens from '\" + request.getQuery() + \"'\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.LowerCaseFilter\");\n    assertNotNull(\"Expcting LowerCaseFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.core.StopFilter\");\n    assertNotNull(\"Expcting StopFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2}, null, false));\n    tokenList = queryPart.get(\"org.apache.lucene.analysis.en.PorterStemFilter\");\n    assertNotNull(\"Expcting PorterStemFilter analysis breakdown\", tokenList);\n    assertEquals(2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"<ALPHANUM>\", 0, 3, 1, new int[]{1,1,1,1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"<ALPHANUM>\", 4, 9, 2, new int[]{2,2,2,2}, null, false));\n\n    NamedList<NamedList> nameTextType = fieldTypes.get(\"nametext\");\n    assertNotNull(\"expecting result for field type 'nametext'\", nameTextType);\n\n    indexPart = nameTextType.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field type 'nametext'\", indexPart);\n\n    tokenList = indexPart.get(\"org.apache.lucene.analysis.core.WhitespaceTokenizer\");\n    assertNotNull(\"Expcting WhitespaceTokenizer analysis breakdown\", tokenList);\n    assertEquals(10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = nameTextType.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field type 'nametext'\", queryPart);\n    tokenList = queryPart.get(WhitespaceTokenizer.class.getName());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> fieldNames = result.get(\"field_names\");\n    assertNotNull(\"field_nameds should never be null\", fieldNames);\n\n    NamedList<NamedList> whitetok = fieldNames.get(\"whitetok\");\n    assertNotNull(\"expecting result for field 'whitetok'\", whitetok);\n\n    indexPart = whitetok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'whitetok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 10 tokens\", 10, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"quick\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n    assertToken(tokenList.get(2), new TokenInfo(\"red\", null, \"word\", 10, 13, 3, new int[]{3}, null, false));\n    assertToken(tokenList.get(3), new TokenInfo(\"fox\", null, \"word\", 14, 17, 4, new int[]{4}, null, true));\n    assertToken(tokenList.get(4), new TokenInfo(\"jumped\", null, \"word\", 18, 24, 5, new int[]{5}, null, false));\n    assertToken(tokenList.get(5), new TokenInfo(\"over\", null, \"word\", 25, 29, 6, new int[]{6}, null, false));\n    assertToken(tokenList.get(6), new TokenInfo(\"the\", null, \"word\", 30, 33, 7, new int[]{7}, null, false));\n    assertToken(tokenList.get(7), new TokenInfo(\"lazy\", null, \"word\", 34, 38, 8, new int[]{8}, null, false));\n    assertToken(tokenList.get(8), new TokenInfo(\"brown\", null, \"word\", 39, 44, 9, new int[]{9}, null, true));\n    assertToken(tokenList.get(9), new TokenInfo(\"dogs\", null, \"word\", 45, 49, 10, new int[]{10}, null, false));\n\n    queryPart = whitetok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'whitetok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 2 tokens\", 2, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox\", null, \"word\", 0, 3, 1, new int[]{1}, null, false));\n    assertToken(tokenList.get(1), new TokenInfo(\"brown\", null, \"word\", 4, 9, 2, new int[]{2}, null, false));\n\n    NamedList<NamedList> keywordtok = fieldNames.get(\"keywordtok\");\n    assertNotNull(\"expecting result for field 'keywordtok'\", keywordtok);\n\n    indexPart = keywordtok.get(\"index\");\n    assertNotNull(\"expecting an index token analysis for field 'keywordtok'\", indexPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, indexPart.size());\n    tokenList = indexPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"the quick red fox jumped over the lazy brown dogs\", null, \"word\", 0, 49, 1, new int[]{1}, null, false));\n\n    queryPart = keywordtok.get(\"query\");\n    assertNotNull(\"expecting a query token analysis for field 'keywordtok'\", queryPart);\n    assertEquals(\"expecting only MockTokenizer to be applied\", 1, queryPart.size());\n    tokenList = queryPart.get(MockTokenizer.class.getName());\n    assertNotNull(\"expecting only MockTokenizer to be applied\", tokenList);\n    assertEquals(\"expecting MockTokenizer to produce 1 token\", 1, tokenList.size());\n    assertToken(tokenList.get(0), new TokenInfo(\"fox brown\", null, \"word\", 0, 9, 1, new int[]{1}, null, false));\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9ae131ec9cda430231fcefc081a4c4f5d29238ee":["7043e7411cdd4dbbc872bf9fedd21231168cd5b8"],"c26f00b574427b55127e869b935845554afde1fa":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c903c3d15906a3da96b8c0c2fb704491005fdbdb"],"77d924c3b8deab5881ed0d996d597a4ea5bbc40a":["c26f00b574427b55127e869b935845554afde1fa"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["08970e5b8411182a29412c177eff67ec1110095b"],"efe33d200a98c0d3bb8038d038fd5ed68f8e54a6":["77d924c3b8deab5881ed0d996d597a4ea5bbc40a"],"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345":["9ae131ec9cda430231fcefc081a4c4f5d29238ee"],"e98520789adb1d5ad05afb4956eca0944a929688":["205f0e81aafd8115e4cd61788ef9e7a6476aa175"],"08970e5b8411182a29412c177eff67ec1110095b":["efe33d200a98c0d3bb8038d038fd5ed68f8e54a6"],"89424def13674ea17829b41c5883c54ecc31a132":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","128099577578723971decd2fd3c3b0a043ff9855"],"205f0e81aafd8115e4cd61788ef9e7a6476aa175":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","205f0e81aafd8115e4cd61788ef9e7a6476aa175"],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"7043e7411cdd4dbbc872bf9fedd21231168cd5b8":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345","89424def13674ea17829b41c5883c54ecc31a132"],"128099577578723971decd2fd3c3b0a043ff9855":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345"],"a258fbb26824fd104ed795e5d9033d2d040049ee":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["e98520789adb1d5ad05afb4956eca0944a929688"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","205f0e81aafd8115e4cd61788ef9e7a6476aa175"]},"commit2Childs":{"9ae131ec9cda430231fcefc081a4c4f5d29238ee":["b266fe0ac2172d4ad87cff12bd9bf9f8c8247345"],"c26f00b574427b55127e869b935845554afde1fa":["77d924c3b8deab5881ed0d996d597a4ea5bbc40a"],"77d924c3b8deab5881ed0d996d597a4ea5bbc40a":["efe33d200a98c0d3bb8038d038fd5ed68f8e54a6"],"eafa8c5eabc3dacd34680054e6a33bda024080ac":["7043e7411cdd4dbbc872bf9fedd21231168cd5b8"],"efe33d200a98c0d3bb8038d038fd5ed68f8e54a6":["08970e5b8411182a29412c177eff67ec1110095b"],"b266fe0ac2172d4ad87cff12bd9bf9f8c8247345":["89424def13674ea17829b41c5883c54ecc31a132","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","128099577578723971decd2fd3c3b0a043ff9855"],"08970e5b8411182a29412c177eff67ec1110095b":["eafa8c5eabc3dacd34680054e6a33bda024080ac"],"e98520789adb1d5ad05afb4956eca0944a929688":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"89424def13674ea17829b41c5883c54ecc31a132":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"205f0e81aafd8115e4cd61788ef9e7a6476aa175":["e98520789adb1d5ad05afb4956eca0944a929688","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"c903c3d15906a3da96b8c0c2fb704491005fdbdb":["c26f00b574427b55127e869b935845554afde1fa"],"7043e7411cdd4dbbc872bf9fedd21231168cd5b8":["9ae131ec9cda430231fcefc081a4c4f5d29238ee"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["c26f00b574427b55127e869b935845554afde1fa","c903c3d15906a3da96b8c0c2fb704491005fdbdb","a258fbb26824fd104ed795e5d9033d2d040049ee"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["205f0e81aafd8115e4cd61788ef9e7a6476aa175","0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"128099577578723971decd2fd3c3b0a043ff9855":["89424def13674ea17829b41c5883c54ecc31a132"],"a258fbb26824fd104ed795e5d9033d2d040049ee":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","a258fbb26824fd104ed795e5d9033d2d040049ee","cd5edd1f2b162a5cfa08efd17851a07373a96817","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}