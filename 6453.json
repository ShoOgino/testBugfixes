{"path":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","commits":[{"id":"e3ce1ef883d26aa73919aa2d53991726e96caa13","date":1445421402,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"/dev/null","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);\n    out.writeVInt(numDims);\n    out.writeVInt(maxPointsInLeafNode);\n    out.writeVInt(bytesPerDim);\n\n    out.writeVInt(numLeaves);\n\n    // NOTE: splitPackedValues[0] is unused, because nodeID is 1-based:\n    out.writeBytes(splitPackedValues, 0, splitPackedValues.length);\n\n    for (int i=0;i<leafBlockFPs.length;i++) {\n      out.writeVLong(leafBlockFPs[i]);\n    }\n\n    return indexFP;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ca792c26af46bd6c4a08d81117c60440cf6a7e3d","date":1445938295,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);\n    out.writeVInt(numDims);\n    out.writeVInt(maxPointsInLeafNode);\n    out.writeVInt(bytesPerDim);\n\n    out.writeVInt(numLeaves);\n\n    // NOTE: splitPackedValues[0] is unused, because nodeID is 1-based:\n    out.writeBytes(splitPackedValues, 0, splitPackedValues.length);\n\n    for (int i=0;i<leafBlockFPs.length;i++) {\n      out.writeVLong(leafBlockFPs[i]);\n    }\n\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1eee4175312c41f89aa23427f9e4edfc00deeaac","date":1446373190,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1904709ea0185dc04e3d77ea01c79e909caf2796","date":1447006699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b9f70b31079ec002469ee49df3b8f9bd8d10df23","date":1447755747,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n    //System.out.println(\"LEAVES: \" + numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1786be6a11f9cf5e48ce84869d1bb71e9c02f966","date":1448381196,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet = new LongBitSet(pointCount);\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    //System.out.println(\"innerNodeCount=\" + innerNodeCount);\n\n    if (1+2*innerNodeCount >= Integer.MAX_VALUE) {\n      throw new IllegalStateException(\"too many nodes; increase maxPointsInLeafNode (currently \" + maxPointsInLeafNode + \") and reindex\");\n    }\n\n    innerNodeCount--;\n\n    int numLeaves = (int) (innerNodeCount+1);\n    //System.out.println(\"LEAVES: \" + numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d53f98721d7cda12df9fd4b2e8e2c235be9ac494","date":1450448699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6bfe104fc023fadc9e709f8d17403d2cc61133fe","date":1454446396,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFiles(Collections.singleton(tempInput.getName()));\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f4e2dcb5e470991d83a63c264bfe20880d3b3c1","date":1454513757,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFiles(Collections.singleton(tempInput.getName()));\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"adc9dc8ef0ce617b940a039fd12f79e8b098cc7f","date":1456936072,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7","date":1456959208,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf1a614098b46c9c22afebd7b898ae4d1d2fc273","date":1457088850,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    //System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    byte[] minPacked = new byte[packedBytesLength];\n    byte[] maxPacked = new byte[packedBytesLength];\n    Arrays.fill(maxPacked, (byte) 0xff);\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPacked, maxPacked,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"950b7a6881d14da782b60444c11295e3ec50d41a","date":1458379095,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recruse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416f9e28900210be57b69bc12e2954fb98ed7ebe","date":1458479803,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        if (tempInput != null) {\n          IOUtils.closeWhileHandlingException(tempInput);\n        }\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        tempInput = null;\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      ordBitSet = new LongBitSet(pointCount);\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40","date":1458553787,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        if (tempInput != null) {\n          IOUtils.closeWhileHandlingException(tempInput);\n        }\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        tempInput = null;\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7","date":1482745036,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15fe6782474c00ec2ccc636052a025f8fe0bdb8b","date":1484743707,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"302d34f2c66e8d489ee13078305c330cbf67b226","date":1484754357,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":3,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDataDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDataDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      // even with selective indexing we create the sortedPointWriters so we can compress\n      // the leaf node data by common prefix\n      for(int dim=0;dim<numDataDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"452aca01058c8a4e6827ff9096664dde4a1d9790","date":1543310809,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numIndexDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numIndexDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numIndexDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numDataDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numDataDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      // even with selective indexing we create the sortedPointWriters so we can compress\n      // the leaf node data by common prefix\n      for(int dim=0;dim<numDataDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5db3224bb6ba28cb735531b45593da725fa751d1","date":1547448966,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numIndexDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Slices are created as they are needed\n    PathSlice[] sortedPointWriters = new PathSlice[numIndexDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numIndexDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Sort all docs once by each dimension:\n    PathSlice[] sortedPointWriters = new PathSlice[numIndexDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n      //long t0 = System.nanoTime();\n      for(int dim=0;dim<numIndexDims;dim++) {\n        sortedPointWriters[dim] = new PathSlice(sort(dim), 0, pointCount);\n      }\n      //long t1 = System.nanoTime();\n      //System.out.println(\"sort time: \" + ((t1-t0)/1000000.0) + \" msec\");\n\n      if (tempInput != null) {\n        tempDir.deleteFile(tempInput.getName());\n        tempInput = null;\n      } else {\n        assert heapPointWriter != null;\n        heapPointWriter = null;\n      }\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      for(PathSlice slice : sortedPointWriters) {\n        slice.writer.destroy();\n      }\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78bdc7d6906146edb12a1a6c1f765ba680ed5124","date":1549523533,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    PointWriter writer;\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n      writer = offlinePointWriter;\n      tempInput = null;\n    } else {\n      writer = heapPointWriter;\n      heapPointWriter = null;\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, writer,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    LongBitSet ordBitSet;\n    if (numIndexDims > 1) {\n      if (singleValuePerDoc) {\n        ordBitSet = new LongBitSet(maxDoc);\n      } else {\n        ordBitSet = new LongBitSet(pointCount);\n      }\n    } else {\n      ordBitSet = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    // Slices are created as they are needed\n    PathSlice[] sortedPointWriters = new PathSlice[numIndexDims];\n\n    // This is only used on exception; on normal code paths we close all files we opened:\n    List<Closeable> toCloseHeroically = new ArrayList<>();\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, sortedPointWriters,\n            ordBitSet, out,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            toCloseHeroically);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n        IOUtils.closeWhileHandlingException(toCloseHeroically);\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a69ebf290ab26d026cc224e517e0d93d931ac87b","date":1549869083,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    BKDRadixSelector.PathSlice points;\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n      points = new BKDRadixSelector.PathSlice(offlinePointWriter, 0, pointCount);\n      tempInput = null;\n    } else {\n      points = new BKDRadixSelector.PathSlice(heapPointWriter, 0, pointCount);\n      heapPointWriter = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    PointWriter writer;\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n      writer = offlinePointWriter;\n      tempInput = null;\n    } else {\n      writer = heapPointWriter;\n      heapPointWriter = null;\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, writer,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"76a51551f05a6c96a115b5a656837ecc8fd0b1ff","date":1551422476,"type":3,"author":"iverase","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (heapPointWriter == null && tempInput == null) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    BKDRadixSelector.PathSlice points;\n    if (offlinePointWriter != null) {\n      offlinePointWriter.close();\n      points = new BKDRadixSelector.PathSlice(offlinePointWriter, 0, pointCount);\n      tempInput = null;\n    } else {\n      points = new BKDRadixSelector.PathSlice(heapPointWriter, 0, pointCount);\n      heapPointWriter = null;\n    }\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c7f06758793500ca773d0df1037290e6e404fb33","date":1562230223,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0f206e78bea6261260b24c406e920d05c7ca2f3","date":1570809619,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0b597c65628ca9e73913a07e81691f8229bae35","date":1571224353,"type":3,"author":"jimczi","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue, maxPackedValue,\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d3929a60a731a8848bb9bc0bbfd3c5e3d59195e7","date":1588412059,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, 0, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, maxPointsInLeafNode, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n\n    long countPerLeaf = pointCount;\n    long innerNodeCount = 1;\n\n    while (countPerLeaf > maxPointsInLeafNode) {\n      countPerLeaf = (countPerLeaf+1)/2;\n      innerNodeCount *= 2;\n    }\n\n    int numLeaves = (int) innerNodeCount;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, Math.toIntExact(countPerLeaf), leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"125e5eeb7e933deec0cc0510c2368fe1ec7c36ce","date":1589215155,"type":3,"author":"Ignacio Vera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numSplits*bytesPerDim)];\n    byte[] splitDimensionValues = new byte[numSplits];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(0, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            splitDimensionValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    scratchBytesRef1.bytes = splitPackedValues;\n    scratchBytesRef1.length = bytesPerDim;\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, maxPointsInLeafNode, leafNodes);\n    return indexFP;\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numLeaves*(1+bytesPerDim))];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(1, 0, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    //System.out.println(\"Total nodes: \" + innerNodeCount);\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, maxPointsInLeafNode, leafBlockFPs, splitPackedValues);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"78e689a3b60e84c75dc6dd7b181a71fc19ef8482","date":1591689554,"type":5,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput,IndexOutput,IndexOutput).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter#finish(IndexOutput).mjava","sourceNew":"  /** Writes the BKD tree to the provided {@link IndexOutput}s and returns a {@link Runnable} that\n   *  writes the index of the tree if at least one point has been added, or {@code null} otherwise. */\n  public Runnable finish(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      return null;\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numSplits*bytesPerDim)];\n    byte[] splitDimensionValues = new byte[numSplits];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    final long dataStartFP = dataOut.getFilePointer();\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(0, numLeaves, points,\n             dataOut, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            splitDimensionValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    scratchBytesRef1.bytes = splitPackedValues;\n    scratchBytesRef1.length = bytesPerDim;\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    return () -> {\n      // Write index:\n      try {\n        writeIndex(metaOut, indexOut, maxPointsInLeafNode, leafNodes, dataStartFP);\n      } catch (IOException e) {\n        throw new UncheckedIOException(e);\n      }\n    };\n  }\n\n","sourceOld":"  /** Writes the BKD tree to the provided {@link IndexOutput} and returns the file offset where index was written. */\n  public long finish(IndexOutput out) throws IOException {\n    // System.out.println(\"\\nBKDTreeWriter.finish pointCount=\" + pointCount + \" out=\" + out + \" heapWriter=\" + heapPointWriter);\n\n    // TODO: specialize the 1D case?  it's much faster at indexing time (no partitioning on recurse...)\n\n    // Catch user silliness:\n    if (finished == true) {\n      throw new IllegalStateException(\"already finished\");\n    }\n\n    if (pointCount == 0) {\n      throw new IllegalStateException(\"must index at least one point\");\n    }\n\n    //mark as finished\n    finished = true;\n\n    pointWriter.close();\n    BKDRadixSelector.PathSlice points = new BKDRadixSelector.PathSlice(pointWriter, 0, pointCount);\n    //clean up pointers\n    tempInput = null;\n    pointWriter = null;\n\n    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);\n    final int numSplits = numLeaves - 1;\n\n    checkMaxLeafNodeCount(numLeaves);\n\n    // NOTE: we could save the 1+ here, to use a bit less heap at search time, but then we'd need a somewhat costly check at each\n    // step of the recursion to recompute the split dim:\n\n    // Indexed by nodeID, but first (root) nodeID is 1.  We do 1+ because the lead byte at each recursion says which dim we split on.\n    byte[] splitPackedValues = new byte[Math.toIntExact(numSplits*bytesPerDim)];\n    byte[] splitDimensionValues = new byte[numSplits];\n\n    // +1 because leaf count is power of 2 (e.g. 8), and innerNodeCount is power of 2 minus 1 (e.g. 7)\n    long[] leafBlockFPs = new long[numLeaves];\n\n    // Make sure the math above \"worked\":\n    assert pointCount / numLeaves <= maxPointsInLeafNode: \"pointCount=\" + pointCount + \" numLeaves=\" + numLeaves + \" maxPointsInLeafNode=\" + maxPointsInLeafNode;\n\n    //We re-use the selector so we do not need to create an object every time.\n    BKDRadixSelector radixSelector = new BKDRadixSelector(numDataDims, numIndexDims, bytesPerDim, maxPointsSortInHeap, tempDir, tempFileNamePrefix);\n\n    boolean success = false;\n    try {\n\n      final int[] parentSplits = new int[numIndexDims];\n      build(0, numLeaves, points,\n             out, radixSelector,\n            minPackedValue.clone(), maxPackedValue.clone(),\n            parentSplits,\n            splitPackedValues,\n            splitDimensionValues,\n            leafBlockFPs,\n            new int[maxPointsInLeafNode]);\n      assert Arrays.equals(parentSplits, new int[numIndexDims]);\n\n      // If no exception, we should have cleaned everything up:\n      assert tempDir.getCreatedFiles().isEmpty();\n      //long t2 = System.nanoTime();\n      //System.out.println(\"write time: \" + ((t2-t1)/1000000.0) + \" msec\");\n\n      success = true;\n    } finally {\n      if (success == false) {\n        IOUtils.deleteFilesIgnoringExceptions(tempDir, tempDir.getCreatedFiles());\n      }\n    }\n\n    scratchBytesRef1.bytes = splitPackedValues;\n    scratchBytesRef1.length = bytesPerDim;\n    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() {\n      @Override\n      public long getLeafLP(int index) {\n        return leafBlockFPs[index];\n      }\n\n      @Override\n      public BytesRef getSplitValue(int index) {\n        scratchBytesRef1.offset = index * bytesPerDim;\n        return scratchBytesRef1;\n      }\n\n      @Override\n      public int getSplitDimension(int index) {\n        return splitDimensionValues[index] & 0xff;\n      }\n\n      @Override\n      public int numLeaves() {\n        return leafBlockFPs.length;\n      }\n    };\n\n    // Write index:\n    long indexFP = out.getFilePointer();\n    writeIndex(out, maxPointsInLeafNode, leafNodes);\n    return indexFP;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"452aca01058c8a4e6827ff9096664dde4a1d9790":["f6652c943595e92c187ee904c382863013eae28f"],"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"5db3224bb6ba28cb735531b45593da725fa751d1":["452aca01058c8a4e6827ff9096664dde4a1d9790"],"f6652c943595e92c187ee904c382863013eae28f":["15fe6782474c00ec2ccc636052a025f8fe0bdb8b"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["5db3224bb6ba28cb735531b45593da725fa751d1"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["a69ebf290ab26d026cc224e517e0d93d931ac87b"],"b9f70b31079ec002469ee49df3b8f9bd8d10df23":["1904709ea0185dc04e3d77ea01c79e909caf2796"],"8f4e2dcb5e470991d83a63c264bfe20880d3b3c1":["6bfe104fc023fadc9e709f8d17403d2cc61133fe"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"1eee4175312c41f89aa23427f9e4edfc00deeaac":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["125e5eeb7e933deec0cc0510c2368fe1ec7c36ce"],"d3929a60a731a8848bb9bc0bbfd3c5e3d59195e7":["d0f206e78bea6261260b24c406e920d05c7ca2f3"],"950b7a6881d14da782b60444c11295e3ec50d41a":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"d53f98721d7cda12df9fd4b2e8e2c235be9ac494":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["d53f98721d7cda12df9fd4b2e8e2c235be9ac494"],"15fe6782474c00ec2ccc636052a025f8fe0bdb8b":["b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7"],"1786be6a11f9cf5e48ce84869d1bb71e9c02f966":["b9f70b31079ec002469ee49df3b8f9bd8d10df23"],"1904709ea0185dc04e3d77ea01c79e909caf2796":["1eee4175312c41f89aa23427f9e4edfc00deeaac"],"e3ce1ef883d26aa73919aa2d53991726e96caa13":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"adc9dc8ef0ce617b940a039fd12f79e8b098cc7f":["d53f98721d7cda12df9fd4b2e8e2c235be9ac494"],"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40","b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7"],"302d34f2c66e8d489ee13078305c330cbf67b226":["f03e4bed5023ec3ef93a771b8888cae991cf448d","15fe6782474c00ec2ccc636052a025f8fe0bdb8b"],"c7f06758793500ca773d0df1037290e6e404fb33":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"],"125e5eeb7e933deec0cc0510c2368fe1ec7c36ce":["d3929a60a731a8848bb9bc0bbfd3c5e3d59195e7"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["e3ce1ef883d26aa73919aa2d53991726e96caa13"],"d0f206e78bea6261260b24c406e920d05c7ca2f3":["c7f06758793500ca773d0df1037290e6e404fb33"],"a69ebf290ab26d026cc224e517e0d93d931ac87b":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["950b7a6881d14da782b60444c11295e3ec50d41a"],"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7":["adc9dc8ef0ce617b940a039fd12f79e8b098cc7f"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["d53f98721d7cda12df9fd4b2e8e2c235be9ac494","879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"b0b597c65628ca9e73913a07e81691f8229bae35":["c7f06758793500ca773d0df1037290e6e404fb33","d0f206e78bea6261260b24c406e920d05c7ca2f3"]},"commit2Childs":{"452aca01058c8a4e6827ff9096664dde4a1d9790":["5db3224bb6ba28cb735531b45593da725fa751d1"],"51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40":["b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"5db3224bb6ba28cb735531b45593da725fa751d1":["78bdc7d6906146edb12a1a6c1f765ba680ed5124"],"f6652c943595e92c187ee904c382863013eae28f":["452aca01058c8a4e6827ff9096664dde4a1d9790"],"78bdc7d6906146edb12a1a6c1f765ba680ed5124":["a69ebf290ab26d026cc224e517e0d93d931ac87b"],"76a51551f05a6c96a115b5a656837ecc8fd0b1ff":["c7f06758793500ca773d0df1037290e6e404fb33"],"b9f70b31079ec002469ee49df3b8f9bd8d10df23":["1786be6a11f9cf5e48ce84869d1bb71e9c02f966"],"8f4e2dcb5e470991d83a63c264bfe20880d3b3c1":[],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e3ce1ef883d26aa73919aa2d53991726e96caa13"],"1eee4175312c41f89aa23427f9e4edfc00deeaac":["1904709ea0185dc04e3d77ea01c79e909caf2796"],"6bfe104fc023fadc9e709f8d17403d2cc61133fe":["8f4e2dcb5e470991d83a63c264bfe20880d3b3c1"],"78e689a3b60e84c75dc6dd7b181a71fc19ef8482":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d3929a60a731a8848bb9bc0bbfd3c5e3d59195e7":["125e5eeb7e933deec0cc0510c2368fe1ec7c36ce"],"950b7a6881d14da782b60444c11295e3ec50d41a":["416f9e28900210be57b69bc12e2954fb98ed7ebe"],"d53f98721d7cda12df9fd4b2e8e2c235be9ac494":["6bfe104fc023fadc9e709f8d17403d2cc61133fe","adc9dc8ef0ce617b940a039fd12f79e8b098cc7f","cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"15fe6782474c00ec2ccc636052a025f8fe0bdb8b":["f6652c943595e92c187ee904c382863013eae28f","302d34f2c66e8d489ee13078305c330cbf67b226"],"1786be6a11f9cf5e48ce84869d1bb71e9c02f966":["d53f98721d7cda12df9fd4b2e8e2c235be9ac494"],"1904709ea0185dc04e3d77ea01c79e909caf2796":["b9f70b31079ec002469ee49df3b8f9bd8d10df23"],"e3ce1ef883d26aa73919aa2d53991726e96caa13":["ca792c26af46bd6c4a08d81117c60440cf6a7e3d"],"adc9dc8ef0ce617b940a039fd12f79e8b098cc7f":["879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7"],"b35cfd3fa0a5c9e066b0256c4818af1d2a9f22d7":["15fe6782474c00ec2ccc636052a025f8fe0bdb8b","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["302d34f2c66e8d489ee13078305c330cbf67b226"],"302d34f2c66e8d489ee13078305c330cbf67b226":[],"c7f06758793500ca773d0df1037290e6e404fb33":["d0f206e78bea6261260b24c406e920d05c7ca2f3","b0b597c65628ca9e73913a07e81691f8229bae35"],"ca792c26af46bd6c4a08d81117c60440cf6a7e3d":["1eee4175312c41f89aa23427f9e4edfc00deeaac"],"125e5eeb7e933deec0cc0510c2368fe1ec7c36ce":["78e689a3b60e84c75dc6dd7b181a71fc19ef8482"],"416f9e28900210be57b69bc12e2954fb98ed7ebe":["51b2a4c0f6c28a8ba7c41911b421cea2ede8ef40"],"a69ebf290ab26d026cc224e517e0d93d931ac87b":["76a51551f05a6c96a115b5a656837ecc8fd0b1ff"],"d0f206e78bea6261260b24c406e920d05c7ca2f3":["d3929a60a731a8848bb9bc0bbfd3c5e3d59195e7","b0b597c65628ca9e73913a07e81691f8229bae35"],"879e8cf5ab626b9bf29f1ef603e3a28601fcb1a7":["cf1a614098b46c9c22afebd7b898ae4d1d2fc273"],"cf1a614098b46c9c22afebd7b898ae4d1d2fc273":["950b7a6881d14da782b60444c11295e3ec50d41a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b0b597c65628ca9e73913a07e81691f8229bae35":[]},"heads":["8f4e2dcb5e470991d83a63c264bfe20880d3b3c1","302d34f2c66e8d489ee13078305c330cbf67b226","cd5edd1f2b162a5cfa08efd17851a07373a96817","b0b597c65628ca9e73913a07e81691f8229bae35"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}