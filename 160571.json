{"path":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"406e7055a3e99d3fa6ce49a555a51dd18b321806","date":1282520243,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":["d736930237c54e1516a9e3bae803c92ff19ec4e5"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7f367dfb9086b92a13c77e2d31112c715cd4502c","date":1290190924,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3bb13258feba31ab676502787ab2e1779f129b7a","date":1291596436,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = MultiFields.getDeletedDocs(reader);\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (reader.isDeleted(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (reader.isDeleted(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b0c7a8f7304b75b1528814c5820fa23a96816c27","date":1298314239,"type":3,"author":"Michael Busch","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"14ec33385f6fbb6ce172882d14605790418a5d31","date":1298910796,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1224a4027481acce15495b03bce9b48b93b42722","date":1300792329,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d619839baa8ce5503e496b94a9e42ad6f079293f","date":1301309428,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3e06be49006ecac364d39d12b9c9f74882f9b9f","date":1304289513,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field \n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n        \n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits delDocs = reader.getDeletedDocs();\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (delDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (delDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0061262413ecc163d6eebba1b5c43ab91a0c2dc5","date":1311195279,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,MergeState.IndexReaderAndLiveDocs,FieldsReader).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/SegmentMerger#copyFieldsWithDeletions(FieldsWriter,IndexReader,FieldsReader).mjava","sourceNew":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final MergeState.IndexReaderAndLiveDocs reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.reader.maxDoc();\n    final Bits liveDocs = reader.liveDocs;\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","sourceOld":"  private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,\n                                      final FieldsReader matchingFieldsReader)\n    throws IOException, MergeAbortedException, CorruptIndexException {\n    int docCount = 0;\n    final int maxDoc = reader.maxDoc();\n    final Bits liveDocs = reader.getLiveDocs();\n    assert liveDocs != null;\n    if (matchingFieldsReader != null) {\n      // We can bulk-copy because the fieldInfos are \"congruent\"\n      for (int j = 0; j < maxDoc;) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          ++j;\n          continue;\n        }\n        // We can optimize this case (doing a bulk byte copy) since the field\n        // numbers are identical\n        int start = j, numDocs = 0;\n        do {\n          j++;\n          numDocs++;\n          if (j >= maxDoc) break;\n          if (!liveDocs.get(j)) {\n            j++;\n            break;\n          }\n        } while(numDocs < MAX_RAW_MERGE_DOCS);\n\n        IndexInput stream = matchingFieldsReader.rawDocs(rawDocLengths, start, numDocs);\n        fieldsWriter.addRawDocuments(stream, rawDocLengths, numDocs);\n        docCount += numDocs;\n        checkAbort.work(300 * numDocs);\n      }\n    } else {\n      for (int j = 0; j < maxDoc; j++) {\n        if (!liveDocs.get(j)) {\n          // skip deleted docs\n          continue;\n        }\n        // NOTE: it's very important to first assign to doc then pass it to\n        // termVectorsWriter.addAllDocVectors; see LUCENE-1282\n        Document doc = reader.document(j);\n        fieldsWriter.addDocument(doc, fieldInfos);\n        docCount++;\n        checkAbort.work(300);\n      }\n    }\n    return docCount;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["d619839baa8ce5503e496b94a9e42ad6f079293f","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["1224a4027481acce15495b03bce9b48b93b42722","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["9454a6510e2db155fb01faa5c049b06ece95fab9","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["406e7055a3e99d3fa6ce49a555a51dd18b321806"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"14ec33385f6fbb6ce172882d14605790418a5d31":["b0c7a8f7304b75b1528814c5820fa23a96816c27"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["3bb13258feba31ab676502787ab2e1779f129b7a","1224a4027481acce15495b03bce9b48b93b42722"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["7f367dfb9086b92a13c77e2d31112c715cd4502c"],"a3776dccca01c11e7046323cfad46a3b4a471233":["1224a4027481acce15495b03bce9b48b93b42722","b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b0c7a8f7304b75b1528814c5820fa23a96816c27"],"3bb13258feba31ab676502787ab2e1779f129b7a":["406e7055a3e99d3fa6ce49a555a51dd18b321806","7f367dfb9086b92a13c77e2d31112c715cd4502c"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5"],"1224a4027481acce15495b03bce9b48b93b42722":["14ec33385f6fbb6ce172882d14605790418a5d31"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"0061262413ecc163d6eebba1b5c43ab91a0c2dc5":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["0061262413ecc163d6eebba1b5c43ab91a0c2dc5","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"b3e06be49006ecac364d39d12b9c9f74882f9b9f":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"7f367dfb9086b92a13c77e2d31112c715cd4502c":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","b0c7a8f7304b75b1528814c5820fa23a96816c27","3bb13258feba31ab676502787ab2e1779f129b7a"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"14ec33385f6fbb6ce172882d14605790418a5d31":["1224a4027481acce15495b03bce9b48b93b42722"],"d619839baa8ce5503e496b94a9e42ad6f079293f":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"b0c7a8f7304b75b1528814c5820fa23a96816c27":["14ec33385f6fbb6ce172882d14605790418a5d31","bde51b089eb7f86171eb3406e38a274743f9b7ac"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"406e7055a3e99d3fa6ce49a555a51dd18b321806":["7f367dfb9086b92a13c77e2d31112c715cd4502c","3bb13258feba31ab676502787ab2e1779f129b7a"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["b3e06be49006ecac364d39d12b9c9f74882f9b9f"],"3bb13258feba31ab676502787ab2e1779f129b7a":["d619839baa8ce5503e496b94a9e42ad6f079293f"],"1224a4027481acce15495b03bce9b48b93b42722":["b3e06be49006ecac364d39d12b9c9f74882f9b9f","d619839baa8ce5503e496b94a9e42ad6f079293f","a3776dccca01c11e7046323cfad46a3b4a471233"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","406e7055a3e99d3fa6ce49a555a51dd18b321806"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}