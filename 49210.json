{"path":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","commits":[{"id":"622a708571e534680618b3c5e0c28ac539a47776","date":1517406892,"type":1,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap).mjava","sourceNew":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = docWriter.codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = docWriter.codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = docWriter.codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = docWriter.codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"49f1924bd448393fbdfef8b5ebed799f938169d3","date":1600069616,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","sourceNew":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = docWriter.codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = docWriter.codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0dcf8f79417865e5028d753e669fae06457e8369","date":1600073240,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","sourceNew":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = docWriter.codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = docWriter.codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9f56073cc34a5845a5b6b0978a769a0239068411","date":1600254322,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","sourceNew":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      TermVectorsReader reader = TEMP_TERM_VECTORS_FORMAT\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      // Don't pull a merge instance, since merge instances optimize for\n      // sequential access while term vectors will likely be accessed in random\n      // order here.\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = reader.get(sortMap == null ? docID : sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"65352f844eb9e9a677ec4eb2abced4404f08181d","date":1600297608,"type":3,"author":"noblepaul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/SortingTermVectorsConsumer#flush(Map[String,TermsHashPerField],SegmentWriteState,Sorter.DocMap,NormsProducer).mjava","sourceNew":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      TermVectorsReader reader = TEMP_TERM_VECTORS_FORMAT\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      // Don't pull a merge instance, since merge instances optimize for\n      // sequential access while term vectors will likely be accessed in random\n      // order here.\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = reader.get(sortMap == null ? docID : sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","sourceOld":"  @Override\n  void flush(Map<String, TermsHashPerField> fieldsToFlush, final SegmentWriteState state, Sorter.DocMap sortMap, NormsProducer norms) throws IOException {\n    super.flush(fieldsToFlush, state, sortMap, norms);\n    if (tmpDirectory != null) {\n      if (sortMap == null) {\n        // we're lucky the index is already sorted, just rename the temporary file and return\n        for (Map.Entry<String, String> entry : tmpDirectory.getTemporaryFiles().entrySet()) {\n          tmpDirectory.rename(entry.getValue(), entry.getKey());\n        }\n        return;\n      }\n      TermVectorsReader reader = codec.termVectorsFormat()\n          .vectorsReader(tmpDirectory, state.segmentInfo, state.fieldInfos, IOContext.DEFAULT);\n      TermVectorsReader mergeReader = reader.getMergeInstance();\n      TermVectorsWriter writer = codec.termVectorsFormat()\n          .vectorsWriter(state.directory, state.segmentInfo, IOContext.DEFAULT);\n      try {\n        reader.checkIntegrity();\n        for (int docID = 0; docID < state.segmentInfo.maxDoc(); docID++) {\n          Fields vectors = mergeReader.get(sortMap.newToOld(docID));\n          writeTermVectors(writer, vectors, state.fieldInfos);\n        }\n        writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());\n      } finally {\n        IOUtils.close(reader, writer);\n        IOUtils.deleteFiles(tmpDirectory,\n            tmpDirectory.getTemporaryFiles().values());\n      }\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["622a708571e534680618b3c5e0c28ac539a47776"],"0dcf8f79417865e5028d753e669fae06457e8369":["622a708571e534680618b3c5e0c28ac539a47776","49f1924bd448393fbdfef8b5ebed799f938169d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"622a708571e534680618b3c5e0c28ac539a47776":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"65352f844eb9e9a677ec4eb2abced4404f08181d":["0dcf8f79417865e5028d753e669fae06457e8369","9f56073cc34a5845a5b6b0978a769a0239068411"],"9f56073cc34a5845a5b6b0978a769a0239068411":["49f1924bd448393fbdfef8b5ebed799f938169d3"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["65352f844eb9e9a677ec4eb2abced4404f08181d"]},"commit2Childs":{"49f1924bd448393fbdfef8b5ebed799f938169d3":["0dcf8f79417865e5028d753e669fae06457e8369","9f56073cc34a5845a5b6b0978a769a0239068411"],"0dcf8f79417865e5028d753e669fae06457e8369":["65352f844eb9e9a677ec4eb2abced4404f08181d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["622a708571e534680618b3c5e0c28ac539a47776"],"622a708571e534680618b3c5e0c28ac539a47776":["49f1924bd448393fbdfef8b5ebed799f938169d3","0dcf8f79417865e5028d753e669fae06457e8369"],"65352f844eb9e9a677ec4eb2abced4404f08181d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9f56073cc34a5845a5b6b0978a769a0239068411":["65352f844eb9e9a677ec4eb2abced4404f08181d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}