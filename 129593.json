{"path":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","commits":[{"id":"85212dad4ed576c7f7e6c165ee19e597b7b4efc8","date":1507997740,"type":1,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,ClusterDataProvider,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Policy.Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getOperation();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          ClusterDataProvider cdp,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n      final ClusterDataProvider delegate = cdp;\n      cdp = new ClusterDataProvider() {\n        @Override\n        public Map<String, Object> getNodeValues(String node, Collection<String> tags) {\n          return delegate.getNodeValues(node, tags);\n        }\n\n        @Override\n        public Map<String, Map<String, List<ReplicaInfo>>> getReplicaInfo(String node, Collection<String> keys) {\n          return delegate.getReplicaInfo(node, keys);\n        }\n\n        @Override\n        public Collection<String> getNodes() {\n          return delegate.getNodes();\n        }\n\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(cdp, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(cdp);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Policy.Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getOperation();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"eaca6a0674512222004d9a2b0ca95d86bda20f1c","date":1508160449,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getOperation();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Policy.Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getOperation();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1bbcda32e5cd37ef61ea1190bacd080308e22070","date":1508850553,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getOperation();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d907c28c7fe6305eaec1756d51365f5149e1e41d","date":1512533044,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n        @Override\n        public String getPolicyNameByCollection(String coll) {\n          return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n              optionalPolicyMapping.get(coll) :\n              delegate.getPolicyNameByCollection(coll);\n        }\n      };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    Policy.Session session = null;\n    try {\n      session = SESSION_REF.get() != null ?\n          SESSION_REF.get().initOrGet(delegatingManager, autoScalingConfig.getPolicy()) :\n          autoScalingConfig.getPolicy().createSession(delegatingManager);\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      if (session != null && SESSION_REF.get() != null) SESSION_REF.get().updateSession(session);\n      policyMapping.remove();\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a4422b331d00607258b0ed3e43934306e67764aa","date":1513943901,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c14cf5d507f6a96d702a87ac21694d5efa725d56","date":1516118674,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Suggestion.ConditionType.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        /*ignore*/\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b94236357aaa22b76c10629851fe4e376e0cea82","date":1516710914,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Suggestion.ConditionType.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        /*ignore*/\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"27639bb5e041490ce599065875dd2f6d8beef62a","date":1532829373,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Suggestion.ConditionType.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() throws InterruptedException, IOException {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Suggestion.ConditionType.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        /*ignore*/\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"427edb17549d4bb82462a16eec4ee0533d12d5b7","date":1533006754,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Suggestion.ConditionType.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"042da0877b8e28fd372a8ed80d11c4506a466ad7","date":1534516670,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true)));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":["bccf7971a36bd151490117582a0a1a695081ead3"],"bugIntro":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"01322d51122b6cbe6b5ba6059fffba67798dae72","date":1539067228,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e5028c6838e2e49cb9da1cf70269851c049f107a","date":1584984100,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session session = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      session = sessionWrapper.session;\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(session);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6dc613ed6f75d1988140301ee8de8fdb056fa337","date":1588034757,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())\n              ); // logOK\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: \" + collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : \" + errorId + \"  \" +\n                  handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString()));\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4","date":1588172214,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())\n              ); // logOK\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b0249ced9c25c3b173d20c3ca74160b9eade78c","date":1591787635,"type":3,"author":"murblanc","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n    Policy.Session origSession = null;\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n\n      }\n      origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      Policy.Session session = new Policy.Session(delegatingManager, origSession.policy, origSession.transaction);\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = session.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(session.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            session = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n    } finally {\n      policyMapping.remove();\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession(origSession);\n      }\n    }\n    return positions;\n  }\n\n","bugFix":["eaca6a0674512222004d9a2b0ca95d86bda20f1c","042da0877b8e28fd372a8ed80d11c4506a466ad7","d907c28c7fe6305eaec1756d51365f5149e1e41d","bccf7971a36bd151490117582a0a1a695081ead3","e5028c6838e2e49cb9da1cf70269851c049f107a"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"527adea7ce767368d7317339023e18e39702132e","date":1592163810,"type":3,"author":"Erick Erickson","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            @SuppressWarnings({\"rawtypes\"})\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd","date":1594731683,"type":3,"author":"Andrzej Bialecki","isMerge":false,"pathNew":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<Replica>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              Replica replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getProperties().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            @SuppressWarnings({\"rawtypes\"})\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<ReplicaInfo>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              ReplicaInfo replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getVariables().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            @SuppressWarnings({\"rawtypes\"})\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f504512a03d978990cbff30db0522b354e846db","date":1595247421,"type":4,"author":"Ishan Chattopadhyaya","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/solrj/src/java/org/apache/solr/client/solrj/cloud/autoscaling/PolicyHelper#getReplicaLocations(String,AutoScalingConfig,SolrCloudManager,Map[String,String],List[String],int,int,int,List[String]).mjava","sourceNew":null,"sourceOld":"  public static List<ReplicaPosition> getReplicaLocations(String collName, AutoScalingConfig autoScalingConfig,\n                                                          SolrCloudManager cloudManager,\n                                                          Map<String, String> optionalPolicyMapping,\n                                                          List<String> shardNames,\n                                                          int nrtReplicas,\n                                                          int tlogReplicas,\n                                                          int pullReplicas,\n                                                          List<String> nodesList) {\n    List<ReplicaPosition> positions = new ArrayList<>();\n    ThreadLocal<Map<String, String>> policyMapping = getPolicyMapping(cloudManager);\n    ClusterStateProvider stateProvider = new DelegatingClusterStateProvider(cloudManager.getClusterStateProvider()) {\n      @Override\n      public String getPolicyNameByCollection(String coll) {\n        return policyMapping.get() != null && policyMapping.get().containsKey(coll) ?\n            optionalPolicyMapping.get(coll) :\n            delegate.getPolicyNameByCollection(coll);\n      }\n    };\n    SolrCloudManager delegatingManager = new DelegatingCloudManager(cloudManager) {\n      @Override\n      public ClusterStateProvider getClusterStateProvider() {\n        return stateProvider;\n      }\n\n      @Override\n      public DistribStateManager getDistribStateManager() {\n        if (autoScalingConfig != null) {\n          return new DelegatingDistribStateManager(null) {\n            @Override\n            public AutoScalingConfig getAutoScalingConfig() {\n              return autoScalingConfig;\n            }\n          };\n        } else {\n          return super.getDistribStateManager();\n        }\n      }\n    };\n\n    policyMapping.set(optionalPolicyMapping);\n    SessionWrapper sessionWrapper = null;\n\n    try {\n      try {\n        SESSION_WRAPPPER_REF.set(sessionWrapper = getSession(delegatingManager));\n      } catch (Exception e) {\n        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"unable to get autoscaling policy session\", e);\n      }\n\n      Policy.Session origSession = sessionWrapper.session;\n      // new session needs to be created to avoid side-effects from per-collection policies\n      // TODO: refactor so cluster state cache is separate from storage of policies to avoid per cluster vs per collection interactions\n      // Need a Session that has all previous history of the original session, NOT filtered by what's present or not in Zookeeper\n      // (as does constructor Session(SolrCloudManager, Policy, Transaction)).\n      Policy.Session newSession = origSession.cloneToNewSession(delegatingManager);\n\n      Map<String, Double> diskSpaceReqd = new HashMap<>();\n      try {\n        DocCollection coll = cloudManager.getClusterStateProvider().getCollection(collName);\n        if (coll != null) {\n          for (String shardName : shardNames) {\n            Replica ldr = coll.getLeader(shardName);\n            if (ldr != null && cloudManager.getClusterStateProvider().getLiveNodes().contains(ldr.getNodeName())) {\n              Map<String, Map<String, List<Replica>>> details = cloudManager.getNodeStateProvider().getReplicaInfo(ldr.getNodeName(),\n                  Collections.singleton(FREEDISK.perReplicaValue));\n              Replica replicaInfo = details.getOrDefault(collName, emptyMap()).getOrDefault(shardName, singletonList(null)).get(0);\n              if (replicaInfo != null) {\n                Object idxSz = replicaInfo.getProperties().get(FREEDISK.perReplicaValue);\n                if (idxSz != null) {\n                  diskSpaceReqd.put(shardName, 1.5 * (Double) Variable.Type.FREEDISK.validate(null, idxSz, false));\n                }\n              }\n            }\n\n          }\n        }\n      } catch (IOException e) {\n        log.warn(\"Exception while reading disk free metric values for nodes to be used for collection: {}\", collName, e);\n      }\n\n\n      Map<Replica.Type, Integer> typeVsCount = new EnumMap<>(Replica.Type.class);\n      typeVsCount.put(Replica.Type.NRT, nrtReplicas);\n      typeVsCount.put(Replica.Type.TLOG, tlogReplicas);\n      typeVsCount.put(Replica.Type.PULL, pullReplicas);\n      for (String shardName : shardNames) {\n        int idx = 0;\n        for (Map.Entry<Replica.Type, Integer> e : typeVsCount.entrySet()) {\n          for (int i = 0; i < e.getValue(); i++) {\n            Suggester suggester = newSession.getSuggester(ADDREPLICA)\n                .hint(Hint.REPLICATYPE, e.getKey())\n                .hint(Hint.COLL_SHARD, new Pair<>(collName, shardName));\n            if (nodesList != null) {\n              for (String nodeName : nodesList) {\n                suggester = suggester.hint(Hint.TARGET_NODE, nodeName);\n              }\n            }\n            if (diskSpaceReqd.get(shardName) != null) {\n              suggester.hint(Hint.MINFREEDISK, diskSpaceReqd.get(shardName));\n            }\n            @SuppressWarnings({\"rawtypes\"})\n            SolrRequest op = suggester.getSuggestion();\n            if (op == null) {\n              String errorId = \"AutoScaling.error.diagnostics.\" + System.nanoTime();\n              Policy.Session sessionCopy = suggester.session;\n              log.error(\"errorId : {} {}\", errorId\n                  , handleExp(log, \"\", () -> Utils.writeJson(getDiagnostics(sessionCopy), new StringWriter(), true).toString())); // logOk\n\n              throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \" No node can satisfy the rules \" +\n                  Utils.toJSONString(Utils.getDeepCopy(newSession.expandedClauses, 4, true) + \" More details from logs in node : \"\n                      + Utils.getMDCNode() + \", errorId : \" + errorId));\n            }\n            newSession = suggester.getSession();\n            positions.add(new ReplicaPosition(shardName, ++idx, e.getKey(), op.getParams().get(NODE)));\n          }\n        }\n      }\n\n      // We're happy with the updated session based on the original one, so let's update what the wrapper would hand\n      // to the next computation that wants a session.\n      sessionWrapper.update(newSession);\n    } finally {\n      policyMapping.remove();\n      // We mark the wrapper (and its session) as being available to others.\n      if (sessionWrapper != null) {\n        sessionWrapper.returnSession();\n      }\n    }\n    return positions;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"b94236357aaa22b76c10629851fe4e376e0cea82":["a4422b331d00607258b0ed3e43934306e67764aa","c14cf5d507f6a96d702a87ac21694d5efa725d56"],"c14cf5d507f6a96d702a87ac21694d5efa725d56":["a4422b331d00607258b0ed3e43934306e67764aa"],"6dc613ed6f75d1988140301ee8de8fdb056fa337":["e5028c6838e2e49cb9da1cf70269851c049f107a"],"3f504512a03d978990cbff30db0522b354e846db":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e5028c6838e2e49cb9da1cf70269851c049f107a":["01322d51122b6cbe6b5ba6059fffba67798dae72"],"042da0877b8e28fd372a8ed80d11c4506a466ad7":["427edb17549d4bb82462a16eec4ee0533d12d5b7"],"01322d51122b6cbe6b5ba6059fffba67798dae72":["042da0877b8e28fd372a8ed80d11c4506a466ad7"],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["527adea7ce767368d7317339023e18e39702132e"],"427edb17549d4bb82462a16eec4ee0533d12d5b7":["27639bb5e041490ce599065875dd2f6d8beef62a"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["6dc613ed6f75d1988140301ee8de8fdb056fa337"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"527adea7ce767368d7317339023e18e39702132e":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"a4422b331d00607258b0ed3e43934306e67764aa":["d907c28c7fe6305eaec1756d51365f5149e1e41d"],"1bbcda32e5cd37ef61ea1190bacd080308e22070":["eaca6a0674512222004d9a2b0ca95d86bda20f1c"],"eaca6a0674512222004d9a2b0ca95d86bda20f1c":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"7b0249ced9c25c3b173d20c3ca74160b9eade78c":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"27639bb5e041490ce599065875dd2f6d8beef62a":["b94236357aaa22b76c10629851fe4e376e0cea82"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3f504512a03d978990cbff30db0522b354e846db"],"d907c28c7fe6305eaec1756d51365f5149e1e41d":["1bbcda32e5cd37ef61ea1190bacd080308e22070"]},"commit2Childs":{"b94236357aaa22b76c10629851fe4e376e0cea82":["27639bb5e041490ce599065875dd2f6d8beef62a"],"c14cf5d507f6a96d702a87ac21694d5efa725d56":["b94236357aaa22b76c10629851fe4e376e0cea82"],"6dc613ed6f75d1988140301ee8de8fdb056fa337":["fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4"],"3f504512a03d978990cbff30db0522b354e846db":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"85212dad4ed576c7f7e6c165ee19e597b7b4efc8":["eaca6a0674512222004d9a2b0ca95d86bda20f1c"],"e5028c6838e2e49cb9da1cf70269851c049f107a":["6dc613ed6f75d1988140301ee8de8fdb056fa337"],"042da0877b8e28fd372a8ed80d11c4506a466ad7":["01322d51122b6cbe6b5ba6059fffba67798dae72"],"01322d51122b6cbe6b5ba6059fffba67798dae72":["e5028c6838e2e49cb9da1cf70269851c049f107a"],"7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd":["3f504512a03d978990cbff30db0522b354e846db"],"427edb17549d4bb82462a16eec4ee0533d12d5b7":["042da0877b8e28fd372a8ed80d11c4506a466ad7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["85212dad4ed576c7f7e6c165ee19e597b7b4efc8"],"fe9f2c4a0d7ac164e4bdd4eee7f87131aec83fd4":["7b0249ced9c25c3b173d20c3ca74160b9eade78c"],"527adea7ce767368d7317339023e18e39702132e":["7e8ce2f9d2ddfcf5cfa7e73b8b2af287a2a276fd"],"a4422b331d00607258b0ed3e43934306e67764aa":["b94236357aaa22b76c10629851fe4e376e0cea82","c14cf5d507f6a96d702a87ac21694d5efa725d56"],"1bbcda32e5cd37ef61ea1190bacd080308e22070":["d907c28c7fe6305eaec1756d51365f5149e1e41d"],"eaca6a0674512222004d9a2b0ca95d86bda20f1c":["1bbcda32e5cd37ef61ea1190bacd080308e22070"],"27639bb5e041490ce599065875dd2f6d8beef62a":["427edb17549d4bb82462a16eec4ee0533d12d5b7"],"7b0249ced9c25c3b173d20c3ca74160b9eade78c":["527adea7ce767368d7317339023e18e39702132e"],"d907c28c7fe6305eaec1756d51365f5149e1e41d":["a4422b331d00607258b0ed3e43934306e67764aa"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}