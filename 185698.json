{"path":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7edb20114e86ec883b0b08bd624eee852c565c06","date":1273941247,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockAnalyzer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new SimpleAnalyzer(TEST_VERSION_CURRENT))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ade882efb2f2235dafb176284c1e35dbdb1c126","date":1274043418,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockAnalyzer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28427ef110c4c5bf5b4057731b83110bd1e13724","date":1276701452,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n\n      DocsEnum docs = null;\n      while(fields.next() != null) {\n        TermsEnum terms = fields.terms();\n        while(terms.next() != null) {\n          String text = terms.term().utf8ToString();\n          docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n\n          while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            int docId = docs.docID();\n            int freq = docs.freq();\n            //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n            TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n            //float tf = sim.tf(freq);\n            //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n            //float qNorm = sim.queryNorm()\n            //This is fine since we don't have stop words\n            //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n            //float coord = sim.coord()\n            //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n            assertTrue(vector != null);\n            String[] vTerms = vector.getTerms();\n            int [] freqs = vector.getTermFrequencies();\n            for (int i = 0; i < vTerms.length; i++)\n              {\n                if (text.equals(vTerms[i]))\n                  {\n                    assertTrue(freqs[i] == freq);\n                  }\n              }\n          }\n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4f29ba80b723649f5feb7e37afe1a558dd2c1304","date":1278318805,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n\n      DocsEnum docs = null;\n      while(fields.next() != null) {\n        TermsEnum terms = fields.terms();\n        while(terms.next() != null) {\n          String text = terms.term().utf8ToString();\n          docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n\n          while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            int docId = docs.docID();\n            int freq = docs.freq();\n            //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n            TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n            //float tf = sim.tf(freq);\n            //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n            //float qNorm = sim.queryNorm()\n            //This is fine since we don't have stop words\n            //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n            //float coord = sim.coord()\n            //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n            assertTrue(vector != null);\n            BytesRef[] vTerms = vector.getTerms();\n            int [] freqs = vector.getTermFrequencies();\n            for (int i = 0; i < vTerms.length; i++)\n              {\n                if (text.equals(vTerms[i].utf8ToString()))\n                  {\n                    assertTrue(freqs[i] == freq);\n                  }\n              }\n          }\n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      BytesRef[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i].utf8ToString();\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n\n      DocsEnum docs = null;\n      while(fields.next() != null) {\n        TermsEnum terms = fields.terms();\n        while(terms.next() != null) {\n          String text = terms.term().utf8ToString();\n          docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n\n          while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            int docId = docs.docID();\n            int freq = docs.freq();\n            //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n            TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n            //float tf = sim.tf(freq);\n            //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n            //float qNorm = sim.queryNorm()\n            //This is fine since we don't have stop words\n            //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n            //float coord = sim.coord()\n            //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n            assertTrue(vector != null);\n            String[] vTerms = vector.getTerms();\n            int [] freqs = vector.getTermFrequencies();\n            for (int i = 0; i < vTerms.length; i++)\n              {\n                if (text.equals(vTerms[i]))\n                  {\n                    assertTrue(freqs[i] == freq);\n                  }\n              }\n          }\n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c084e47df29de3330311d69dabf515ceaa989512","date":1279030906,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n\n      DocsEnum docs = null;\n      while(fields.next() != null) {\n        TermsEnum terms = fields.terms();\n        while(terms.next() != null) {\n          String text = terms.term().utf8ToString();\n          docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n\n          while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n            int docId = docs.docID();\n            int freq = docs.freq();\n            //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n            TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n            //float tf = sim.tf(freq);\n            //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n            //float qNorm = sim.queryNorm()\n            //This is fine since we don't have stop words\n            //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n            //float coord = sim.coord()\n            //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n            assertTrue(vector != null);\n            BytesRef[] vTerms = vector.getTerms();\n            int [] freqs = vector.getTermFrequencies();\n            for (int i = 0; i < vTerms.length; i++)\n              {\n                if (text.equals(vTerms[i].utf8ToString()))\n                  {\n                    assertTrue(freqs[i] == freq);\n                  }\n              }\n          }\n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      BytesRef[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i].utf8ToString();\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5f4e87790277826a2aea119328600dfb07761f32","date":1279827275,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n        \n    Directory dir = new MockRAMDirectory();\n    \n    try {\n      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig(\n          TEST_VERSION_CURRENT, \n          new MockAnalyzer(MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE));\n      writer.addDocument(testDoc1);\n      writer.addDocument(testDoc2);\n      writer.addDocument(testDoc3);\n      writer.addDocument(testDoc4);\n      writer.close();\n      IndexSearcher knownSearcher = new IndexSearcher(dir, true);\n      TermEnum termEnum = knownSearcher.reader.terms();\n      TermDocs termDocs = knownSearcher.reader.termDocs();\n      //System.out.println(\"Terms: \" + termEnum.size() + \" Orig Len: \" + termArray.length);\n      \n      //Similarity sim = knownSearcher.getSimilarity();\n      while (termEnum.next() == true)\n      {\n        Term term = termEnum.term();\n        //System.out.println(\"Term: \" + term);\n        termDocs.seek(term);\n        while (termDocs.next())\n        {\n          int docId = termDocs.doc();\n          int freq = termDocs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          String[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (term.text().equals(vTerms[i]))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n          \n        }\n        //System.out.println(\"--------\");\n      }\n      Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n      ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n      //doc 3 should be the first hit b/c it is the shortest match\n      assertTrue(hits.length == 3);\n      /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n      assertTrue(hits[0].doc == 2);\n      assertTrue(hits[1].doc == 3);\n      assertTrue(hits[2].doc == 0);\n      TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n      assertTrue(vector != null);\n      //System.out.println(\"Vector: \" + vector);\n      String[] terms = vector.getTerms();\n      int [] freqs = vector.getTermFrequencies();\n      assertTrue(terms != null && terms.length == 10);\n      for (int i = 0; i < terms.length; i++) {\n        String term = terms[i];\n        //System.out.println(\"Term: \" + term);\n        int freq = freqs[i];\n        assertTrue(test4.indexOf(term) != -1);\n        Integer freqInt = test4Map.get(term);\n        assertTrue(freqInt != null);\n        assertTrue(freqInt.intValue() == freq);        \n      }\n      SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n      SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n      assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      TermVectorEntry last = null;\n      for (final TermVectorEntry tve : vectorEntrySet) {\n        if (tve != null && last != null)\n        {\n          assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n          Integer expectedFreq =  test4Map.get(tve.getTerm());\n          //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n          assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n        }\n        last = tve;\n\n      }\n\n      FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n      knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n      Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n      assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n      vectorEntrySet = map.get(\"field\");\n      assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n      assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n      knownSearcher.close();\n    } catch (IOException e) {\n      e.printStackTrace();\n      assertTrue(false);\n    }\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"15bbd254c1506df5299c4df8c148262c7bd6301e","date":1279913113,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b103252dee6afa1b6d7a622c773d178788eb85a","date":1280180143,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3242a09f703274d3b9283f2064a1a33064b53a1b","date":1280263474,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c","date":1281646583,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory(random);\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1f653cfcf159baeaafe5d01682a911e95bba4012","date":1284122058,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory(random);\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","date":1292920096,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = new MockRAMDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(random, TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c19f985e36a65cc969e8e564fe337a0d41512075","date":1296330536,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"790e1fde4caa765b3faaad3fbcd25c6973450336","date":1296689245,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"29ef99d61cda9641b6250bf9567329a6e65f901d","date":1297244127,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bde51b089eb7f86171eb3406e38a274743f9b7ac","date":1298336439,"type":3,"author":"Michael Busch","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n        .setOpenMode(OpenMode.CREATE));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = new IndexSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"01e5948db9a07144112d2f08f28ca2e3cd880348","date":1301759232,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"45669a651c970812a680841b97a77cce06af559f","date":1301922222,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f2c5f0cb44df114db4228c8f77861714b5cabaea","date":1302542431,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"962d04139994fce5193143ef35615499a9a96d78","date":1302693744,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"135621f3a0670a9394eb563224a3b76cc4dddc0f","date":1304344257,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3776dccca01c11e7046323cfad46a3b4a471233","date":1306100719,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newInOrderLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","date":1309960478,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"817d8435e9135b756f08ce6710ab0baac51bdf88","date":1309986993,"type":3,"author":"Steven Rowe","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d083e83f225b11e5fdd900e83d26ddb385b6955c","date":1310029438,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getDeletedDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"60e3b31201fd9bb5d73884faa5a38c63ea9239f2","date":1315756041,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n                                                     .setOpenMode(OpenMode.CREATE).setMergePolicy(newLogMergePolicy()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3cc749c053615f5871f3b95715fe292f34e70a53","date":1321470575,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = termsEnum.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      TermsEnum terms = fields.terms();\n      while(terms.next() != null) {\n        String text = terms.term().utf8ToString();\n        docs = terms.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          TermFreqVector vector = knownSearcher.reader.getTermFreqVector(docId, \"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertTrue(vector != null);\n          BytesRef[] vTerms = vector.getTerms();\n          int [] freqs = vector.getTermFrequencies();\n          for (int i = 0; i < vTerms.length; i++)\n          {\n            if (text.equals(vTerms[i].utf8ToString()))\n            {\n              assertTrue(freqs[i] == freq);\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    TermFreqVector vector = knownSearcher.reader.getTermFreqVector(hits[1].doc, \"field\");\n    assertTrue(vector != null);\n    //System.out.println(\"Vector: \" + vector);\n    BytesRef[] terms = vector.getTerms();\n    int [] freqs = vector.getTermFrequencies();\n    assertTrue(terms != null && terms.length == 10);\n    for (int i = 0; i < terms.length; i++) {\n      String term = terms[i].utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = freqs[i];\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertTrue(freqInt.intValue() == freq);        \n    }\n    SortedTermVectorMapper mapper = new SortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, mapper);\n    SortedSet<TermVectorEntry> vectorEntrySet = mapper.getTermVectorEntrySet();\n    assertTrue(\"mapper.getTermVectorEntrySet() Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    TermVectorEntry last = null;\n    for (final TermVectorEntry tve : vectorEntrySet) {\n      if (tve != null && last != null)\n      {\n        assertTrue(\"terms are not properly sorted\", last.getFrequency() >= tve.getFrequency());\n        Integer expectedFreq =  test4Map.get(tve.getTerm().utf8ToString());\n        //we expect double the expectedFreq, since there are two fields with the exact same text and we are collapsing all fields\n        assertTrue(\"Frequency is not correct:\", tve.getFrequency() == 2*expectedFreq.intValue());\n      }\n      last = tve;\n      \n    }\n    \n    FieldSortedTermVectorMapper fieldMapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    knownSearcher.reader.getTermFreqVector(hits[1].doc, fieldMapper);\n    Map<String,SortedSet<TermVectorEntry>> map = fieldMapper.getFieldToTerms();\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 2, map.size() == 2);\n    vectorEntrySet = map.get(\"field\");\n    assertTrue(\"vectorEntrySet is null and it shouldn't be\", vectorEntrySet != null);\n    assertTrue(\"vectorEntrySet Size: \" + vectorEntrySet.size() + \" is not: \" + 10, vectorEntrySet.size() == 10);\n    knownSearcher.close();\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"872cff1d3a554e0cd64014cd97f88d3002b0f491","date":1323024658,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = termsEnum.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b65b350ca9588f9fc76ce7d6804160d06c45ff42","date":1323026297,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = termsEnum.docs(MultiFields.getLiveDocs(knownSearcher.reader), docs);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1ab42b0756bdd98ac6a6767b5a77d10d9ba12b4b","date":1328532481,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarityProvider(new DefaultSimilarityProvider()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarityProvider(new DefaultSimilarityProvider());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestTermVectors#testKnownSetOfDocuments().mjava","sourceNew":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","sourceOld":"  public void testKnownSetOfDocuments() throws IOException {\n    String test1 = \"eating chocolate in a computer lab\"; //6 terms\n    String test2 = \"computer in a computer lab\"; //5 terms\n    String test3 = \"a chocolate lab grows old\"; //5 terms\n    String test4 = \"eating chocolate with a chocolate lab in an old chocolate colored computer lab\"; //13 terms\n    Map<String,Integer> test4Map = new HashMap<String,Integer>();\n    test4Map.put(\"chocolate\", Integer.valueOf(3));\n    test4Map.put(\"lab\", Integer.valueOf(2));\n    test4Map.put(\"eating\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"with\", Integer.valueOf(1));\n    test4Map.put(\"a\", Integer.valueOf(1));\n    test4Map.put(\"colored\", Integer.valueOf(1));\n    test4Map.put(\"in\", Integer.valueOf(1));\n    test4Map.put(\"an\", Integer.valueOf(1));\n    test4Map.put(\"computer\", Integer.valueOf(1));\n    test4Map.put(\"old\", Integer.valueOf(1));\n    \n    Document testDoc1 = new Document();\n    setupDoc(testDoc1, test1);\n    Document testDoc2 = new Document();\n    setupDoc(testDoc2, test2);\n    Document testDoc3 = new Document();\n    setupDoc(testDoc3, test3);\n    Document testDoc4 = new Document();\n    setupDoc(testDoc4, test4);\n    \n    Directory dir = newDirectory();\n    \n    RandomIndexWriter writer = new RandomIndexWriter(random, dir, \n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.SIMPLE, true))\n          .setOpenMode(OpenMode.CREATE)\n          .setMergePolicy(newLogMergePolicy())\n          .setSimilarity(new DefaultSimilarity()));\n    writer.addDocument(testDoc1);\n    writer.addDocument(testDoc2);\n    writer.addDocument(testDoc3);\n    writer.addDocument(testDoc4);\n    IndexReader reader = writer.getReader();\n    writer.close();\n    IndexSearcher knownSearcher = newSearcher(reader);\n    knownSearcher.setSimilarity(new DefaultSimilarity());\n    FieldsEnum fields = MultiFields.getFields(knownSearcher.reader).iterator();\n    \n    DocsEnum docs = null;\n    while(fields.next() != null) {\n      Terms terms = fields.terms();\n      assertNotNull(terms);\n      TermsEnum termsEnum = terms.iterator(null);\n\n      while (termsEnum.next() != null) {\n        String text = termsEnum.term().utf8ToString();\n        docs = _TestUtil.docs(random, termsEnum, MultiFields.getLiveDocs(knownSearcher.reader), docs, true);\n        \n        while (docs.nextDoc() != DocsEnum.NO_MORE_DOCS) {\n          int docId = docs.docID();\n          int freq = docs.freq();\n          //System.out.println(\"Doc Id: \" + docId + \" freq \" + freq);\n          Terms vector = knownSearcher.reader.getTermVectors(docId).terms(\"field\");\n          //float tf = sim.tf(freq);\n          //float idf = sim.idf(knownSearcher.docFreq(term), knownSearcher.maxDoc());\n          //float qNorm = sim.queryNorm()\n          //This is fine since we don't have stop words\n          //float lNorm = sim.lengthNorm(\"field\", vector.getTerms().length);\n          //float coord = sim.coord()\n          //System.out.println(\"TF: \" + tf + \" IDF: \" + idf + \" LenNorm: \" + lNorm);\n          assertNotNull(vector);\n          TermsEnum termsEnum2 = vector.iterator(null);\n\n          while(termsEnum2.next() != null) {\n            if (text.equals(termsEnum2.term().utf8ToString())) {\n              assertEquals(freq, termsEnum2.totalTermFreq());\n            }\n          }\n        }\n      }\n      //System.out.println(\"--------\");\n    }\n    Query query = new TermQuery(new Term(\"field\", \"chocolate\"));\n    ScoreDoc[] hits = knownSearcher.search(query, null, 1000).scoreDocs;\n    //doc 3 should be the first hit b/c it is the shortest match\n    assertTrue(hits.length == 3);\n    /*System.out.println(\"Hit 0: \" + hits.id(0) + \" Score: \" + hits.score(0) + \" String: \" + hits.doc(0).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(0)));\n      System.out.println(\"Hit 1: \" + hits.id(1) + \" Score: \" + hits.score(1) + \" String: \" + hits.doc(1).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(1)));\n      System.out.println(\"Hit 2: \" + hits.id(2) + \" Score: \" + hits.score(2) + \" String: \" +  hits.doc(2).toString());\n      System.out.println(\"Explain: \" + knownSearcher.explain(query, hits.id(2)));*/\n    assertTrue(hits[0].doc == 2);\n    assertTrue(hits[1].doc == 3);\n    assertTrue(hits[2].doc == 0);\n    Terms vector = knownSearcher.reader.getTermVectors(hits[1].doc).terms(\"field\");\n    assertNotNull(vector);\n    //System.out.println(\"Vector: \" + vector);\n    assertEquals(10, vector.getUniqueTermCount());\n    TermsEnum termsEnum = vector.iterator(null);\n    while(termsEnum.next() != null) {\n      String term = termsEnum.term().utf8ToString();\n      //System.out.println(\"Term: \" + term);\n      int freq = (int) termsEnum.totalTermFreq();\n      assertTrue(test4.indexOf(term) != -1);\n      Integer freqInt = test4Map.get(term);\n      assertTrue(freqInt != null);\n      assertEquals(freqInt.intValue(), freq);\n    }\n    reader.close();\n    dir.close();\n  } \n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["1ab42b0756bdd98ac6a6767b5a77d10d9ba12b4b"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["f2c5f0cb44df114db4228c8f77861714b5cabaea"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["c084e47df29de3330311d69dabf515ceaa989512"],"60e3b31201fd9bb5d73884faa5a38c63ea9239f2":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"1ab42b0756bdd98ac6a6767b5a77d10d9ba12b4b":["872cff1d3a554e0cd64014cd97f88d3002b0f491"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["3242a09f703274d3b9283f2064a1a33064b53a1b","1f653cfcf159baeaafe5d01682a911e95bba4012"],"c19f985e36a65cc969e8e564fe337a0d41512075":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"c084e47df29de3330311d69dabf515ceaa989512":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["01e5948db9a07144112d2f08f28ca2e3cd880348"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["1f653cfcf159baeaafe5d01682a911e95bba4012","790e1fde4caa765b3faaad3fbcd25c6973450336"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"28427ef110c4c5bf5b4057731b83110bd1e13724":["2ade882efb2f2235dafb176284c1e35dbdb1c126"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","790e1fde4caa765b3faaad3fbcd25c6973450336"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["28427ef110c4c5bf5b4057731b83110bd1e13724"],"7edb20114e86ec883b0b08bd624eee852c565c06":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["5f4e87790277826a2aea119328600dfb07761f32","4b103252dee6afa1b6d7a622c773d178788eb85a"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["c084e47df29de3330311d69dabf515ceaa989512","15bbd254c1506df5299c4df8c148262c7bd6301e"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":["29ef99d61cda9641b6250bf9567329a6e65f901d","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"3cc749c053615f5871f3b95715fe292f34e70a53":["60e3b31201fd9bb5d73884faa5a38c63ea9239f2"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["3cc749c053615f5871f3b95715fe292f34e70a53"],"5f4e87790277826a2aea119328600dfb07761f32":["2ade882efb2f2235dafb176284c1e35dbdb1c126","c084e47df29de3330311d69dabf515ceaa989512"],"962d04139994fce5193143ef35615499a9a96d78":["45669a651c970812a680841b97a77cce06af559f","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"d083e83f225b11e5fdd900e83d26ddb385b6955c":["f2c5f0cb44df114db4228c8f77861714b5cabaea","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"817d8435e9135b756f08ce6710ab0baac51bdf88":["a3776dccca01c11e7046323cfad46a3b4a471233","e7bd246bb7bc35ac22edfee9157e034dfc4e65eb"],"790e1fde4caa765b3faaad3fbcd25c6973450336":["c19f985e36a65cc969e8e564fe337a0d41512075"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":["3cc749c053615f5871f3b95715fe292f34e70a53","872cff1d3a554e0cd64014cd97f88d3002b0f491"],"a3776dccca01c11e7046323cfad46a3b4a471233":["790e1fde4caa765b3faaad3fbcd25c6973450336","f2c5f0cb44df114db4228c8f77861714b5cabaea"],"2ade882efb2f2235dafb176284c1e35dbdb1c126":["7edb20114e86ec883b0b08bd624eee852c565c06"],"45669a651c970812a680841b97a77cce06af559f":["bde51b089eb7f86171eb3406e38a274743f9b7ac","01e5948db9a07144112d2f08f28ca2e3cd880348"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"]},"commit2Childs":{"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"e7bd246bb7bc35ac22edfee9157e034dfc4e65eb":["60e3b31201fd9bb5d73884faa5a38c63ea9239f2","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88"],"15bbd254c1506df5299c4df8c148262c7bd6301e":["4b103252dee6afa1b6d7a622c773d178788eb85a"],"60e3b31201fd9bb5d73884faa5a38c63ea9239f2":["3cc749c053615f5871f3b95715fe292f34e70a53"],"1ab42b0756bdd98ac6a6767b5a77d10d9ba12b4b":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a":["bde51b089eb7f86171eb3406e38a274743f9b7ac"],"c19f985e36a65cc969e8e564fe337a0d41512075":["790e1fde4caa765b3faaad3fbcd25c6973450336"],"01e5948db9a07144112d2f08f28ca2e3cd880348":["f2c5f0cb44df114db4228c8f77861714b5cabaea","45669a651c970812a680841b97a77cce06af559f"],"c084e47df29de3330311d69dabf515ceaa989512":["15bbd254c1506df5299c4df8c148262c7bd6301e","4b103252dee6afa1b6d7a622c773d178788eb85a","5f4e87790277826a2aea119328600dfb07761f32"],"f2c5f0cb44df114db4228c8f77861714b5cabaea":["e7bd246bb7bc35ac22edfee9157e034dfc4e65eb","135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","a3776dccca01c11e7046323cfad46a3b4a471233"],"1f653cfcf159baeaafe5d01682a911e95bba4012":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a","c19f985e36a65cc969e8e564fe337a0d41512075","29ef99d61cda9641b6250bf9567329a6e65f901d"],"29ef99d61cda9641b6250bf9567329a6e65f901d":["135621f3a0670a9394eb563224a3b76cc4dddc0f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"28427ef110c4c5bf5b4057731b83110bd1e13724":["4f29ba80b723649f5feb7e37afe1a558dd2c1304"],"bde51b089eb7f86171eb3406e38a274743f9b7ac":["45669a651c970812a680841b97a77cce06af559f"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["7edb20114e86ec883b0b08bd624eee852c565c06"],"3242a09f703274d3b9283f2064a1a33064b53a1b":["7c65bc241a96282ca59ae736b4ffb5b7e5eeb23a"],"4f29ba80b723649f5feb7e37afe1a558dd2c1304":["c084e47df29de3330311d69dabf515ceaa989512"],"7edb20114e86ec883b0b08bd624eee852c565c06":["2ade882efb2f2235dafb176284c1e35dbdb1c126"],"4b103252dee6afa1b6d7a622c773d178788eb85a":["3242a09f703274d3b9283f2064a1a33064b53a1b","ab9633cb67e3c0aec3c066147a23a957d6e7ad8c"],"ab9633cb67e3c0aec3c066147a23a957d6e7ad8c":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"135621f3a0670a9394eb563224a3b76cc4dddc0f":[],"3cc749c053615f5871f3b95715fe292f34e70a53":["872cff1d3a554e0cd64014cd97f88d3002b0f491","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"872cff1d3a554e0cd64014cd97f88d3002b0f491":["1ab42b0756bdd98ac6a6767b5a77d10d9ba12b4b","b65b350ca9588f9fc76ce7d6804160d06c45ff42"],"5f4e87790277826a2aea119328600dfb07761f32":["3242a09f703274d3b9283f2064a1a33064b53a1b"],"962d04139994fce5193143ef35615499a9a96d78":[],"d083e83f225b11e5fdd900e83d26ddb385b6955c":[],"817d8435e9135b756f08ce6710ab0baac51bdf88":[],"790e1fde4caa765b3faaad3fbcd25c6973450336":["01e5948db9a07144112d2f08f28ca2e3cd880348","29ef99d61cda9641b6250bf9567329a6e65f901d","bde51b089eb7f86171eb3406e38a274743f9b7ac","a3776dccca01c11e7046323cfad46a3b4a471233"],"b65b350ca9588f9fc76ce7d6804160d06c45ff42":[],"a3776dccca01c11e7046323cfad46a3b4a471233":["817d8435e9135b756f08ce6710ab0baac51bdf88"],"2ade882efb2f2235dafb176284c1e35dbdb1c126":["28427ef110c4c5bf5b4057731b83110bd1e13724","5f4e87790277826a2aea119328600dfb07761f32"],"45669a651c970812a680841b97a77cce06af559f":["962d04139994fce5193143ef35615499a9a96d78"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["135621f3a0670a9394eb563224a3b76cc4dddc0f","962d04139994fce5193143ef35615499a9a96d78","d083e83f225b11e5fdd900e83d26ddb385b6955c","817d8435e9135b756f08ce6710ab0baac51bdf88","b65b350ca9588f9fc76ce7d6804160d06c45ff42","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}