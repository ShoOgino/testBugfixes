{"path":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        ticketQueue.tryPurge(this);\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        ticketQueue.tryPurge(this);\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d9773480aa9e800d0a232ab6ccac265e874b0c51","date":1349461188,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.  But really\n          // this means we have a concurrency issue\n          // (TestBagOfPostings can provoke this):\n          // publishing a flush segment is too heavy today\n          // (it builds CFS, writes .si, etc.) ... we need\n          // to make those ops concurrent too:\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        ticketQueue.tryPurge(this);\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7","date":1349855720,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.  But really\n          // this means we have a concurrency issue\n          // (TestBagOfPostings can provoke this):\n          // publishing a flush segment is too heavy today\n          // (it builds CFS, writes .si, etc.) ... we need\n          // to make those ops concurrent too:\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"088a7ef694fd43d5d9a4d200c4005865f773d1e7","date":1371136274,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriterConfig.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriter.getConfig().getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7af110b00ea8df9429309d83e38e0533d82e144f","date":1376924768,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriterConfig.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":["022a16646a72265e17bbad4aef83cd54efd15804"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31d4861802ca404d78ca1d15f4550eec415b9199","date":1376947894,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriterConfig.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3dffec77fb8f7d0e9ca4869dddd6af94528b4576","date":1377875202,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean maybeMerge = false;\n    while (flushingDWPT != null) {\n      maybeMerge = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          // flush concurrently without locking\n          final FlushedSegment newSegment = flushingDWPT.flush();\n          ticketQueue.addSegment(ticket, newSegment);\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          ticketQueue.forcePurge(this);\n        } else {\n          ticketQueue.tryPurge(this);\n        }\n\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n        indexWriter.flushCount.incrementAndGet();\n        indexWriter.doAfterFlush();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = indexWriterConfig.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      applyAllDeletes(deleteQueue);\n    }\n\n    return maybeMerge;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9299079153fd7895bf3cf6835cf7019af2ba89b3","date":1417813477,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      Throwable exc = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private  boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n        flushingDWPT.checkAndResetHasAborted();\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      Throwable exc = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      Throwable exc = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      if (infoStream.isEnabled(\"DW\")) {\n        infoStream.message(\"DW\", \"force apply deletes bytesUsed=\" + flushControl.getDeleteBytesUsed() + \" vs ramBuffer=\" + (1024*1024*ramBufferSizeMB));\n      }\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c02b804ab16489b95429791a2d8fb0e0728354d4","date":1436551798,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      Throwable exc = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadState()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n    if (hasEvents) {\n      putEvent(MergePendingEvent.INSTANCE);\n    }\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (!this.applyAllDeletes(deleteQueue)) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"022a16646a72265e17bbad4aef83cd54efd15804","date":1499023964,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":["7af110b00ea8df9429309d83e38e0533d82e144f"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","date":1499066739,"type":3,"author":"Adrien Grand","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"30c8e5574b55d57947e989443dfde611646530ee","date":1499131153,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (!flushingDWPT.pendingFilesToDelete().isEmpty()) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (!dwptSuccess) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef","date":1512420564,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n  \n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":["761f91143f5b5b8e3016b2b178ee0f2f61a9b4e4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException, AbortingException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush();\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              putEvent(new DeleteNewFilesEvent(flushingDWPT.pendingFilesToDelete()));\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              putEvent(new FlushFailedEvent(flushingDWPT.getSegmentInfo()));\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          putEvent(ForcedPurgeEvent.INSTANCE);\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      writer.doAfterSegmentFlushed(false, false);\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        putEvent(ApplyDeletesEvent.INSTANCE);\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b8498afacfc8322268ca0d659d274fcce08d557","date":1524577248,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      SegmentFlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9664831d785795f0f380fccc6db560efb979fdbb","date":1526559002,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"34067df01cbbefc83d0b316037fe4e10d89ba6a3","date":1559054674,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes() == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9","date":1559371943,"type":3,"author":"Simon Willnauer","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes() == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes(deleteQueue) == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"264935965977b4a9e2f3920420647072c9c49176","date":1586600626,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/DocumentsWriter#doFlush(DocumentsWriterPerThread).mjava","sourceNew":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      assert flushingDWPT.hasFlushed() == false;\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.size()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes() == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","sourceOld":"  private boolean doFlush(DocumentsWriterPerThread flushingDWPT) throws IOException {\n    boolean hasEvents = false;\n    while (flushingDWPT != null) {\n      hasEvents = true;\n      boolean success = false;\n      DocumentsWriterFlushQueue.FlushTicket ticket = null;\n      try {\n        assert currentFullFlushDelQueue == null\n            || flushingDWPT.deleteQueue == currentFullFlushDelQueue : \"expected: \"\n            + currentFullFlushDelQueue + \"but was: \" + flushingDWPT.deleteQueue\n            + \" \" + flushControl.isFullFlush();\n        /*\n         * Since with DWPT the flush process is concurrent and several DWPT\n         * could flush at the same time we must maintain the order of the\n         * flushes before we can apply the flushed segment and the frozen global\n         * deletes it is buffering. The reason for this is that the global\n         * deletes mark a certain point in time where we took a DWPT out of\n         * rotation and freeze the global deletes.\n         * \n         * Example: A flush 'A' starts and freezes the global deletes, then\n         * flush 'B' starts and freezes all deletes occurred since 'A' has\n         * started. if 'B' finishes before 'A' we need to wait until 'A' is done\n         * otherwise the deletes frozen by 'B' are not applied to 'A' and we\n         * might miss to deletes documents in 'A'.\n         */\n        try {\n          assert assertTicketQueueModification(flushingDWPT.deleteQueue);\n          // Each flush is assigned a ticket in the order they acquire the ticketQueue lock\n          ticket = ticketQueue.addFlushTicket(flushingDWPT);\n          final int flushingDocsInRam = flushingDWPT.getNumDocsInRAM();\n          boolean dwptSuccess = false;\n          try {\n            // flush concurrently without locking\n            final FlushedSegment newSegment = flushingDWPT.flush(flushNotifications);\n            ticketQueue.addSegment(ticket, newSegment);\n            dwptSuccess = true;\n          } finally {\n            subtractFlushedNumDocs(flushingDocsInRam);\n            if (flushingDWPT.pendingFilesToDelete().isEmpty() == false) {\n              Set<String> files = flushingDWPT.pendingFilesToDelete();\n              flushNotifications.deleteUnusedFiles(files);\n              hasEvents = true;\n            }\n            if (dwptSuccess == false) {\n              flushNotifications.flushFailed(flushingDWPT.getSegmentInfo());\n              hasEvents = true;\n            }\n          }\n          // flush was successful once we reached this point - new seg. has been assigned to the ticket!\n          success = true;\n        } finally {\n          if (!success && ticket != null) {\n            // In the case of a failure make sure we are making progress and\n            // apply all the deletes since the segment flush failed since the flush\n            // ticket could hold global deletes see FlushTicket#canPublish()\n            ticketQueue.markTicketFailed(ticket);\n          }\n        }\n        /*\n         * Now we are done and try to flush the ticket queue if the head of the\n         * queue has already finished the flush.\n         */\n        if (ticketQueue.getTicketCount() >= perThreadPool.getActiveThreadStateCount()) {\n          // This means there is a backlog: the one\n          // thread in innerPurge can't keep up with all\n          // other threads flushing segments.  In this case\n          // we forcefully stall the producers.\n          flushNotifications.onTicketBacklog();\n          break;\n        }\n      } finally {\n        flushControl.doAfterFlush(flushingDWPT);\n      }\n     \n      flushingDWPT = flushControl.nextPendingFlush();\n    }\n\n    if (hasEvents) {\n      flushNotifications.afterSegmentsFlushed();\n    }\n\n    // If deletes alone are consuming > 1/2 our RAM\n    // buffer, force them all to apply now. This is to\n    // prevent too-frequent flushing of a long tail of\n    // tiny segments:\n    final double ramBufferSizeMB = config.getRAMBufferSizeMB();\n    if (ramBufferSizeMB != IndexWriterConfig.DISABLE_AUTO_FLUSH &&\n        flushControl.getDeleteBytesUsed() > (1024*1024*ramBufferSizeMB/2)) {\n      hasEvents = true;\n      if (applyAllDeletes() == false) {\n        if (infoStream.isEnabled(\"DW\")) {\n          infoStream.message(\"DW\", String.format(Locale.ROOT, \"force apply deletes after flush bytesUsed=%.1f MB vs ramBuffer=%.1f MB\",\n                                                 flushControl.getDeleteBytesUsed()/(1024.*1024.),\n                                                 ramBufferSizeMB));\n        }\n        flushNotifications.onDeletesApplied();\n      }\n    }\n\n    return hasEvents;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"7af110b00ea8df9429309d83e38e0533d82e144f":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["c02b804ab16489b95429791a2d8fb0e0728354d4"],"6b8498afacfc8322268ca0d659d274fcce08d557":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"9664831d785795f0f380fccc6db560efb979fdbb":["6b8498afacfc8322268ca0d659d274fcce08d557"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"34067df01cbbefc83d0b316037fe4e10d89ba6a3":["9664831d785795f0f380fccc6db560efb979fdbb"],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":["28288370235ed02234a64753cdbf0c6ec096304a","022a16646a72265e17bbad4aef83cd54efd15804"],"264935965977b4a9e2f3920420647072c9c49176":["34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9"],"d9773480aa9e800d0a232ab6ccac265e874b0c51":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":["088a7ef694fd43d5d9a4d200c4005865f773d1e7","7af110b00ea8df9429309d83e38e0533d82e144f"],"31d4861802ca404d78ca1d15f4550eec415b9199":["088a7ef694fd43d5d9a4d200c4005865f773d1e7","7af110b00ea8df9429309d83e38e0533d82e144f"],"34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9":["9664831d785795f0f380fccc6db560efb979fdbb","34067df01cbbefc83d0b316037fe4e10d89ba6a3"],"022a16646a72265e17bbad4aef83cd54efd15804":["28288370235ed02234a64753cdbf0c6ec096304a"],"30c8e5574b55d57947e989443dfde611646530ee":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","022a16646a72265e17bbad4aef83cd54efd15804"],"c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7":["d9773480aa9e800d0a232ab6ccac265e874b0c51"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["c02b804ab16489b95429791a2d8fb0e0728354d4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"28288370235ed02234a64753cdbf0c6ec096304a":["c02b804ab16489b95429791a2d8fb0e0728354d4","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"c02b804ab16489b95429791a2d8fb0e0728354d4":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["7af110b00ea8df9429309d83e38e0533d82e144f"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["9299079153fd7895bf3cf6835cf7019af2ba89b3"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["022a16646a72265e17bbad4aef83cd54efd15804"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["264935965977b4a9e2f3920420647072c9c49176"]},"commit2Childs":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"7af110b00ea8df9429309d83e38e0533d82e144f":["3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","9299079153fd7895bf3cf6835cf7019af2ba89b3"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["6b8498afacfc8322268ca0d659d274fcce08d557"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["d9773480aa9e800d0a232ab6ccac265e874b0c51"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"6b8498afacfc8322268ca0d659d274fcce08d557":["9664831d785795f0f380fccc6db560efb979fdbb"],"9664831d785795f0f380fccc6db560efb979fdbb":["34067df01cbbefc83d0b316037fe4e10d89ba6a3","34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"34067df01cbbefc83d0b316037fe4e10d89ba6a3":["34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9"],"6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35":[],"264935965977b4a9e2f3920420647072c9c49176":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d9773480aa9e800d0a232ab6ccac265e874b0c51":["c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7"],"3dffec77fb8f7d0e9ca4869dddd6af94528b4576":[],"31d4861802ca404d78ca1d15f4550eec415b9199":[],"34b5cf63f4ff3d985a7fc828c1a0d1624b0176d9":["264935965977b4a9e2f3920420647072c9c49176"],"022a16646a72265e17bbad4aef83cd54efd15804":["6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","30c8e5574b55d57947e989443dfde611646530ee","fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef"],"30c8e5574b55d57947e989443dfde611646530ee":[],"c6bb01d819ee2a06924d25bb5683fe4dcf8cf1a7":["088a7ef694fd43d5d9a4d200c4005865f773d1e7"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["30c8e5574b55d57947e989443dfde611646530ee"],"28288370235ed02234a64753cdbf0c6ec096304a":["6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","022a16646a72265e17bbad4aef83cd54efd15804"],"c02b804ab16489b95429791a2d8fb0e0728354d4":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"088a7ef694fd43d5d9a4d200c4005865f773d1e7":["7af110b00ea8df9429309d83e38e0533d82e144f","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199"],"9299079153fd7895bf3cf6835cf7019af2ba89b3":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["c02b804ab16489b95429791a2d8fb0e0728354d4"],"fdc3f2b9a4e1c1aacfa53b304c4e42c13a1677ef":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["6324f236dd5a5430f2ff91ebd6cb3f0ae8d34a35","3dffec77fb8f7d0e9ca4869dddd6af94528b4576","31d4861802ca404d78ca1d15f4550eec415b9199","30c8e5574b55d57947e989443dfde611646530ee","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}