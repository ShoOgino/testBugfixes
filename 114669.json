{"path":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","commits":[{"id":"d17d4fe0503a62f6522b1dd15204dd25cd231edf","date":1313599393,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"69e043c521d4e8db770cc140c63f5ef51f03426a","date":1317187614,"type":3,"author":"Christopher John Male","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.reusableTokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase#assertThreadSafe(Analyzer).mjava","sourceNew":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","sourceOld":"  public void assertThreadSafe(final Analyzer analyzer) throws Exception {\n    int numTestPoints = 100;\n    int numThreads = _TestUtil.nextInt(random, 3, 5);\n    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();\n    \n    // create a map<String,SortKey> up front.\n    // then with multiple threads, generate sort keys for all the keys in the map\n    // and ensure they are the same as the ones we produced in serial fashion.\n\n    for (int i = 0; i < numTestPoints; i++) {\n      String term = _TestUtil.randomSimpleString(random);\n      TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n      ts.reset();\n      assertTrue(ts.incrementToken());\n      termAtt.fillBytesRef();\n      // ensure we make a copy of the actual bytes too\n      map.put(term, new BytesRef(bytes));\n    }\n    \n    Thread threads[] = new Thread[numThreads];\n    for (int i = 0; i < numThreads; i++) {\n      threads[i] = new Thread() {\n        @Override\n        public void run() {\n          try {\n            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {\n              String term = mapping.getKey();\n              BytesRef expected = mapping.getValue();\n              TokenStream ts = analyzer.tokenStream(\"fake\", new StringReader(term));\n              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);\n              BytesRef bytes = termAtt.getBytesRef();\n              ts.reset();\n              assertTrue(ts.incrementToken());\n              termAtt.fillBytesRef();\n              assertEquals(expected, bytes);\n            }\n          } catch (IOException e) {\n            throw new RuntimeException(e);\n          }\n        }\n      };\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].start();\n    }\n    for (int i = 0; i < numThreads; i++) {\n      threads[i].join();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"69e043c521d4e8db770cc140c63f5ef51f03426a":["d17d4fe0503a62f6522b1dd15204dd25cd231edf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7b91922b55d15444d554721b352861d028eb8278"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"d17d4fe0503a62f6522b1dd15204dd25cd231edf":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["d17d4fe0503a62f6522b1dd15204dd25cd231edf"],"69e043c521d4e8db770cc140c63f5ef51f03426a":["7b91922b55d15444d554721b352861d028eb8278"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}