{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","commits":[{"id":"7d6adf8ea59977891966389011f3905e09932183","date":1332622471,"type":0,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","pathOld":"/dev/null","sourceNew":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n\n      // Final pass: verify clean tokenization matches\n      // results from first pass:\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n      }\n      reader = new StringReader(text);\n\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n        // offset + pos + posLength + type\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  types.toArray(new String[types.size()]),\n                                  toIntArray(positions),\n                                  toIntArray(positionLengths),\n                                  text.length());\n      } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n        // offset + pos + type\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  types.toArray(new String[types.size()]),\n                                  toIntArray(positions),\n                                  null,\n                                  text.length());\n      } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n        // offset + pos + posLength\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  toIntArray(positions),\n                                  toIntArray(positionLengths),\n                                  text.length());\n      } else if (posIncAtt != null && offsetAtt != null) {\n        // offset + pos\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  toIntArray(positions),\n                                  null,\n                                  text.length());\n      } else if (offsetAtt != null) {\n        // offset\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  null,\n                                  null,\n                                  text.length());\n      } else {\n        // terms only\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]));\n      }\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","1fe9452de26a70442324c5bdc5a5a333e55f07db"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1fe9452de26a70442324c5bdc5a5a333e55f07db","date":1333912637,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","sourceNew":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length());\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","sourceOld":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n\n      // Final pass: verify clean tokenization matches\n      // results from first pass:\n\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n      }\n      reader = new StringReader(text);\n\n      if (random.nextInt(30) == 7) {\n        if (VERBOSE) {\n          System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n        }\n\n        reader = new MockReaderWrapper(random, reader);\n      }\n\n      ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n      if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n        // offset + pos + posLength + type\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  types.toArray(new String[types.size()]),\n                                  toIntArray(positions),\n                                  toIntArray(positionLengths),\n                                  text.length());\n      } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n        // offset + pos + type\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  types.toArray(new String[types.size()]),\n                                  toIntArray(positions),\n                                  null,\n                                  text.length());\n      } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n        // offset + pos + posLength\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  toIntArray(positions),\n                                  toIntArray(positionLengths),\n                                  text.length());\n      } else if (posIncAtt != null && offsetAtt != null) {\n        // offset + pos\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  toIntArray(positions),\n                                  null,\n                                  text.length());\n      } else if (offsetAtt != null) {\n        // offset\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]),\n                                  toIntArray(startOffsets),\n                                  toIntArray(endOffsets),\n                                  null,\n                                  null,\n                                  null,\n                                  text.length());\n      } else {\n        // terms only\n        assertTokenStreamContents(ts, \n                                  tokens.toArray(new String[tokens.size()]));\n      }\n    }\n  }\n\n","bugFix":["7d6adf8ea59977891966389011f3905e09932183"],"bugIntro":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","sourceNew":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n    checkAnalysisConsistency(random, a, useCharFilter, text, true);\n  }\n\n","sourceOld":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length());\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#checkAnalysisConsistency(Random,Analyzer,boolean,String).mjava","sourceNew":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n    checkAnalysisConsistency(random, a, useCharFilter, text, true);\n  }\n\n","sourceOld":"  public static void checkAnalysisConsistency(Random random, Analyzer a, boolean useCharFilter, String text) throws IOException {\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: get first token stream now text=\" + text);\n    }\n\n    int remainder = random.nextInt(10);\n    Reader reader = new StringReader(text);\n    TokenStream ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    assertTrue(\"has no CharTermAttribute\", ts.hasAttribute(CharTermAttribute.class));\n    CharTermAttribute termAtt = ts.getAttribute(CharTermAttribute.class);\n    OffsetAttribute offsetAtt = ts.hasAttribute(OffsetAttribute.class) ? ts.getAttribute(OffsetAttribute.class) : null;\n    PositionIncrementAttribute posIncAtt = ts.hasAttribute(PositionIncrementAttribute.class) ? ts.getAttribute(PositionIncrementAttribute.class) : null;\n    PositionLengthAttribute posLengthAtt = ts.hasAttribute(PositionLengthAttribute.class) ? ts.getAttribute(PositionLengthAttribute.class) : null;\n    TypeAttribute typeAtt = ts.hasAttribute(TypeAttribute.class) ? ts.getAttribute(TypeAttribute.class) : null;\n    List<String> tokens = new ArrayList<String>();\n    List<String> types = new ArrayList<String>();\n    List<Integer> positions = new ArrayList<Integer>();\n    List<Integer> positionLengths = new ArrayList<Integer>();\n    List<Integer> startOffsets = new ArrayList<Integer>();\n    List<Integer> endOffsets = new ArrayList<Integer>();\n    ts.reset();\n\n    // First pass: save away \"correct\" tokens\n    while (ts.incrementToken()) {\n      tokens.add(termAtt.toString());\n      if (typeAtt != null) types.add(typeAtt.type());\n      if (posIncAtt != null) positions.add(posIncAtt.getPositionIncrement());\n      if (posLengthAtt != null) positionLengths.add(posLengthAtt.getPositionLength());\n      if (offsetAtt != null) {\n        startOffsets.add(offsetAtt.startOffset());\n        endOffsets.add(offsetAtt.endOffset());\n      }\n    }\n    ts.end();\n    ts.close();\n\n    // verify reusing is \"reproducable\" and also get the normal tokenstream sanity checks\n    if (!tokens.isEmpty()) {\n\n      // KWTokenizer (for example) can produce a token\n      // even when input is length 0:\n      if (text.length() != 0) {\n\n        // (Optional) second pass: do something evil:\n        final int evilness = random.nextInt(50);\n        if (evilness == 17) {\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis w/ exception\");\n          }\n          // Throw an errant exception from the Reader:\n\n          MockReaderWrapper evilReader = new MockReaderWrapper(random, new StringReader(text));\n          evilReader.throwExcAfterChar(random.nextInt(text.length()+1));\n          reader = evilReader;\n\n          try {\n            // NOTE: some Tokenizers go and read characters\n            // when you call .setReader(Reader), eg\n            // PatternTokenizer.  This is a bit\n            // iffy... (really, they should only\n            // pull from the Reader when you call\n            // .incremenToken(), I think?), but we\n            // currently allow it, so, we must call\n            // a.tokenStream inside the try since we may\n            // hit the exc on init:\n            ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(evilReader, remainder) : evilReader);\n            ts.reset();\n            while (ts.incrementToken());\n            fail(\"did not hit exception\");\n          } catch (RuntimeException re) {\n            assertTrue(MockReaderWrapper.isMyEvilException(re));\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        } else if (evilness == 7) {\n          // Only consume a subset of the tokens:\n          final int numTokensToRead = random.nextInt(tokens.size());\n          if (VERBOSE) {\n            System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis, only consuming \" + numTokensToRead + \" of \" + tokens.size() + \" tokens\");\n          }\n\n          reader = new StringReader(text);\n          ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n          ts.reset();\n          for(int tokenCount=0;tokenCount<numTokensToRead;tokenCount++) {\n            assertTrue(ts.incrementToken());\n          }\n          try {\n            ts.end();\n          } catch (AssertionError ae) {\n            // Catch & ignore MockTokenizer's\n            // anger...\n            if (\"end() called before incrementToken() returned false!\".equals(ae.getMessage())) {\n              // OK\n            } else {\n              throw ae;\n            }\n          }\n          ts.close();\n        }\n      }\n    }\n\n    // Final pass: verify clean tokenization matches\n    // results from first pass:\n\n    if (VERBOSE) {\n      System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: re-run analysis; \" + tokens.size() + \" tokens\");\n    }\n    reader = new StringReader(text);\n\n    if (random.nextInt(30) == 7) {\n      if (VERBOSE) {\n        System.out.println(Thread.currentThread().getName() + \": NOTE: BaseTokenStreamTestCase: using spoon-feed reader\");\n      }\n\n      reader = new MockReaderWrapper(random, reader);\n    }\n\n    ts = a.tokenStream(\"dummy\", useCharFilter ? new MockCharFilter(reader, remainder) : reader);\n    if (typeAtt != null && posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (typeAtt != null && posIncAtt != null && offsetAtt != null) {\n      // offset + pos + type\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                types.toArray(new String[types.size()]),\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (posIncAtt != null && posLengthAtt != null && offsetAtt != null) {\n      // offset + pos + posLength\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                toIntArray(positionLengths),\n                                text.length());\n    } else if (posIncAtt != null && offsetAtt != null) {\n      // offset + pos\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                toIntArray(positions),\n                                null,\n                                text.length());\n    } else if (offsetAtt != null) {\n      // offset\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]),\n                                toIntArray(startOffsets),\n                                toIntArray(endOffsets),\n                                null,\n                                null,\n                                null,\n                                text.length());\n    } else {\n      // terms only\n      assertTokenStreamContents(ts, \n                                tokens.toArray(new String[tokens.size()]));\n    }\n  }\n\n","bugFix":["7d6adf8ea59977891966389011f3905e09932183","1fe9452de26a70442324c5bdc5a5a333e55f07db"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["1fe9452de26a70442324c5bdc5a5a333e55f07db"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["1fe9452de26a70442324c5bdc5a5a333e55f07db","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"7d6adf8ea59977891966389011f3905e09932183":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1fe9452de26a70442324c5bdc5a5a333e55f07db":["7d6adf8ea59977891966389011f3905e09932183"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"]},"commit2Childs":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["7d6adf8ea59977891966389011f3905e09932183"],"7d6adf8ea59977891966389011f3905e09932183":["1fe9452de26a70442324c5bdc5a5a333e55f07db"],"1fe9452de26a70442324c5bdc5a5a333e55f07db":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}