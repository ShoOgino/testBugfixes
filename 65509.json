{"path":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","commits":[{"id":"91109046a59c58ee0ee5d0d2767b08d1f30d6702","date":1000830588,"type":0,"author":"Jason van Zyl","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"/dev/null","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n    \n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"115938faea54dfe03fc2cddc7fe92bafb1be6c68","date":1032401502,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"    final Document doc(int n)\n        throws IOException\n    {\n        // TODO: document the magic number 8L\n        indexStream.seek(n * 8L);\n        long position = indexStream.readLong();\n        fieldsStream.seek(position);\n\n        Document doc = new Document();\n        int numFields = fieldsStream.readVInt();\n        for (int i = 0; i < numFields; i++)\n        {\n            int fieldNumber = fieldsStream.readVInt();\n            FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n            byte bits = fieldsStream.readByte();\n\n            doc.add(new Field(fi.name, \t\t   // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t   // stored\n\t\t\tfi.isIndexed,\t\t   // indexed\n\t\t\t(bits & 1) != 0));\t   // tokenized\n        }\n\n        return doc;\n    }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n    \n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3f039c46e21390e3fc1f89f327ed17c2efd47212","date":1032488491,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n    \n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","sourceOld":"    final Document doc(int n)\n        throws IOException\n    {\n        // TODO: document the magic number 8L\n        indexStream.seek(n * 8L);\n        long position = indexStream.readLong();\n        fieldsStream.seek(position);\n\n        Document doc = new Document();\n        int numFields = fieldsStream.readVInt();\n        for (int i = 0; i < numFields; i++)\n        {\n            int fieldNumber = fieldsStream.readVInt();\n            FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n            byte bits = fieldsStream.readByte();\n\n            doc.add(new Field(fi.name, \t\t   // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t   // stored\n\t\t\tfi.isIndexed,\t\t   // indexed\n\t\t\t(bits & 1) != 0));\t   // tokenized\n        }\n\n        return doc;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1a9cbbf404fa8d0264b8a5eea3587233e17dc1f0","date":1065176686,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n    \n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"770281b8a8459cafcdd2354b6a06078fea2d83c9","date":1077308096,"type":3,"author":"Doug Cutting","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0, fi.storeTermVector)); // vector\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0));\t  // tokenized\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"416a5704f8ebded17d0bf54a9897f40469b74f21","date":1094069052,"type":3,"author":"Daniel Naber","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      Field.Index index;\n      boolean tokenize = (bits & 1) != 0;\n      if (fi.isIndexed && tokenize)\n        index = Field.Index.TOKENIZED;\n      else if (fi.isIndexed && !tokenize)\n        index = Field.Index.UN_TOKENIZED;\n      else\n        index = Field.Index.NO;\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\tField.Store.YES, index,\n\t\t\tfi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\ttrue,\t\t\t  // stored\n\t\t\tfi.isIndexed,\t\t  // indexed\n\t\t\t(bits & 1) != 0, fi.storeTermVector)); // vector\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e0eacdfc94f700086189bb44f7f73ed9cfd85c75","date":1095252623,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      if ((bits & 2) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        doc.add(new Field(fi.name, b));\n      }\n      else {\n        Field.Index index;\n        boolean tokenize = (bits & 1) != 0;\n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        doc.add(new Field(fi.name,\t\t  // name\n  \t\t\tfieldsStream.readString(), // read value\n  \t\t\tField.Store.YES, index,\n  \t\t\tfi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n      }\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      Field.Index index;\n      boolean tokenize = (bits & 1) != 0;\n      if (fi.isIndexed && tokenize)\n        index = Field.Index.TOKENIZED;\n      else if (fi.isIndexed && !tokenize)\n        index = Field.Index.UN_TOKENIZED;\n      else\n        index = Field.Index.NO;\n      doc.add(new Field(fi.name,\t\t  // name\n\t\t\tfieldsStream.readString(), // read value\n\t\t\tField.Store.YES, index,\n\t\t\tfi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b3d038c41e70424a134ef69a524f3cc0fb40d854","date":1096548028,"type":3,"author":"Christoph Goller","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          doc.add(new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              fi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n        }\n        else\n          doc.add(new Field(fi.name,      // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                fi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n      }\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n\n      if ((bits & 2) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        doc.add(new Field(fi.name, b));\n      }\n      else {\n        Field.Index index;\n        boolean tokenize = (bits & 1) != 0;\n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        doc.add(new Field(fi.name,\t\t  // name\n  \t\t\tfieldsStream.readString(), // read value\n  \t\t\tField.Store.YES, index,\n  \t\t\tfi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n      }\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"19a6b7ba320c4a6ad503295d894fcc4d3c1a85ae","date":1129740886,"type":3,"author":"Bernhard Messer","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        Field.TermVector termVector = null;\n        if (fi.storeTermVector) {\n          if (fi.storeOffsetWithTermVector) {\n            if (fi.storePositionWithTermVector) {\n              termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;\n            }\n            else {\n              termVector = Field.TermVector.WITH_OFFSETS;\n            }\n          }\n          else if (fi.storePositionWithTermVector) {\n            termVector = Field.TermVector.WITH_POSITIONS;\n          }\n          else {\n            termVector = Field.TermVector.YES;\n          }\n        }\n        else {\n          termVector = Field.TermVector.NO;\n        }\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          doc.add(new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              termVector));\n        }\n        else\n          doc.add(new Field(fi.name,      // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                termVector));\n      }\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          doc.add(new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              fi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n        }\n        else\n          doc.add(new Field(fi.name,      // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                fi.storeTermVector ? Field.TermVector.YES : Field.TermVector.NO));\n      }\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7aad95cf6c57b77f6233289a0c9e7467b6e59458","date":1130650726,"type":3,"author":"Yonik Seeley","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        Field.TermVector termVector = null;\n        if (fi.storeTermVector) {\n          if (fi.storeOffsetWithTermVector) {\n            if (fi.storePositionWithTermVector) {\n              termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;\n            }\n            else {\n              termVector = Field.TermVector.WITH_OFFSETS;\n            }\n          }\n          else if (fi.storePositionWithTermVector) {\n            termVector = Field.TermVector.WITH_POSITIONS;\n          }\n          else {\n            termVector = Field.TermVector.YES;\n          }\n        }\n        else {\n          termVector = Field.TermVector.NO;\n        }\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          Field f = new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              termVector);\n          f.setOmitNorms(fi.omitNorms);\n          doc.add(f);\n        }\n        else {\n          Field f = new Field(fi.name,     // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                termVector);\n          f.setOmitNorms(fi.omitNorms);\n          doc.add(f);\n        }\n      }\n    }\n\n    return doc;\n  }\n\n","sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        Field.TermVector termVector = null;\n        if (fi.storeTermVector) {\n          if (fi.storeOffsetWithTermVector) {\n            if (fi.storePositionWithTermVector) {\n              termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;\n            }\n            else {\n              termVector = Field.TermVector.WITH_OFFSETS;\n            }\n          }\n          else if (fi.storePositionWithTermVector) {\n            termVector = Field.TermVector.WITH_POSITIONS;\n          }\n          else {\n            termVector = Field.TermVector.YES;\n          }\n        }\n        else {\n          termVector = Field.TermVector.NO;\n        }\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          doc.add(new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              termVector));\n        }\n        else\n          doc.add(new Field(fi.name,      // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                termVector));\n      }\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"12d40284fd9481f79444bc63bc5d13847caddd3d","date":1149902602,"type":4,"author":"Grant Ingersoll","isMerge":false,"pathNew":"/dev/null","pathOld":"src/java/org/apache/lucene/index/FieldsReader#doc(int).mjava","sourceNew":null,"sourceOld":"  final Document doc(int n) throws IOException {\n    indexStream.seek(n * 8L);\n    long position = indexStream.readLong();\n    fieldsStream.seek(position);\n\n    Document doc = new Document();\n    int numFields = fieldsStream.readVInt();\n    for (int i = 0; i < numFields; i++) {\n      int fieldNumber = fieldsStream.readVInt();\n      FieldInfo fi = fieldInfos.fieldInfo(fieldNumber);\n\n      byte bits = fieldsStream.readByte();\n      \n      boolean compressed = (bits & FieldsWriter.FIELD_IS_COMPRESSED) != 0;\n      boolean tokenize = (bits & FieldsWriter.FIELD_IS_TOKENIZED) != 0;\n      \n      if ((bits & FieldsWriter.FIELD_IS_BINARY) != 0) {\n        final byte[] b = new byte[fieldsStream.readVInt()];\n        fieldsStream.readBytes(b, 0, b.length);\n        if (compressed)\n          doc.add(new Field(fi.name, uncompress(b), Field.Store.COMPRESS));\n        else\n          doc.add(new Field(fi.name, b, Field.Store.YES));\n      }\n      else {\n        Field.Index index;\n        Field.Store store = Field.Store.YES;\n        \n        if (fi.isIndexed && tokenize)\n          index = Field.Index.TOKENIZED;\n        else if (fi.isIndexed && !tokenize)\n          index = Field.Index.UN_TOKENIZED;\n        else\n          index = Field.Index.NO;\n        \n        Field.TermVector termVector = null;\n        if (fi.storeTermVector) {\n          if (fi.storeOffsetWithTermVector) {\n            if (fi.storePositionWithTermVector) {\n              termVector = Field.TermVector.WITH_POSITIONS_OFFSETS;\n            }\n            else {\n              termVector = Field.TermVector.WITH_OFFSETS;\n            }\n          }\n          else if (fi.storePositionWithTermVector) {\n            termVector = Field.TermVector.WITH_POSITIONS;\n          }\n          else {\n            termVector = Field.TermVector.YES;\n          }\n        }\n        else {\n          termVector = Field.TermVector.NO;\n        }\n        \n        if (compressed) {\n          store = Field.Store.COMPRESS;\n          final byte[] b = new byte[fieldsStream.readVInt()];\n          fieldsStream.readBytes(b, 0, b.length);\n          Field f = new Field(fi.name,      // field name\n              new String(uncompress(b), \"UTF-8\"), // uncompress the value and add as string\n              store,\n              index,\n              termVector);\n          f.setOmitNorms(fi.omitNorms);\n          doc.add(f);\n        }\n        else {\n          Field f = new Field(fi.name,     // name\n                fieldsStream.readString(), // read value\n                store,\n                index,\n                termVector);\n          f.setOmitNorms(fi.omitNorms);\n          doc.add(f);\n        }\n      }\n    }\n\n    return doc;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7aad95cf6c57b77f6233289a0c9e7467b6e59458":["19a6b7ba320c4a6ad503295d894fcc4d3c1a85ae"],"3f039c46e21390e3fc1f89f327ed17c2efd47212":["115938faea54dfe03fc2cddc7fe92bafb1be6c68"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"1a9cbbf404fa8d0264b8a5eea3587233e17dc1f0":["3f039c46e21390e3fc1f89f327ed17c2efd47212"],"416a5704f8ebded17d0bf54a9897f40469b74f21":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"115938faea54dfe03fc2cddc7fe92bafb1be6c68":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"12d40284fd9481f79444bc63bc5d13847caddd3d":["7aad95cf6c57b77f6233289a0c9e7467b6e59458"],"b3d038c41e70424a134ef69a524f3cc0fb40d854":["e0eacdfc94f700086189bb44f7f73ed9cfd85c75"],"19a6b7ba320c4a6ad503295d894fcc4d3c1a85ae":["b3d038c41e70424a134ef69a524f3cc0fb40d854"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["1a9cbbf404fa8d0264b8a5eea3587233e17dc1f0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"e0eacdfc94f700086189bb44f7f73ed9cfd85c75":["416a5704f8ebded17d0bf54a9897f40469b74f21"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["12d40284fd9481f79444bc63bc5d13847caddd3d"]},"commit2Childs":{"7aad95cf6c57b77f6233289a0c9e7467b6e59458":["12d40284fd9481f79444bc63bc5d13847caddd3d"],"3f039c46e21390e3fc1f89f327ed17c2efd47212":["1a9cbbf404fa8d0264b8a5eea3587233e17dc1f0"],"91109046a59c58ee0ee5d0d2767b08d1f30d6702":["115938faea54dfe03fc2cddc7fe92bafb1be6c68"],"1a9cbbf404fa8d0264b8a5eea3587233e17dc1f0":["770281b8a8459cafcdd2354b6a06078fea2d83c9"],"416a5704f8ebded17d0bf54a9897f40469b74f21":["e0eacdfc94f700086189bb44f7f73ed9cfd85c75"],"115938faea54dfe03fc2cddc7fe92bafb1be6c68":["3f039c46e21390e3fc1f89f327ed17c2efd47212"],"12d40284fd9481f79444bc63bc5d13847caddd3d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b3d038c41e70424a134ef69a524f3cc0fb40d854":["19a6b7ba320c4a6ad503295d894fcc4d3c1a85ae"],"19a6b7ba320c4a6ad503295d894fcc4d3c1a85ae":["7aad95cf6c57b77f6233289a0c9e7467b6e59458"],"770281b8a8459cafcdd2354b6a06078fea2d83c9":["416a5704f8ebded17d0bf54a9897f40469b74f21"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["91109046a59c58ee0ee5d0d2767b08d1f30d6702"],"e0eacdfc94f700086189bb44f7f73ed9cfd85c75":["b3d038c41e70424a134ef69a524f3cc0fb40d854"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}