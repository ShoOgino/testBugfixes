{"path":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","commits":[{"id":"31f025ae60076ae95274433f3fe8e6ace2857a87","date":1326669465,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","pathOld":"/dev/null","sourceNew":"  public void testBasic() throws Exception {\n\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a44b232879361a7ace3520b5b313094a9a35e044","date":1327356188,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","sourceNew":"  public void testBasic() throws Exception {\n    Directory dir = newDirectory();\n    \n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testBasic() throws Exception {\n\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5a89676536a5d3e2e875a9eed6b3f22a63cca643","date":1327356915,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","sourceNew":"  public void testBasic() throws Exception {\n    Directory dir = newDirectory();\n    \n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testBasic() throws Exception {\n\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","date":1327523564,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","sourceNew":"  public void testBasic() throws Exception {\n    Directory dir = newDirectory();\n    \n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testBasic() throws Exception {\n\n    // Currently only SimpleText can index offsets into postings:\n    Assume.assumeTrue(Codec.getDefault().getName().equals(\"SimpleText\"));\n\n    Directory dir = newDirectory();\n    RandomIndexWriter w = new RandomIndexWriter(random, dir);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":5,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestPostingsOffsets#testBasic().mjava","sourceNew":"  public void testBasic() throws Exception {\n    Directory dir = newDirectory();\n    \n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testBasic() throws Exception {\n    Directory dir = newDirectory();\n    \n    RandomIndexWriter w = new RandomIndexWriter(random, dir, iwc);\n    Document doc = new Document();\n\n    FieldType ft = new FieldType(TextField.TYPE_UNSTORED);\n    ft.setIndexOptions(FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);\n    Token[] tokens = new Token[] {\n      makeToken(\"a\", 1, 0, 6),\n      makeToken(\"b\", 1, 8, 9),\n      makeToken(\"a\", 1, 9, 17),\n      makeToken(\"c\", 1, 19, 50),\n    };\n    doc.add(new Field(\"content\", new CannedAnalyzer.CannedTokenizer(tokens), ft));\n\n    w.addDocument(doc, new CannedAnalyzer(tokens));\n    IndexReader r = w.getReader();\n    w.close();\n\n    DocsAndPositionsEnum dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"a\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(2, dp.freq());\n    assertEquals(0, dp.nextPosition());\n    assertEquals(0, dp.startOffset());\n    assertEquals(6, dp.endOffset());\n    assertEquals(2, dp.nextPosition());\n    assertEquals(9, dp.startOffset());\n    assertEquals(17, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"b\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(1, dp.nextPosition());\n    assertEquals(8, dp.startOffset());\n    assertEquals(9, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    dp = MultiFields.getTermPositionsEnum(r, null, \"content\", new BytesRef(\"c\"), true);\n    assertNotNull(dp);\n    assertEquals(0, dp.nextDoc());\n    assertEquals(1, dp.freq());\n    assertEquals(3, dp.nextPosition());\n    assertEquals(19, dp.startOffset());\n    assertEquals(50, dp.endOffset());\n    assertEquals(DocsEnum.NO_MORE_DOCS, dp.nextDoc());\n\n    r.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":["31f025ae60076ae95274433f3fe8e6ace2857a87","a44b232879361a7ace3520b5b313094a9a35e044"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a44b232879361a7ace3520b5b313094a9a35e044"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":["31f025ae60076ae95274433f3fe8e6ace2857a87","a44b232879361a7ace3520b5b313094a9a35e044"],"a44b232879361a7ace3520b5b313094a9a35e044":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"5a89676536a5d3e2e875a9eed6b3f22a63cca643":[],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["31f025ae60076ae95274433f3fe8e6ace2857a87"],"a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d":[],"a44b232879361a7ace3520b5b313094a9a35e044":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","3a119bbc8703c10faa329ec201c654b3a35a1e3e","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d"],"31f025ae60076ae95274433f3fe8e6ace2857a87":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","a44b232879361a7ace3520b5b313094a9a35e044"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["5a89676536a5d3e2e875a9eed6b3f22a63cca643","a3c68e20c73359a10cf3eb4a35c9fa7ab1f3c30d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}