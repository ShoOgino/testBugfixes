{"path":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#SimpleTextBKDWriter(int,Directory,String,int,int,int,double,long,boolean,boolean,long,int).mjava","commits":[{"id":"9fc0d60683b47b5d922124c31f57c8b34734f9e6","date":1480846684,"type":0,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#SimpleTextBKDWriter(int,Directory,String,int,int,int,double,long,boolean,boolean,long,int).mjava","pathOld":"/dev/null","sourceNew":"  private SimpleTextBKDWriter(int maxDoc, Directory tempDir, String tempFileNamePrefix, int numDims, int bytesPerDim,\n                              int maxPointsInLeafNode, double maxMBSortInHeap, long totalPointCount,\n                              boolean singleValuePerDoc, boolean longOrds, long offlineSorterBufferMB, int offlineSorterMaxTempFiles) throws IOException {\n    verifyParams(numDims, maxPointsInLeafNode, maxMBSortInHeap, totalPointCount);\n    // We use tracking dir to deal with removing files on exception, so each place that\n    // creates temp files doesn't need crazy try/finally/sucess logic:\n    this.tempDir = new TrackingDirectoryWrapper(tempDir);\n    this.tempFileNamePrefix = tempFileNamePrefix;\n    this.maxPointsInLeafNode = maxPointsInLeafNode;\n    this.numDims = numDims;\n    this.bytesPerDim = bytesPerDim;\n    this.totalPointCount = totalPointCount;\n    this.maxDoc = maxDoc;\n    this.offlineSorterBufferMB = OfflineSorter.BufferSize.megabytes(offlineSorterBufferMB);\n    this.offlineSorterMaxTempFiles = offlineSorterMaxTempFiles;\n    docsSeen = new FixedBitSet(maxDoc);\n    packedBytesLength = numDims * bytesPerDim;\n\n    scratchDiff = new byte[bytesPerDim];\n    scratch1 = new byte[packedBytesLength];\n    scratch2 = new byte[packedBytesLength];\n    commonPrefixLengths = new int[numDims];\n\n    minPackedValue = new byte[packedBytesLength];\n    maxPackedValue = new byte[packedBytesLength];\n\n    // If we may have more than 1+Integer.MAX_VALUE values, then we must encode ords with long (8 bytes), else we can use int (4 bytes).\n    this.longOrds = longOrds;\n\n    this.singleValuePerDoc = singleValuePerDoc;\n\n    // dimensional values (numDims * bytesPerDim) + ord (int or long) + docID (int)\n    if (singleValuePerDoc) {\n      // Lucene only supports up to 2.1 docs, so we better not need longOrds in this case:\n      assert longOrds == false;\n      bytesPerDoc = packedBytesLength + Integer.BYTES;\n    } else if (longOrds) {\n      bytesPerDoc = packedBytesLength + Long.BYTES + Integer.BYTES;\n    } else {\n      bytesPerDoc = packedBytesLength + Integer.BYTES + Integer.BYTES;\n    }\n\n    // As we recurse, we compute temporary partitions of the data, halving the\n    // number of points at each recursion.  Once there are few enough points,\n    // we can switch to sorting in heap instead of offline (on disk).  At any\n    // time in the recursion, we hold the number of points at that level, plus\n    // all recursive halves (i.e. 16 + 8 + 4 + 2) so the memory usage is 2X\n    // what that level would consume, so we multiply by 0.5 to convert from\n    // bytes to points here.  Each dimension has its own sorted partition, so\n    // we must divide by numDims as wel.\n\n    maxPointsSortInHeap = (int) (0.5 * (maxMBSortInHeap * 1024 * 1024) / (bytesPerDoc * numDims));\n\n    // Finally, we must be able to hold at least the leaf node in heap during build:\n    if (maxPointsSortInHeap < maxPointsInLeafNode) {\n      throw new IllegalArgumentException(\"maxMBSortInHeap=\" + maxMBSortInHeap + \" only allows for maxPointsSortInHeap=\" + maxPointsSortInHeap + \", but this is less than maxPointsInLeafNode=\" + maxPointsInLeafNode + \"; either increase maxMBSortInHeap or decrease maxPointsInLeafNode\");\n    }\n\n    // We write first maxPointsSortInHeap in heap, then cutover to offline for additional points:\n    heapPointWriter = new HeapPointWriter(16, maxPointsSortInHeap, packedBytesLength, longOrds, singleValuePerDoc);\n\n    this.maxMBSortInHeap = maxMBSortInHeap;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":0,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#SimpleTextBKDWriter(int,Directory,String,int,int,int,double,long,boolean,boolean,long,int).mjava","pathOld":"/dev/null","sourceNew":"  private SimpleTextBKDWriter(int maxDoc, Directory tempDir, String tempFileNamePrefix, int numDims, int bytesPerDim,\n                              int maxPointsInLeafNode, double maxMBSortInHeap, long totalPointCount,\n                              boolean singleValuePerDoc, boolean longOrds, long offlineSorterBufferMB, int offlineSorterMaxTempFiles) throws IOException {\n    verifyParams(numDims, maxPointsInLeafNode, maxMBSortInHeap, totalPointCount);\n    // We use tracking dir to deal with removing files on exception, so each place that\n    // creates temp files doesn't need crazy try/finally/sucess logic:\n    this.tempDir = new TrackingDirectoryWrapper(tempDir);\n    this.tempFileNamePrefix = tempFileNamePrefix;\n    this.maxPointsInLeafNode = maxPointsInLeafNode;\n    this.numDims = numDims;\n    this.bytesPerDim = bytesPerDim;\n    this.totalPointCount = totalPointCount;\n    this.maxDoc = maxDoc;\n    this.offlineSorterBufferMB = OfflineSorter.BufferSize.megabytes(offlineSorterBufferMB);\n    this.offlineSorterMaxTempFiles = offlineSorterMaxTempFiles;\n    docsSeen = new FixedBitSet(maxDoc);\n    packedBytesLength = numDims * bytesPerDim;\n\n    scratchDiff = new byte[bytesPerDim];\n    scratch1 = new byte[packedBytesLength];\n    scratch2 = new byte[packedBytesLength];\n    commonPrefixLengths = new int[numDims];\n\n    minPackedValue = new byte[packedBytesLength];\n    maxPackedValue = new byte[packedBytesLength];\n\n    // If we may have more than 1+Integer.MAX_VALUE values, then we must encode ords with long (8 bytes), else we can use int (4 bytes).\n    this.longOrds = longOrds;\n\n    this.singleValuePerDoc = singleValuePerDoc;\n\n    // dimensional values (numDims * bytesPerDim) + ord (int or long) + docID (int)\n    if (singleValuePerDoc) {\n      // Lucene only supports up to 2.1 docs, so we better not need longOrds in this case:\n      assert longOrds == false;\n      bytesPerDoc = packedBytesLength + Integer.BYTES;\n    } else if (longOrds) {\n      bytesPerDoc = packedBytesLength + Long.BYTES + Integer.BYTES;\n    } else {\n      bytesPerDoc = packedBytesLength + Integer.BYTES + Integer.BYTES;\n    }\n\n    // As we recurse, we compute temporary partitions of the data, halving the\n    // number of points at each recursion.  Once there are few enough points,\n    // we can switch to sorting in heap instead of offline (on disk).  At any\n    // time in the recursion, we hold the number of points at that level, plus\n    // all recursive halves (i.e. 16 + 8 + 4 + 2) so the memory usage is 2X\n    // what that level would consume, so we multiply by 0.5 to convert from\n    // bytes to points here.  Each dimension has its own sorted partition, so\n    // we must divide by numDims as wel.\n\n    maxPointsSortInHeap = (int) (0.5 * (maxMBSortInHeap * 1024 * 1024) / (bytesPerDoc * numDims));\n\n    // Finally, we must be able to hold at least the leaf node in heap during build:\n    if (maxPointsSortInHeap < maxPointsInLeafNode) {\n      throw new IllegalArgumentException(\"maxMBSortInHeap=\" + maxMBSortInHeap + \" only allows for maxPointsSortInHeap=\" + maxPointsSortInHeap + \", but this is less than maxPointsInLeafNode=\" + maxPointsInLeafNode + \"; either increase maxMBSortInHeap or decrease maxPointsInLeafNode\");\n    }\n\n    // We write first maxPointsSortInHeap in heap, then cutover to offline for additional points:\n    heapPointWriter = new HeapPointWriter(16, maxPointsSortInHeap, packedBytesLength, longOrds, singleValuePerDoc);\n\n    this.maxMBSortInHeap = maxMBSortInHeap;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f6652c943595e92c187ee904c382863013eae28f","date":1539042663,"type":5,"author":"Nicholas Knize","isMerge":false,"pathNew":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#SimpleTextBKDWriter(int,Directory,String,int,int,int,int,double,long,boolean,boolean,long,int).mjava","pathOld":"lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextBKDWriter#SimpleTextBKDWriter(int,Directory,String,int,int,int,double,long,boolean,boolean,long,int).mjava","sourceNew":"  private SimpleTextBKDWriter(int maxDoc, Directory tempDir, String tempFileNamePrefix, int numDataDims, int numIndexDims, int bytesPerDim,\n                              int maxPointsInLeafNode, double maxMBSortInHeap, long totalPointCount,\n                              boolean singleValuePerDoc, boolean longOrds, long offlineSorterBufferMB, int offlineSorterMaxTempFiles) throws IOException {\n    verifyParams(numDataDims, numIndexDims, maxPointsInLeafNode, maxMBSortInHeap, totalPointCount);\n    // We use tracking dir to deal with removing files on exception, so each place that\n    // creates temp files doesn't need crazy try/finally/sucess logic:\n    this.tempDir = new TrackingDirectoryWrapper(tempDir);\n    this.tempFileNamePrefix = tempFileNamePrefix;\n    this.maxPointsInLeafNode = maxPointsInLeafNode;\n    this.numDataDims = numDataDims;\n    this.numIndexDims = numIndexDims;\n    this.bytesPerDim = bytesPerDim;\n    this.totalPointCount = totalPointCount;\n    this.maxDoc = maxDoc;\n    this.offlineSorterBufferMB = OfflineSorter.BufferSize.megabytes(offlineSorterBufferMB);\n    this.offlineSorterMaxTempFiles = offlineSorterMaxTempFiles;\n    docsSeen = new FixedBitSet(maxDoc);\n    packedBytesLength = numDataDims * bytesPerDim;\n    packedIndexBytesLength = numIndexDims * bytesPerDim;\n\n    scratchDiff = new byte[bytesPerDim];\n    scratch1 = new byte[packedBytesLength];\n    scratch2 = new byte[packedBytesLength];\n    commonPrefixLengths = new int[numDataDims];\n\n    minPackedValue = new byte[packedIndexBytesLength];\n    maxPackedValue = new byte[packedIndexBytesLength];\n\n    // If we may have more than 1+Integer.MAX_VALUE values, then we must encode ords with long (8 bytes), else we can use int (4 bytes).\n    this.longOrds = longOrds;\n\n    this.singleValuePerDoc = singleValuePerDoc;\n\n    // dimensional values (numDims * bytesPerDim) + ord (int or long) + docID (int)\n    if (singleValuePerDoc) {\n      // Lucene only supports up to 2.1 docs, so we better not need longOrds in this case:\n      assert longOrds == false;\n      bytesPerDoc = packedBytesLength + Integer.BYTES;\n    } else if (longOrds) {\n      bytesPerDoc = packedBytesLength + Long.BYTES + Integer.BYTES;\n    } else {\n      bytesPerDoc = packedBytesLength + Integer.BYTES + Integer.BYTES;\n    }\n\n    // As we recurse, we compute temporary partitions of the data, halving the\n    // number of points at each recursion.  Once there are few enough points,\n    // we can switch to sorting in heap instead of offline (on disk).  At any\n    // time in the recursion, we hold the number of points at that level, plus\n    // all recursive halves (i.e. 16 + 8 + 4 + 2) so the memory usage is 2X\n    // what that level would consume, so we multiply by 0.5 to convert from\n    // bytes to points here.  Each dimension has its own sorted partition, so\n    // we must divide by numDims as wel.\n\n    maxPointsSortInHeap = (int) (0.5 * (maxMBSortInHeap * 1024 * 1024) / (bytesPerDoc * numDataDims));\n\n    // Finally, we must be able to hold at least the leaf node in heap during build:\n    if (maxPointsSortInHeap < maxPointsInLeafNode) {\n      throw new IllegalArgumentException(\"maxMBSortInHeap=\" + maxMBSortInHeap + \" only allows for maxPointsSortInHeap=\" + maxPointsSortInHeap + \", but this is less than maxPointsInLeafNode=\" + maxPointsInLeafNode + \"; either increase maxMBSortInHeap or decrease maxPointsInLeafNode\");\n    }\n\n    // We write first maxPointsSortInHeap in heap, then cutover to offline for additional points:\n    heapPointWriter = new HeapPointWriter(16, maxPointsSortInHeap, packedBytesLength, longOrds, singleValuePerDoc);\n\n    this.maxMBSortInHeap = maxMBSortInHeap;\n  }\n\n","sourceOld":"  private SimpleTextBKDWriter(int maxDoc, Directory tempDir, String tempFileNamePrefix, int numDims, int bytesPerDim,\n                              int maxPointsInLeafNode, double maxMBSortInHeap, long totalPointCount,\n                              boolean singleValuePerDoc, boolean longOrds, long offlineSorterBufferMB, int offlineSorterMaxTempFiles) throws IOException {\n    verifyParams(numDims, maxPointsInLeafNode, maxMBSortInHeap, totalPointCount);\n    // We use tracking dir to deal with removing files on exception, so each place that\n    // creates temp files doesn't need crazy try/finally/sucess logic:\n    this.tempDir = new TrackingDirectoryWrapper(tempDir);\n    this.tempFileNamePrefix = tempFileNamePrefix;\n    this.maxPointsInLeafNode = maxPointsInLeafNode;\n    this.numDims = numDims;\n    this.bytesPerDim = bytesPerDim;\n    this.totalPointCount = totalPointCount;\n    this.maxDoc = maxDoc;\n    this.offlineSorterBufferMB = OfflineSorter.BufferSize.megabytes(offlineSorterBufferMB);\n    this.offlineSorterMaxTempFiles = offlineSorterMaxTempFiles;\n    docsSeen = new FixedBitSet(maxDoc);\n    packedBytesLength = numDims * bytesPerDim;\n\n    scratchDiff = new byte[bytesPerDim];\n    scratch1 = new byte[packedBytesLength];\n    scratch2 = new byte[packedBytesLength];\n    commonPrefixLengths = new int[numDims];\n\n    minPackedValue = new byte[packedBytesLength];\n    maxPackedValue = new byte[packedBytesLength];\n\n    // If we may have more than 1+Integer.MAX_VALUE values, then we must encode ords with long (8 bytes), else we can use int (4 bytes).\n    this.longOrds = longOrds;\n\n    this.singleValuePerDoc = singleValuePerDoc;\n\n    // dimensional values (numDims * bytesPerDim) + ord (int or long) + docID (int)\n    if (singleValuePerDoc) {\n      // Lucene only supports up to 2.1 docs, so we better not need longOrds in this case:\n      assert longOrds == false;\n      bytesPerDoc = packedBytesLength + Integer.BYTES;\n    } else if (longOrds) {\n      bytesPerDoc = packedBytesLength + Long.BYTES + Integer.BYTES;\n    } else {\n      bytesPerDoc = packedBytesLength + Integer.BYTES + Integer.BYTES;\n    }\n\n    // As we recurse, we compute temporary partitions of the data, halving the\n    // number of points at each recursion.  Once there are few enough points,\n    // we can switch to sorting in heap instead of offline (on disk).  At any\n    // time in the recursion, we hold the number of points at that level, plus\n    // all recursive halves (i.e. 16 + 8 + 4 + 2) so the memory usage is 2X\n    // what that level would consume, so we multiply by 0.5 to convert from\n    // bytes to points here.  Each dimension has its own sorted partition, so\n    // we must divide by numDims as wel.\n\n    maxPointsSortInHeap = (int) (0.5 * (maxMBSortInHeap * 1024 * 1024) / (bytesPerDoc * numDims));\n\n    // Finally, we must be able to hold at least the leaf node in heap during build:\n    if (maxPointsSortInHeap < maxPointsInLeafNode) {\n      throw new IllegalArgumentException(\"maxMBSortInHeap=\" + maxMBSortInHeap + \" only allows for maxPointsSortInHeap=\" + maxPointsSortInHeap + \", but this is less than maxPointsInLeafNode=\" + maxPointsInLeafNode + \"; either increase maxMBSortInHeap or decrease maxPointsInLeafNode\");\n    }\n\n    // We write first maxPointsSortInHeap in heap, then cutover to offline for additional points:\n    heapPointWriter = new HeapPointWriter(16, maxPointsSortInHeap, packedBytesLength, longOrds, singleValuePerDoc);\n\n    this.maxMBSortInHeap = maxMBSortInHeap;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f6652c943595e92c187ee904c382863013eae28f":["9fc0d60683b47b5d922124c31f57c8b34734f9e6"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9856095f7afb5a607bf5e65077615ed91273508c":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","9fc0d60683b47b5d922124c31f57c8b34734f9e6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["f6652c943595e92c187ee904c382863013eae28f"]},"commit2Childs":{"9fc0d60683b47b5d922124c31f57c8b34734f9e6":["f6652c943595e92c187ee904c382863013eae28f","9856095f7afb5a607bf5e65077615ed91273508c"],"f6652c943595e92c187ee904c382863013eae28f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9fc0d60683b47b5d922124c31f57c8b34734f9e6","9856095f7afb5a607bf5e65077615ed91273508c"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}