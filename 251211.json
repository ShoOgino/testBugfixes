{"path":"lucene/test-framework/src/java/org/apache/lucene/util/TestSecurityManager#checkExec(String).mjava","commits":[{"id":"846ecaa92e1487e1e68f08867aebbae94690bcb2","date":1575282989,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/util/TestSecurityManager#checkExec(String).mjava","pathOld":"/dev/null","sourceNew":"  // TODO: move this stuff into a Solr (non-test) SecurityManager!\n  /**\n   * {@inheritDoc}\n   * <p>This method implements hacks to workaround hadoop's garbage Shell and FileUtil code\n   */\n  @Override\n  public void checkExec(String cmd) {\n    // NOTE: it would be tempting to just allow anything from hadoop's Shell class, but then\n    // that would just give an easy vector for RCE (use hadoop Shell instead of e.g. ProcessBuilder)\n    // so we whitelist actual caller impl methods instead.\n    for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\n      // hadoop insists on shelling out to get the user's supplementary groups?\n      if (\"org.apache.hadoop.security.ShellBasedUnixGroupsMapping\".equals(element.getClassName()) &&\n          \"getGroups\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'df' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DF\".equals(element.getClassName()) &&\n          \"getFilesystem\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'du' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DU\".equals(element.getClassName()) &&\n          \"refresh\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'ls' command instead of java nio apis?\n      if (\"org.apache.hadoop.util.DiskChecker\".equals(element.getClassName()) &&\n          \"checkDir\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'stat' command instead of Files.getAttributes?\n      if (\"org.apache.hadoop.fs.HardLink\".equals(element.getClassName()) &&\n          \"getLinkCount\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop \"canExecute\" method doesn't handle securityexception and fails completely.\n      // so, lie to it, and tell it we will happily execute, so it does not crash.\n      if (\"org.apache.hadoop.fs.FileUtil\".equals(element.getClassName()) &&\n          \"canExecute\".equals(element.getMethodName())) {\n        return;\n      }\n    }\n    super.checkExec(cmd);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cf4186ad2efcdebf9859a7b14723a280571c6587","date":1575575603,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"solr/test-framework/src/java/org/apache/solr/util/SolrSecurityManager#checkExec(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/util/TestSecurityManager#checkExec(String).mjava","sourceNew":"  // TODO: move this stuff into a Solr (non-test) SecurityManager!\n  /**\n   * {@inheritDoc}\n   * <p>This method implements hacks to workaround hadoop's garbage Shell and FileUtil code\n   */\n  @Override\n  public void checkExec(String cmd) {\n    // NOTE: it would be tempting to just allow anything from hadoop's Shell class, but then\n    // that would just give an easy vector for RCE (use hadoop Shell instead of e.g. ProcessBuilder)\n    // so we whitelist actual caller impl methods instead.\n    for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\n      // hadoop insists on shelling out to get the user's supplementary groups?\n      if (\"org.apache.hadoop.security.ShellBasedUnixGroupsMapping\".equals(element.getClassName()) &&\n          \"getGroups\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'df' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DF\".equals(element.getClassName()) &&\n          \"getFilesystem\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'du' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DU\".equals(element.getClassName()) &&\n          \"refresh\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'ls' command instead of java nio apis?\n      if (\"org.apache.hadoop.util.DiskChecker\".equals(element.getClassName()) &&\n          \"checkDir\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'stat' command instead of Files.getAttributes?\n      if (\"org.apache.hadoop.fs.HardLink\".equals(element.getClassName()) &&\n          \"getLinkCount\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop \"canExecute\" method doesn't handle securityexception and fails completely.\n      // so, lie to it, and tell it we will happily execute, so it does not crash.\n      if (\"org.apache.hadoop.fs.FileUtil\".equals(element.getClassName()) &&\n          \"canExecute\".equals(element.getMethodName())) {\n        return;\n      }\n    }\n    super.checkExec(cmd);\n  }\n\n","sourceOld":"  // TODO: move this stuff into a Solr (non-test) SecurityManager!\n  /**\n   * {@inheritDoc}\n   * <p>This method implements hacks to workaround hadoop's garbage Shell and FileUtil code\n   */\n  @Override\n  public void checkExec(String cmd) {\n    // NOTE: it would be tempting to just allow anything from hadoop's Shell class, but then\n    // that would just give an easy vector for RCE (use hadoop Shell instead of e.g. ProcessBuilder)\n    // so we whitelist actual caller impl methods instead.\n    for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\n      // hadoop insists on shelling out to get the user's supplementary groups?\n      if (\"org.apache.hadoop.security.ShellBasedUnixGroupsMapping\".equals(element.getClassName()) &&\n          \"getGroups\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'df' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DF\".equals(element.getClassName()) &&\n          \"getFilesystem\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'du' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DU\".equals(element.getClassName()) &&\n          \"refresh\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'ls' command instead of java nio apis?\n      if (\"org.apache.hadoop.util.DiskChecker\".equals(element.getClassName()) &&\n          \"checkDir\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'stat' command instead of Files.getAttributes?\n      if (\"org.apache.hadoop.fs.HardLink\".equals(element.getClassName()) &&\n          \"getLinkCount\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop \"canExecute\" method doesn't handle securityexception and fails completely.\n      // so, lie to it, and tell it we will happily execute, so it does not crash.\n      if (\"org.apache.hadoop.fs.FileUtil\".equals(element.getClassName()) &&\n          \"canExecute\".equals(element.getMethodName())) {\n        return;\n      }\n    }\n    super.checkExec(cmd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c173aec5dba4a880e26706e8ca1ec9e67b74ed5","date":1575629849,"type":5,"author":"Dawid Weiss","isMerge":true,"pathNew":"solr/test-framework/src/java/org/apache/solr/util/SolrSecurityManager#checkExec(String).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/util/TestSecurityManager#checkExec(String).mjava","sourceNew":"  // TODO: move this stuff into a Solr (non-test) SecurityManager!\n  /**\n   * {@inheritDoc}\n   * <p>This method implements hacks to workaround hadoop's garbage Shell and FileUtil code\n   */\n  @Override\n  public void checkExec(String cmd) {\n    // NOTE: it would be tempting to just allow anything from hadoop's Shell class, but then\n    // that would just give an easy vector for RCE (use hadoop Shell instead of e.g. ProcessBuilder)\n    // so we whitelist actual caller impl methods instead.\n    for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\n      // hadoop insists on shelling out to get the user's supplementary groups?\n      if (\"org.apache.hadoop.security.ShellBasedUnixGroupsMapping\".equals(element.getClassName()) &&\n          \"getGroups\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'df' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DF\".equals(element.getClassName()) &&\n          \"getFilesystem\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'du' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DU\".equals(element.getClassName()) &&\n          \"refresh\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'ls' command instead of java nio apis?\n      if (\"org.apache.hadoop.util.DiskChecker\".equals(element.getClassName()) &&\n          \"checkDir\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'stat' command instead of Files.getAttributes?\n      if (\"org.apache.hadoop.fs.HardLink\".equals(element.getClassName()) &&\n          \"getLinkCount\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop \"canExecute\" method doesn't handle securityexception and fails completely.\n      // so, lie to it, and tell it we will happily execute, so it does not crash.\n      if (\"org.apache.hadoop.fs.FileUtil\".equals(element.getClassName()) &&\n          \"canExecute\".equals(element.getMethodName())) {\n        return;\n      }\n    }\n    super.checkExec(cmd);\n  }\n\n","sourceOld":"  // TODO: move this stuff into a Solr (non-test) SecurityManager!\n  /**\n   * {@inheritDoc}\n   * <p>This method implements hacks to workaround hadoop's garbage Shell and FileUtil code\n   */\n  @Override\n  public void checkExec(String cmd) {\n    // NOTE: it would be tempting to just allow anything from hadoop's Shell class, but then\n    // that would just give an easy vector for RCE (use hadoop Shell instead of e.g. ProcessBuilder)\n    // so we whitelist actual caller impl methods instead.\n    for (StackTraceElement element : Thread.currentThread().getStackTrace()) {\n      // hadoop insists on shelling out to get the user's supplementary groups?\n      if (\"org.apache.hadoop.security.ShellBasedUnixGroupsMapping\".equals(element.getClassName()) &&\n          \"getGroups\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'df' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DF\".equals(element.getClassName()) &&\n          \"getFilesystem\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'du' command instead of using FileStore?\n      if (\"org.apache.hadoop.fs.DU\".equals(element.getClassName()) &&\n          \"refresh\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'ls' command instead of java nio apis?\n      if (\"org.apache.hadoop.util.DiskChecker\".equals(element.getClassName()) &&\n          \"checkDir\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop insists on shelling out to parse 'stat' command instead of Files.getAttributes?\n      if (\"org.apache.hadoop.fs.HardLink\".equals(element.getClassName()) &&\n          \"getLinkCount\".equals(element.getMethodName())) {\n        return;\n      }\n      // hadoop \"canExecute\" method doesn't handle securityexception and fails completely.\n      // so, lie to it, and tell it we will happily execute, so it does not crash.\n      if (\"org.apache.hadoop.fs.FileUtil\".equals(element.getClassName()) &&\n          \"canExecute\".equals(element.getMethodName())) {\n        return;\n      }\n    }\n    super.checkExec(cmd);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"cf4186ad2efcdebf9859a7b14723a280571c6587":["846ecaa92e1487e1e68f08867aebbae94690bcb2"],"846ecaa92e1487e1e68f08867aebbae94690bcb2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"2c173aec5dba4a880e26706e8ca1ec9e67b74ed5":["846ecaa92e1487e1e68f08867aebbae94690bcb2","cf4186ad2efcdebf9859a7b14723a280571c6587"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["cf4186ad2efcdebf9859a7b14723a280571c6587"]},"commit2Childs":{"cf4186ad2efcdebf9859a7b14723a280571c6587":["2c173aec5dba4a880e26706e8ca1ec9e67b74ed5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"846ecaa92e1487e1e68f08867aebbae94690bcb2":["cf4186ad2efcdebf9859a7b14723a280571c6587","2c173aec5dba4a880e26706e8ca1ec9e67b74ed5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["846ecaa92e1487e1e68f08867aebbae94690bcb2"],"2c173aec5dba4a880e26706e8ca1ec9e67b74ed5":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["2c173aec5dba4a880e26706e8ca1ec9e67b74ed5","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}