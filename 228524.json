{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","commits":[{"id":"2d5e92410233ff9d02c5bdec5b485552b55be67e","date":1440681066,"type":0,"author":"Mark Harwood","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","pathOld":"/dev/null","sourceNew":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a","date":1528168051,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","sourceNew":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    //TODO lets append directly to termAttribute?\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","sourceOld":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","sourceNew":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    //TODO lets append directly to termAttribute?\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","sourceOld":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/FingerprintFilter#buildSingleOutputToken().mjava","sourceNew":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    //TODO lets append directly to termAttribute?\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","sourceOld":"  /**\n   * Gathers all tokens from input, de-duplicates, sorts then concatenates.\n   * \n   * @return false for end of stream; true otherwise\n   */\n  private final boolean buildSingleOutputToken() throws IOException {\n    inputEnded = false;\n\n    char clonedLastTerm[] = null;\n    uniqueTerms = new CharArraySet(8, false);\n    int outputTokenSize = 0;\n    while (input.incrementToken()) {\n      if (outputTokenSize > maxOutputTokenSize) {\n        continue;\n      }\n\n      final char term[] = termAttribute.buffer();\n      final int length = termAttribute.length();\n\n      if (!uniqueTerms.contains(term, 0, length)) {\n        // clone the term, and add to the set of seen terms.\n        clonedLastTerm = new char[length];\n        System.arraycopy(term, 0, clonedLastTerm, 0, length);\n        if (uniqueTerms.size() > 0) {\n          outputTokenSize++; //Add 1 for the separator char we will output\n        }\n        uniqueTerms.add(clonedLastTerm);\n        outputTokenSize += length;\n      }\n    }\n    //Force end-of-stream operations to get the final state.\n    input.end();\n    inputEnded = true;\n\n    //Gathering complete - now output exactly zero or one token:\n\n    //Set the attributes for the single output token\n    offsetAtt.setOffset(0, offsetAtt.endOffset());\n    posLenAtt.setPositionLength(1);\n    posIncrAtt.setPositionIncrement(1);\n    typeAtt.setType(\"fingerprint\");\n\n    //No tokens gathered - no output\n    if (uniqueTerms.size() < 1) {\n      termAttribute.setEmpty();\n      return false;\n    }\n\n    //Tokens gathered are too large - no output\n    if (outputTokenSize > maxOutputTokenSize) {\n      termAttribute.setEmpty();\n      uniqueTerms.clear();\n      return false;\n    }\n\n    // Special case - faster option when we have a single token\n    if (uniqueTerms.size() == 1) {\n      termAttribute.setEmpty().append(new String(clonedLastTerm));\n      uniqueTerms.clear();\n      return true;\n    }\n\n    // Sort the set of deduplicated tokens and combine \n    Object[] items = uniqueTerms.toArray();\n\n    Arrays.sort(items, new Comparator<Object>() {\n      @Override\n      public int compare(Object o1, Object o2) {\n        char v1[] = (char[]) o1;\n        char v2[] = (char[]) o2;\n        int len1 = v1.length;\n        int len2 = v2.length;\n        int lim = Math.min(len1, len2);\n\n        int k = 0;\n        while (k < lim) {\n          char c1 = v1[k];\n          char c2 = v2[k];\n          if (c1 != c2) {\n            return c1 - c2;\n          }\n          k++;\n        }\n        return len1 - len2;\n      }\n    });\n\n    StringBuilder sb = new StringBuilder();\n    for (Object item : items) {\n      if (sb.length() >= 1) {\n        sb.append(separator);\n      }\n      sb.append((char[]) item);\n    }\n    termAttribute.setEmpty().append(sb);\n    uniqueTerms.clear();\n    return true;\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2d5e92410233ff9d02c5bdec5b485552b55be67e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["2d5e92410233ff9d02c5bdec5b485552b55be67e","9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a"],"9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a":["2d5e92410233ff9d02c5bdec5b485552b55be67e"],"f592209545c71895260367152601e9200399776d":["2d5e92410233ff9d02c5bdec5b485552b55be67e","9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a"]},"commit2Childs":{"2d5e92410233ff9d02c5bdec5b485552b55be67e":["b70042a8a492f7054d480ccdd2be9796510d4327","9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a","f592209545c71895260367152601e9200399776d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["2d5e92410233ff9d02c5bdec5b485552b55be67e"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"9e45ad199e1b1a4bbc15c1c08dcd73dc08fa927a":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}