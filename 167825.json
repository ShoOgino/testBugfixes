{"path":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","commits":[{"id":"3542cf82b8acd1e9ff2ee90fb0bf35e08698a1c2","date":1460953142,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(boolean,double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(boolean small, double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      final Polygon polygon;\n      if (small) {\n        polygon = nextPolygonNear(originLat, originLon);\n      } else {\n        polygon = nextPolygon();\n      }\n      \n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"713122036535651642e6d4fe57ce12b449e18473","date":1461000510,"type":1,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(boolean,double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(boolean small, double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      final Polygon polygon;\n      if (small) {\n        polygon = nextPolygonNear(originLat, originLon);\n      } else {\n        polygon = nextPolygon();\n      }\n      \n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"925fce43f3bac029d912b9b7de03c789e258a333","date":1461612714,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":["95ddcac9ac08381985af6eac8f8b77ba881d4ad8"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f7f901826e47f75f810d7aae24b0455d21ea1fe2","date":1461678892,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = polygon.contains(lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6652c74b2358a0b13223817a6a793bf1c9d0749d","date":1474465301,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"17e5da53e4e5bd659e22add9bba1cfa222e7e30d","date":1475435902,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      for(int docID=0;docID<maxDoc;docID++) {\n        int id = (int) docIDToID.get(docID);\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9fc47cb7b4346802411bb432f501ed0673d7119e","date":1512640179,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"417142ff08fda9cf0b72d5133e63097a166c6458","date":1512729693,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public boolean needsScores() {\n            return false;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"04e775de416dd2d8067b10db1c8af975a1d5017e","date":1539906554,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiBits.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiFields.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2ffe681f212e5073c69955b4ad22946794c84940","date":1560182863,"type":3,"author":"Koen De Groote","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/geo/BaseGeoPointTestCase#verifyRandomPolygons(double[],double[]).mjava","sourceNew":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiBits.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\").append(id).append(\" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\").append(id).append(\" should not match but did\\n\");\n          }\n          b.append(\"  query=\").append(query).append(\" docID=\").append(docID).append(\"\\n\");\n          b.append(\"  lat=\").append(lats[id]).append(\" lon=\").append(lons[id]).append(\"\\n\");\n          b.append(\"  deleted?=\").append(liveDocs != null && liveDocs.get(docID) == false);\n          b.append(\"  polygon=\").append(polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","sourceOld":"  protected void verifyRandomPolygons(double[] lats, double[] lons) throws Exception {\n    IndexWriterConfig iwc = newIndexWriterConfig();\n    // Else seeds may not reproduce:\n    iwc.setMergeScheduler(new SerialMergeScheduler());\n    // Else we can get O(N^2) merging:\n    int mbd = iwc.getMaxBufferedDocs();\n    if (mbd != -1 && mbd < lats.length/100) {\n      iwc.setMaxBufferedDocs(lats.length/100);\n    }\n    Directory dir;\n    if (lats.length > 100000) {\n      dir = newFSDirectory(createTempDir(getClass().getSimpleName()));\n    } else {\n      dir = newDirectory();\n    }\n\n    Set<Integer> deleted = new HashSet<>();\n    // RandomIndexWriter is too slow here:\n    IndexWriter w = new IndexWriter(dir, iwc);\n    for(int id=0;id<lats.length;id++) {\n      Document doc = new Document();\n      doc.add(newStringField(\"id\", \"\"+id, Field.Store.NO));\n      doc.add(new NumericDocValuesField(\"id\", id));\n      if (Double.isNaN(lats[id]) == false) {\n        addPointToDoc(FIELD_NAME, doc, lats[id], lons[id]);\n      }\n      w.addDocument(doc);\n      if (id > 0 && random().nextInt(100) == 42) {\n        int idToDelete = random().nextInt(id);\n        w.deleteDocuments(new Term(\"id\", \"\"+idToDelete));\n        deleted.add(idToDelete);\n        if (VERBOSE) {\n          System.out.println(\"  delete id=\" + idToDelete);\n        }\n      }\n    }\n\n    if (random().nextBoolean()) {\n      w.forceMerge(1);\n    }\n    final IndexReader r = DirectoryReader.open(w);\n    w.close();\n\n    // We can't wrap with \"exotic\" readers because points needs to work:\n    IndexSearcher s = newSearcher(r);\n\n    final int iters = atLeast(75);\n\n    Bits liveDocs = MultiBits.getLiveDocs(s.getIndexReader());\n    int maxDoc = s.getIndexReader().maxDoc();\n\n    for (int iter=0;iter<iters;iter++) {\n\n      if (VERBOSE) {\n        System.out.println(\"\\nTEST: iter=\" + iter + \" s=\" + s);\n      }\n\n      // Polygon\n      Polygon polygon = nextPolygon();\n      Query query = newPolygonQuery(FIELD_NAME, polygon);\n\n      if (VERBOSE) {\n        System.out.println(\"  query=\" + query);\n      }\n\n      final FixedBitSet hits = new FixedBitSet(maxDoc);\n      s.search(query, new SimpleCollector() {\n\n          private int docBase;\n\n          @Override\n          public ScoreMode scoreMode() {\n            return ScoreMode.COMPLETE_NO_SCORES;\n          }\n\n          @Override\n          protected void doSetNextReader(LeafReaderContext context) throws IOException {\n            docBase = context.docBase;\n          }\n\n          @Override\n          public void collect(int doc) {\n            hits.set(docBase+doc);\n          }\n        });\n\n      boolean fail = false;\n      NumericDocValues docIDToID = MultiDocValues.getNumericValues(r, \"id\");\n      for(int docID=0;docID<maxDoc;docID++) {\n        assertEquals(docID, docIDToID.nextDoc());\n        int id = (int) docIDToID.longValue();\n        boolean expected;\n        if (liveDocs != null && liveDocs.get(docID) == false) {\n          // document is deleted\n          expected = false;\n        } else if (Double.isNaN(lats[id])) {\n          expected = false;\n        } else {\n          expected = GeoTestUtil.containsSlowly(polygon, lats[id], lons[id]);\n        }\n\n        if (hits.get(docID) != expected) {\n          StringBuilder b = new StringBuilder();\n\n          if (expected) {\n            b.append(\"FAIL: id=\" + id + \" should match but did not\\n\");\n          } else {\n            b.append(\"FAIL: id=\" + id + \" should not match but did\\n\");\n          }\n          b.append(\"  query=\" + query + \" docID=\" + docID + \"\\n\");\n          b.append(\"  lat=\" + lats[id] + \" lon=\" + lons[id] + \"\\n\");\n          b.append(\"  deleted?=\" + (liveDocs != null && liveDocs.get(docID) == false));\n          b.append(\"  polygon=\" + polygon);\n          if (true) {\n            fail(\"wrong hit (first of possibly more):\\n\\n\" + b);\n          } else {\n            System.out.println(b.toString());\n            fail = true;\n          }\n        }\n      }\n      if (fail) {\n        fail(\"some hits were wrong\");\n      }\n    }\n\n    IOUtils.close(r, dir);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"2ffe681f212e5073c69955b4ad22946794c84940":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"f7f901826e47f75f810d7aae24b0455d21ea1fe2":["713122036535651642e6d4fe57ce12b449e18473","925fce43f3bac029d912b9b7de03c789e258a333"],"3542cf82b8acd1e9ff2ee90fb0bf35e08698a1c2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["f7f901826e47f75f810d7aae24b0455d21ea1fe2","17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"925fce43f3bac029d912b9b7de03c789e258a333":["713122036535651642e6d4fe57ce12b449e18473"],"417142ff08fda9cf0b72d5133e63097a166c6458":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d","9fc47cb7b4346802411bb432f501ed0673d7119e"],"713122036535651642e6d4fe57ce12b449e18473":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","3542cf82b8acd1e9ff2ee90fb0bf35e08698a1c2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["f7f901826e47f75f810d7aae24b0455d21ea1fe2","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["f7f901826e47f75f810d7aae24b0455d21ea1fe2"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["2ffe681f212e5073c69955b4ad22946794c84940"]},"commit2Childs":{"2ffe681f212e5073c69955b4ad22946794c84940":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"f7f901826e47f75f810d7aae24b0455d21ea1fe2":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","17e5da53e4e5bd659e22add9bba1cfa222e7e30d","6652c74b2358a0b13223817a6a793bf1c9d0749d"],"3542cf82b8acd1e9ff2ee90fb0bf35e08698a1c2":["713122036535651642e6d4fe57ce12b449e18473"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"925fce43f3bac029d912b9b7de03c789e258a333":["f7f901826e47f75f810d7aae24b0455d21ea1fe2"],"417142ff08fda9cf0b72d5133e63097a166c6458":["04e775de416dd2d8067b10db1c8af975a1d5017e"],"713122036535651642e6d4fe57ce12b449e18473":["f7f901826e47f75f810d7aae24b0455d21ea1fe2","925fce43f3bac029d912b9b7de03c789e258a333"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3542cf82b8acd1e9ff2ee90fb0bf35e08698a1c2","713122036535651642e6d4fe57ce12b449e18473"],"17e5da53e4e5bd659e22add9bba1cfa222e7e30d":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","417142ff08fda9cf0b72d5133e63097a166c6458","9fc47cb7b4346802411bb432f501ed0673d7119e"],"6652c74b2358a0b13223817a6a793bf1c9d0749d":["17e5da53e4e5bd659e22add9bba1cfa222e7e30d"],"04e775de416dd2d8067b10db1c8af975a1d5017e":["2ffe681f212e5073c69955b4ad22946794c84940"],"9fc47cb7b4346802411bb432f501ed0673d7119e":["417142ff08fda9cf0b72d5133e63097a166c6458"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}