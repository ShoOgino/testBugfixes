{"path":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","commits":[{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"/dev/null","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = _TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = _TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = _TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","date":1389274049,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = _TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = _TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = _TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = _TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = _TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = _TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = _TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = _TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = _TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"379db3ad24c4f0214f30a122265a6d6be003a99d","date":1407537768,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(TEST_VERSION_CURRENT, tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8a255765a5625ff80fba75863de5a16ea392015e","date":1528161860,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    for (int i = 0; i < 20; i++) {\n      final String s = TestUtil.randomUnicodeString(random(), 10);\n      final int codePointCount = s.codePointCount(0, s.length());\n      final int minGram = TestUtil.nextInt(random(), 1, 3);\n      final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n      final boolean preserveOriginal = TestUtil.nextInt(random(), 0, 1) % 2 == 0;\n\n      TokenStream tk = new KeywordTokenizer();\n      ((Tokenizer)tk).setReader(new StringReader(s));\n      tk = new NGramTokenFilter(tk, minGram, maxGram, preserveOriginal);\n      final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n      tk.reset();\n\n      if (codePointCount < minGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      for (int start = 0; start < codePointCount; ++start) {\n        for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n          assertTrue(tk.incrementToken());\n          assertEquals(0, offsetAtt.startOffset());\n          assertEquals(s.length(), offsetAtt.endOffset());\n          final int startIndex = Character.offsetByCodePoints(s, 0, start);\n          final int endIndex = Character.offsetByCodePoints(s, 0, end);\n          assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n        }\n      }\n      \n      if (codePointCount > maxGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      assertFalse(tk.incrementToken());\n      tk.close();\n    }\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":["379db3ad24c4f0214f30a122265a6d6be003a99d","bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","ae889fd5c8a69f6b5d130d3c895bfa5b04d07338","6613659748fe4411a7dcf85266e55db1f95f7315"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    for (int i = 0; i < 20; i++) {\n      final String s = TestUtil.randomUnicodeString(random(), 10);\n      final int codePointCount = s.codePointCount(0, s.length());\n      final int minGram = TestUtil.nextInt(random(), 1, 3);\n      final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n      final boolean preserveOriginal = TestUtil.nextInt(random(), 0, 1) % 2 == 0;\n\n      TokenStream tk = new KeywordTokenizer();\n      ((Tokenizer)tk).setReader(new StringReader(s));\n      tk = new NGramTokenFilter(tk, minGram, maxGram, preserveOriginal);\n      final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n      tk.reset();\n\n      if (codePointCount < minGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      for (int start = 0; start < codePointCount; ++start) {\n        for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n          assertTrue(tk.incrementToken());\n          assertEquals(0, offsetAtt.startOffset());\n          assertEquals(s.length(), offsetAtt.endOffset());\n          final int startIndex = Character.offsetByCodePoints(s, 0, start);\n          final int endIndex = Character.offsetByCodePoints(s, 0, end);\n          assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n        }\n      }\n      \n      if (codePointCount > maxGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      assertFalse(tk.incrementToken());\n      tk.close();\n    }\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","pathOld":"lucene/analysis/common/src/test/org/apache/lucene/analysis/ngram/NGramTokenFilterTest#testSupplementaryCharacters().mjava","sourceNew":"  public void testSupplementaryCharacters() throws IOException {\n    for (int i = 0; i < 20; i++) {\n      final String s = TestUtil.randomUnicodeString(random(), 10);\n      final int codePointCount = s.codePointCount(0, s.length());\n      final int minGram = TestUtil.nextInt(random(), 1, 3);\n      final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n      final boolean preserveOriginal = TestUtil.nextInt(random(), 0, 1) % 2 == 0;\n\n      TokenStream tk = new KeywordTokenizer();\n      ((Tokenizer)tk).setReader(new StringReader(s));\n      tk = new NGramTokenFilter(tk, minGram, maxGram, preserveOriginal);\n      final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n      final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n      tk.reset();\n\n      if (codePointCount < minGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      for (int start = 0; start < codePointCount; ++start) {\n        for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n          assertTrue(tk.incrementToken());\n          assertEquals(0, offsetAtt.startOffset());\n          assertEquals(s.length(), offsetAtt.endOffset());\n          final int startIndex = Character.offsetByCodePoints(s, 0, start);\n          final int endIndex = Character.offsetByCodePoints(s, 0, end);\n          assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n        }\n      }\n      \n      if (codePointCount > maxGram && preserveOriginal) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        assertEquals(s, termAtt.toString());\n      }\n      \n      assertFalse(tk.incrementToken());\n      tk.close();\n    }\n  }\n\n","sourceOld":"  public void testSupplementaryCharacters() throws IOException {\n    final String s = TestUtil.randomUnicodeString(random(), 10);\n    final int codePointCount = s.codePointCount(0, s.length());\n    final int minGram = TestUtil.nextInt(random(), 1, 3);\n    final int maxGram = TestUtil.nextInt(random(), minGram, 10);\n    TokenStream tk = new KeywordTokenizer();\n    ((Tokenizer)tk).setReader(new StringReader(s));\n    tk = new NGramTokenFilter(tk, minGram, maxGram);\n    final CharTermAttribute termAtt = tk.addAttribute(CharTermAttribute.class);\n    final OffsetAttribute offsetAtt = tk.addAttribute(OffsetAttribute.class);\n    tk.reset();\n    for (int start = 0; start < codePointCount; ++start) {\n      for (int end = start + minGram; end <= Math.min(codePointCount, start + maxGram); ++end) {\n        assertTrue(tk.incrementToken());\n        assertEquals(0, offsetAtt.startOffset());\n        assertEquals(s.length(), offsetAtt.endOffset());\n        final int startIndex = Character.offsetByCodePoints(s, 0, start);\n        final int endIndex = Character.offsetByCodePoints(s, 0, end);\n        assertEquals(s.substring(startIndex, endIndex), termAtt.toString());\n      }\n    }\n    assertFalse(tk.incrementToken());\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"8a255765a5625ff80fba75863de5a16ea392015e":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"6613659748fe4411a7dcf85266e55db1f95f7315":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["6613659748fe4411a7dcf85266e55db1f95f7315"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["379db3ad24c4f0214f30a122265a6d6be003a99d","8a255765a5625ff80fba75863de5a16ea392015e"],"f592209545c71895260367152601e9200399776d":["379db3ad24c4f0214f30a122265a6d6be003a99d","8a255765a5625ff80fba75863de5a16ea392015e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a255765a5625ff80fba75863de5a16ea392015e"]},"commit2Childs":{"8a255765a5625ff80fba75863de5a16ea392015e":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["ae889fd5c8a69f6b5d130d3c895bfa5b04d07338"],"ae889fd5c8a69f6b5d130d3c895bfa5b04d07338":["6613659748fe4411a7dcf85266e55db1f95f7315"],"6613659748fe4411a7dcf85266e55db1f95f7315":["379db3ad24c4f0214f30a122265a6d6be003a99d"],"379db3ad24c4f0214f30a122265a6d6be003a99d":["8a255765a5625ff80fba75863de5a16ea392015e","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"f592209545c71895260367152601e9200399776d":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}