{"path":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","commits":[{"id":"c2c3a504730329ae644b009dee43024116605d47","date":1345253449,"type":0,"author":"Jan HÃ¸ydahl","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","pathOld":"/dev/null","sourceNew":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"70fa1c0f4d75735ff2e1485e059d9bc5efa50598","date":1345296911,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","pathOld":"/dev/null","sourceNew":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"616c1830142ff5c1ddedec1ed898733b73c8e23b","date":1345368925,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","pathOld":"/dev/null","sourceNew":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","sourceNew":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","sourceOld":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<String>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15","date":1554259533,"type":3,"author":"Gus Heck","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","pathOld":"solr/core/src/java/org/apache/solr/util/SimplePostTool.PageFetcher#isDisallowedByRobots(URL).mjava","sourceNew":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<>();\n        URL urlRobot;\n        try {\n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n\n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","sourceOld":"    public boolean isDisallowedByRobots(URL url) {\n      String host = url.getHost();\n      String strRobot = url.getProtocol() + \"://\" + host + \"/robots.txt\";\n      List<String> disallows = robotsCache.get(host);\n      if(disallows == null) {\n        disallows = new ArrayList<>();\n        URL urlRobot;\n        try { \n          urlRobot = new URL(strRobot);\n          disallows = parseRobotsTxt(urlRobot.openStream());\n        } catch (MalformedURLException e) {\n          return true; // We cannot trust this robots URL, should not happen\n        } catch (IOException e) {\n          // There is no robots.txt, will cache an empty disallow list\n        }\n      }\n      \n      robotsCache.put(host, disallows);\n\n      String strURL = url.getFile();\n      for (String path : disallows) {\n        if (path.equals(\"/\") || strURL.indexOf(path) == 0)\n          return true;\n      }\n      return false;\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["c2c3a504730329ae644b009dee43024116605d47"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","c2c3a504730329ae644b009dee43024116605d47"],"c2c3a504730329ae644b009dee43024116605d47":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15"],"616c1830142ff5c1ddedec1ed898733b73c8e23b":[],"70fa1c0f4d75735ff2e1485e059d9bc5efa50598":[],"c2c3a504730329ae644b009dee43024116605d47":["634f330c54fd3f9f491d52036dc3f40b4f4d8934","616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","c2c3a504730329ae644b009dee43024116605d47"],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["616c1830142ff5c1ddedec1ed898733b73c8e23b","70fa1c0f4d75735ff2e1485e059d9bc5efa50598","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}