{"path":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["953de31d76c9d58f1e3f4e41ff8a48a1529226de"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random);\n\n    RandomIndexWriter w  = new RandomIndexWriter(random, dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random;\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random, 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newField(\"f\", \"\", TextField.TYPE_UNSTORED);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", term);\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", term);\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", new StringReader(term));\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          TokenStream ts = analyzer.tokenStream(\"ignore\", term);\n          CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n          ts.reset();\n          while(ts.incrementToken()) {\n            String text = termAttr.toString();\n            doc.add(text);\n            sb.append(text).append(' ');\n          }\n          ts.end();\n          ts.close();\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["953de31d76c9d58f1e3f4e41ff8a48a1529226de","c83d6c4335f31cae14f625a222bc842f20073dcd","e2efdd13c0f37dbe4a292a6f98ddcf8e8f872ac4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"6613659748fe4411a7dcf85266e55db1f95f7315","date":1392773913,"type":3,"author":"Benson Margulies","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = _TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = _TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = _TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = _TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["71da933d30aea361ccc224d6544c451cbf49916d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<List<String>>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<String>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.shutdown();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.shutdown();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.shutdown();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.shutdown();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"51f5280f31484820499077f41fcdfe92d527d9dc","date":1423229122,"type":3,"author":"Alan Woodward","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start + \", it=\" + i + \", expected doc \" + docID, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e9e1499c5d26c936238506df90a3c02c76707722","date":1434449920,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery.Builder builder = new PhraseQuery.Builder();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        builder.add(new Term(\"f\", doc.get(t)), t);\n        sb.append(doc.get(t)).append(' ');\n      }\n      PhraseQuery pq = builder.build();\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start + \", it=\" + i + \", expected doc \" + docID, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery pq = new PhraseQuery();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        pq.add(new Term(\"f\", doc.get(t)));\n        sb.append(doc.get(t)).append(' ');\n      }\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start + \", it=\" + i + \", expected doc \" + docID, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"71da933d30aea361ccc224d6544c451cbf49916d","date":1579874339,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/search/TestPhraseQuery#testRandomPhrases().mjava","sourceNew":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // at night, must be > 4096 so it spans multiple chunks\n      int termCount = TEST_NIGHTLY ? atLeast(4097) : atLeast(200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(3);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery.Builder builder = new PhraseQuery.Builder();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        builder.add(new Term(\"f\", doc.get(t)), t);\n        sb.append(doc.get(t)).append(' ');\n      }\n      PhraseQuery pq = builder.build();\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start + \", it=\" + i + \", expected doc \" + docID, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  public void testRandomPhrases() throws Exception {\n    Directory dir = newDirectory();\n    Analyzer analyzer = new MockAnalyzer(random());\n\n    RandomIndexWriter w  = new RandomIndexWriter(random(), dir, newIndexWriterConfig(analyzer).setMergePolicy(newLogMergePolicy()));\n    List<List<String>> docs = new ArrayList<>();\n    Document d = new Document();\n    Field f = newTextField(\"f\", \"\", Field.Store.NO);\n    d.add(f);\n\n    Random r = random();\n\n    int NUM_DOCS = atLeast(10);\n    for (int i = 0; i < NUM_DOCS; i++) {\n      // must be > 4096 so it spans multiple chunks\n      int termCount = TestUtil.nextInt(random(), 4097, 8200);\n\n      List<String> doc = new ArrayList<>();\n\n      StringBuilder sb = new StringBuilder();\n      while(doc.size() < termCount) {\n        if (r.nextInt(5) == 1 || docs.size() == 0) {\n          // make new non-empty-string term\n          String term;\n          while(true) {\n            term = TestUtil.randomUnicodeString(r);\n            if (term.length() > 0) {\n              break;\n            }\n          }\n          try (TokenStream ts = analyzer.tokenStream(\"ignore\", term)) {\n            CharTermAttribute termAttr = ts.addAttribute(CharTermAttribute.class);\n            ts.reset();\n            while(ts.incrementToken()) {\n              String text = termAttr.toString();\n              doc.add(text);\n              sb.append(text).append(' ');\n            }\n            ts.end();\n          }\n        } else {\n          // pick existing sub-phrase\n          List<String> lastDoc = docs.get(r.nextInt(docs.size()));\n          int len = TestUtil.nextInt(r, 1, 10);\n          int start = r.nextInt(lastDoc.size()-len);\n          for(int k=start;k<start+len;k++) {\n            String t = lastDoc.get(k);\n            doc.add(t);\n            sb.append(t).append(' ');\n          }\n        }\n      }\n      docs.add(doc);\n      f.setStringValue(sb.toString());\n      w.addDocument(d);\n    }\n\n    IndexReader reader = w.getReader();\n    IndexSearcher s = newSearcher(reader);\n    w.close();\n\n    // now search\n    int num = atLeast(10);\n    for(int i=0;i<num;i++) {\n      int docID = r.nextInt(docs.size());\n      List<String> doc = docs.get(docID);\n      \n      final int numTerm = TestUtil.nextInt(r, 2, 20);\n      final int start = r.nextInt(doc.size()-numTerm);\n      PhraseQuery.Builder builder = new PhraseQuery.Builder();\n      StringBuilder sb = new StringBuilder();\n      for(int t=start;t<start+numTerm;t++) {\n        builder.add(new Term(\"f\", doc.get(t)), t);\n        sb.append(doc.get(t)).append(' ');\n      }\n      PhraseQuery pq = builder.build();\n\n      TopDocs hits = s.search(pq, NUM_DOCS);\n      boolean found = false;\n      for(int j=0;j<hits.scoreDocs.length;j++) {\n        if (hits.scoreDocs[j].doc == docID) {\n          found = true;\n          break;\n        }\n      }\n\n      assertTrue(\"phrase '\" + sb + \"' not found; start=\" + start + \", it=\" + i + \", expected doc \" + docID, found);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":["953de31d76c9d58f1e3f4e41ff8a48a1529226de","0762b640e0d0d12b6edb96db68986e13145c3484","6613659748fe4411a7dcf85266e55db1f95f7315"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["6613659748fe4411a7dcf85266e55db1f95f7315"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["04f07771a2a7dd3a395700665ed839c3dae2def2","c83d6c4335f31cae14f625a222bc842f20073dcd"],"6613659748fe4411a7dcf85266e55db1f95f7315":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"51f5280f31484820499077f41fcdfe92d527d9dc":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"e9e1499c5d26c936238506df90a3c02c76707722":["51f5280f31484820499077f41fcdfe92d527d9dc"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["71da933d30aea361ccc224d6544c451cbf49916d"],"71da933d30aea361ccc224d6544c451cbf49916d":["e9e1499c5d26c936238506df90a3c02c76707722"]},"commit2Childs":{"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"6613659748fe4411a7dcf85266e55db1f95f7315":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["6613659748fe4411a7dcf85266e55db1f95f7315"],"51f5280f31484820499077f41fcdfe92d527d9dc":["e9e1499c5d26c936238506df90a3c02c76707722"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"e9e1499c5d26c936238506df90a3c02c76707722":["71da933d30aea361ccc224d6544c451cbf49916d"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["51f5280f31484820499077f41fcdfe92d527d9dc"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"71da933d30aea361ccc224d6544c451cbf49916d":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}