{"path":"lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","commits":[{"id":"edcc2c2cbab6bf89ea584169ffb3ca83a31827f9","date":1316963893,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"/dev/null","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final String defaultCodec = CodecProvider.getDefault().getDefaultFieldCodec();\n    if (defaultCodec.equals(\"SimpleText\") || defaultCodec.equals(\"Memory\")) {\n      // no\n      CodecProvider.getDefault().setDefaultFieldCodec(\"Standard\");\n    }\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(IndexReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n        searcher.close();\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n    \n    writer = new IndexWriter(dir, conf);\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    writer.commit();\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    assertFalse(writer.anyNonBulkMerges);\n    doClose();\n    writer.close(false);\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"576017a3864f5d8d12be8dc6b7b47dd9c41cc08d","date":1317143951,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final String defaultCodec = CodecProvider.getDefault().getDefaultFieldCodec();\n    if (defaultCodec.equals(\"SimpleText\") || defaultCodec.equals(\"Memory\")) {\n      // no\n      CodecProvider.getDefault().setDefaultFieldCodec(\"Standard\");\n    }\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(IndexReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n        searcher.close();\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n    \n    writer = new IndexWriter(dir, conf);\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    assertFalse(writer.anyNonBulkMerges);\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final String defaultCodec = CodecProvider.getDefault().getDefaultFieldCodec();\n    if (defaultCodec.equals(\"SimpleText\") || defaultCodec.equals(\"Memory\")) {\n      // no\n      CodecProvider.getDefault().setDefaultFieldCodec(\"Standard\");\n    }\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(IndexReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n        searcher.close();\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n    \n    writer = new IndexWriter(dir, conf);\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    writer.commit();\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    assertFalse(writer.anyNonBulkMerges);\n    doClose();\n    writer.close(false);\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7b91922b55d15444d554721b352861d028eb8278","date":1320421415,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/src/test-framework/java/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","pathOld":"lucene/src/test-framework/org/apache/lucene/index/ThreadedIndexingAndSearchingTestCase#runTest(String).mjava","sourceNew":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(IndexReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n        searcher.close();\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n    \n    writer = new IndexWriter(dir, conf);\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    assertFalse(writer.anyNonBulkMerges);\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","sourceOld":"  public void runTest(String testName) throws Exception {\n\n    failed.set(false);\n    addCount.set(0);\n    delCount.set(0);\n    packCount.set(0);\n\n    final long t0 = System.currentTimeMillis();\n\n    final String defaultCodec = CodecProvider.getDefault().getDefaultFieldCodec();\n    if (defaultCodec.equals(\"SimpleText\") || defaultCodec.equals(\"Memory\")) {\n      // no\n      CodecProvider.getDefault().setDefaultFieldCodec(\"Standard\");\n    }\n\n    final LineFileDocs docs = new LineFileDocs(random);\n    final File tempDir = _TestUtil.getTempDir(testName);\n    dir = newFSDirectory(tempDir);\n    ((MockDirectoryWrapper) dir).setCheckIndexOnClose(false); // don't double-checkIndex, we do it ourselves.\n    final IndexWriterConfig conf = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));\n\n    if (LuceneTestCase.TEST_NIGHTLY) {\n      // newIWConfig makes smallish max seg size, which\n      // results in tons and tons of segments for this test\n      // when run nightly:\n      MergePolicy mp = conf.getMergePolicy();\n      if (mp instanceof TieredMergePolicy) {\n        ((TieredMergePolicy) mp).setMaxMergedSegmentMB(5000.);\n      } else if (mp instanceof LogByteSizeMergePolicy) {\n        ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1000.);\n      } else if (mp instanceof LogMergePolicy) {\n        ((LogMergePolicy) mp).setMaxMergeDocs(100000);\n      }\n    }\n\n    conf.setMergedSegmentWarmer(new IndexWriter.IndexReaderWarmer() {\n      @Override\n      public void warm(IndexReader reader) throws IOException {\n        if (VERBOSE) {\n          System.out.println(\"TEST: now warm merged reader=\" + reader);\n        }\n        final int maxDoc = reader.maxDoc();\n        final Bits liveDocs = reader.getLiveDocs();\n        int sum = 0;\n        final int inc = Math.max(1, maxDoc/50);\n        for(int docID=0;docID<maxDoc;docID += inc) {\n          if (liveDocs == null || liveDocs.get(docID)) {\n            final Document doc = reader.document(docID);\n            sum += doc.getFields().size();\n          }\n        }\n\n        IndexSearcher searcher = newSearcher(reader);\n        sum += searcher.search(new TermQuery(new Term(\"body\", \"united\")), 10).totalHits;\n        searcher.close();\n\n        if (VERBOSE) {\n          System.out.println(\"TEST: warm visited \" + sum + \" fields\");\n        }\n      }\n      });\n    \n    writer = new IndexWriter(dir, conf);\n    if (VERBOSE) {\n      writer.setInfoStream(System.out);\n    }\n    _TestUtil.reduceOpenFiles(writer);\n\n    final ExecutorService es = random.nextBoolean() ? null : Executors.newCachedThreadPool(new NamedThreadFactory(testName));\n\n    doAfterWriter(es);\n\n    final int NUM_INDEX_THREADS = _TestUtil.nextInt(random, 2, 4);\n\n    final int RUN_TIME_SEC = LuceneTestCase.TEST_NIGHTLY ? 300 : RANDOM_MULTIPLIER;\n\n    final Set<String> delIDs = Collections.synchronizedSet(new HashSet<String>());\n    final Set<String> delPackIDs = Collections.synchronizedSet(new HashSet<String>());\n    final List<SubDocs> allSubDocs = Collections.synchronizedList(new ArrayList<SubDocs>());\n\n    final long stopTime = System.currentTimeMillis() + RUN_TIME_SEC*1000;\n\n    final Thread[] indexThreads = launchIndexingThreads(docs, NUM_INDEX_THREADS, stopTime, delIDs, delPackIDs, allSubDocs);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: DONE start indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n\n    // Let index build up a bit\n    Thread.sleep(100);\n\n    doSearching(es, stopTime);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: all searching done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n    \n    for(int thread=0;thread<indexThreads.length;thread++) {\n      indexThreads[thread].join();\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done join indexing threads [\" + (System.currentTimeMillis()-t0) + \" ms]; addCount=\" + addCount + \" delCount=\" + delCount);\n    }\n\n    final IndexSearcher s = getFinalSearcher();\n    if (VERBOSE) {\n      System.out.println(\"TEST: finalSearcher=\" + s);\n    }\n    boolean doFail = false;\n\n    // Verify: make sure delIDs are in fact deleted:\n    for(String id : delIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"doc id=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" hits; first docID=\" + hits.scoreDocs[0].doc);\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure delPackIDs are in fact deleted:\n    for(String id : delPackIDs) {\n      final TopDocs hits = s.search(new TermQuery(new Term(\"packID\", id)), 1);\n      if (hits.totalHits != 0) {\n        System.out.println(\"packID=\" + id + \" is supposed to be deleted, but got \" + hits.totalHits + \" matches\");\n        doFail = true;\n      }\n    }\n\n    // Verify: make sure each group of sub-docs are still in docID order:\n    for(SubDocs subDocs : allSubDocs) {\n      TopDocs hits = s.search(new TermQuery(new Term(\"packID\", subDocs.packID)), 20);\n      if (!subDocs.deleted) {\n        // We sort by relevance but the scores should be identical so sort falls back to by docID:\n        if (hits.totalHits != subDocs.subIDs.size()) {\n          System.out.println(\"packID=\" + subDocs.packID + \": expected \" + subDocs.subIDs.size() + \" hits but got \" + hits.totalHits);\n          doFail = true;\n        } else {\n          int lastDocID = -1;\n          int startDocID = -1;\n          for(ScoreDoc scoreDoc : hits.scoreDocs) {\n            final int docID = scoreDoc.doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            } else {\n              startDocID = docID;\n            }\n            lastDocID = docID;\n            final Document doc = s.doc(docID);\n            assertEquals(subDocs.packID, doc.get(\"packID\"));\n          }\n\n          lastDocID = startDocID - 1;\n          for(String subID : subDocs.subIDs) {\n            hits = s.search(new TermQuery(new Term(\"docid\", subID)), 1);\n            assertEquals(1, hits.totalHits);\n            final int docID = hits.scoreDocs[0].doc;\n            if (lastDocID != -1) {\n              assertEquals(1+lastDocID, docID);\n            }\n            lastDocID = docID;\n          }\n        }\n      } else {\n        // Pack was deleted -- make sure its docs are\n        // deleted.  We can't verify packID is deleted\n        // because we can re-use packID for update:\n        for(String subID : subDocs.subIDs) {\n          assertEquals(0, s.search(new TermQuery(new Term(\"docid\", subID)), 1).totalHits);\n        }\n      }\n    }\n\n    // Verify: make sure all not-deleted docs are in fact\n    // not deleted:\n    final int endID = Integer.parseInt(docs.nextDoc().get(\"docid\"));\n    docs.close();\n\n    for(int id=0;id<endID;id++) {\n      String stringID = \"\"+id;\n      if (!delIDs.contains(stringID)) {\n        final TopDocs hits = s.search(new TermQuery(new Term(\"docid\", stringID)), 1);\n        if (hits.totalHits != 1) {\n          System.out.println(\"doc id=\" + stringID + \" is not supposed to be deleted, but got hitCount=\" + hits.totalHits);\n          doFail = true;\n        }\n      }\n    }\n    assertFalse(doFail);\n    \n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), s.getIndexReader().numDocs());\n    releaseSearcher(s);\n\n    writer.commit();\n\n    assertEquals(\"index=\" + writer.segString() + \" addCount=\" + addCount + \" delCount=\" + delCount, addCount.get() - delCount.get(), writer.numDocs());\n\n    assertFalse(writer.anyNonBulkMerges);\n    doClose();\n    writer.close(false);\n\n    // Cannot shutdown until after writer is closed because\n    // writer has merged segment warmer that uses IS to run\n    // searches, and that IS may be using this es!\n    if (es != null) {\n      es.shutdown();\n      es.awaitTermination(1, TimeUnit.SECONDS);\n    }\n\n    _TestUtil.checkIndex(dir);\n    dir.close();\n    _TestUtil.rmDir(tempDir);\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done [\" + (System.currentTimeMillis()-t0) + \" ms]\");\n    }\n  }\n\n","bugFix":null,"bugIntro":["d19974432be9aed28ee7dca73bdf01d139e763a9","71da933d30aea361ccc224d6544c451cbf49916d"],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"7b91922b55d15444d554721b352861d028eb8278":["576017a3864f5d8d12be8dc6b7b47dd9c41cc08d"],"576017a3864f5d8d12be8dc6b7b47dd9c41cc08d":["edcc2c2cbab6bf89ea584169ffb3ca83a31827f9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"edcc2c2cbab6bf89ea584169ffb3ca83a31827f9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["7b91922b55d15444d554721b352861d028eb8278"]},"commit2Childs":{"7b91922b55d15444d554721b352861d028eb8278":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"576017a3864f5d8d12be8dc6b7b47dd9c41cc08d":["7b91922b55d15444d554721b352861d028eb8278"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["edcc2c2cbab6bf89ea584169ffb3ca83a31827f9"],"edcc2c2cbab6bf89ea584169ffb3ca83a31827f9":["576017a3864f5d8d12be8dc6b7b47dd9c41cc08d"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}