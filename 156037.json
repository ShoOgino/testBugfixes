{"path":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","commits":[{"id":"a4d374b2bebd0d52acaa61038fbf23068620fba7","date":1353240004,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"/dev/null","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit used packed ints like below!\n        final byte[][] values = new byte[valuesIn.getValueCount()][];\n        BytesRef scratch = new BytesRef();\n        for(int ord=0;ord<values.length;ord++) {\n          valuesIn.lookupOrd(ord, scratch);\n          values[ord] = new byte[scratch.length];\n          System.arraycopy(scratch.bytes, scratch.offset, values[ord], 0, scratch.length);\n        }\n\n        final int[] docToOrd = new int[maxDoc];\n        for(int docID=0;docID<maxDoc;docID++) {\n          docToOrd[docID] = valuesIn.getOrd(docID);\n        }\n\n        return new DocTermsIndex() {\n\n          @Override\n          public PackedInts.Reader getDocToOrd() {\n            // nocommit\n            return null;\n          }\n\n          @Override\n          public int numOrd() {\n            return values.length;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return docToOrd[docID];\n          }\n\n          @Override\n          public int size() {\n            return docToOrd.length;\n          }\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef ret) {\n            ret.bytes = values[ord];\n            ret.length = ret.bytes.length;\n            ret.offset = 0;\n            return ret;\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit\n            return null;\n          }\n        };\n\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        bytes.copyUsingLengthPrefix(new BytesRef());\n        int termOrd = 1;\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["b0b3768e97375c7a745c68f0b54710e8bedccc11","b0b3768e97375c7a745c68f0b54710e8bedccc11","b0b3768e97375c7a745c68f0b54710e8bedccc11"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee","date":1353299109,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n\n          @Override\n          public Reader getDocToOrd() {\n            // nocommit: add this to the codec api!\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        bytes.copyUsingLengthPrefix(new BytesRef());\n        int termOrd = 1;\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        // nocommit used packed ints like below!\n        final byte[][] values = new byte[valuesIn.getValueCount()][];\n        BytesRef scratch = new BytesRef();\n        for(int ord=0;ord<values.length;ord++) {\n          valuesIn.lookupOrd(ord, scratch);\n          values[ord] = new byte[scratch.length];\n          System.arraycopy(scratch.bytes, scratch.offset, values[ord], 0, scratch.length);\n        }\n\n        final int[] docToOrd = new int[maxDoc];\n        for(int docID=0;docID<maxDoc;docID++) {\n          docToOrd[docID] = valuesIn.getOrd(docID);\n        }\n\n        return new DocTermsIndex() {\n\n          @Override\n          public PackedInts.Reader getDocToOrd() {\n            // nocommit\n            return null;\n          }\n\n          @Override\n          public int numOrd() {\n            return values.length;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return docToOrd[docID];\n          }\n\n          @Override\n          public int size() {\n            return docToOrd.length;\n          }\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef ret) {\n            ret.bytes = values[ord];\n            ret.length = ret.bytes.length;\n            ret.offset = 0;\n            return ret;\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit\n            return null;\n          }\n        };\n\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        bytes.copyUsingLengthPrefix(new BytesRef());\n        int termOrd = 1;\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"29f7cc7c185412da66c1d0089d9e75da01329a00","date":1353364851,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n\n          @Override\n          public Reader getDocToOrd() {\n            // nocommit: add this to the codec api!\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        bytes.copyUsingLengthPrefix(new BytesRef());\n        int termOrd = 1;\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"854f97cd3613b9579fba83755c80b697e2f3993f","date":1353527621,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        // 0 is reserved for \"unset\"\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9d7e5f3aa5935964617824d1f9b2599ddb334464","date":1353762831,"type":5,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.SortedDocValuesCache#createValue(AtomicReader,CacheKey,boolean).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/search/FieldCacheImpl.DocTermsIndexCache#createValue(AtomicReader,CacheKey,boolean).mjava","sourceNew":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        return valuesIn.newRAMInstance();\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new SortedDocValuesImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","sourceOld":"    @Override\n    protected Object createValue(AtomicReader reader, CacheKey key, boolean setDocsWithField /* ignored */)\n        throws IOException {\n\n      final int maxDoc = reader.maxDoc();\n      SortedDocValues valuesIn = reader.getSortedDocValues(key.field);\n      if (valuesIn != null) {\n        final SortedDocValues ramInstance = valuesIn.newRAMInstance();\n        return new DocTermsIndex() {\n\n          @Override\n          public BytesRef lookup(int ord, BytesRef reuse) {\n            ramInstance.lookupOrd(ord, reuse);\n            return reuse;\n          }\n\n          @Override\n          public int getOrd(int docID) {\n            return ramInstance.getOrd(docID);\n          }\n\n          @Override\n          public int numOrd() {\n            return ramInstance.getValueCount();\n          }\n\n          @Override\n          public int size() {\n            return ramInstance.size();\n          }\n\n          @Override\n          public TermsEnum getTermsEnum() {\n            // nocommit: to the codec api? or can that termsenum just use this thing?\n            return null;\n          }\n        };\n      } else {\n\n        Terms terms = reader.terms(key.field);\n\n        final float acceptableOverheadRatio = ((Float) key.custom).floatValue();\n\n        final PagedBytes bytes = new PagedBytes(15);\n\n        int startBytesBPV;\n        int startTermsBPV;\n        int startNumUniqueTerms;\n\n        final int termCountHardLimit;\n        if (maxDoc == Integer.MAX_VALUE) {\n          termCountHardLimit = Integer.MAX_VALUE;\n        } else {\n          termCountHardLimit = maxDoc+1;\n        }\n\n        // nocommit use Uninvert?\n        if (terms != null) {\n          // Try for coarse estimate for number of bits; this\n          // should be an underestimate most of the time, which\n          // is fine -- GrowableWriter will reallocate as needed\n          long numUniqueTerms = terms.size();\n          if (numUniqueTerms != -1L) {\n            if (numUniqueTerms > termCountHardLimit) {\n              // app is misusing the API (there is more than\n              // one term per doc); in this case we make best\n              // effort to load what we can (see LUCENE-2142)\n              numUniqueTerms = termCountHardLimit;\n            }\n\n            startBytesBPV = PackedInts.bitsRequired(numUniqueTerms*4);\n            startTermsBPV = PackedInts.bitsRequired(numUniqueTerms);\n\n            startNumUniqueTerms = (int) numUniqueTerms;\n          } else {\n            startBytesBPV = 1;\n            startTermsBPV = 1;\n            startNumUniqueTerms = 1;\n          }\n        } else {\n          startBytesBPV = 1;\n          startTermsBPV = 1;\n          startNumUniqueTerms = 1;\n        }\n\n        GrowableWriter termOrdToBytesOffset = new GrowableWriter(startBytesBPV, 1+startNumUniqueTerms, acceptableOverheadRatio);\n        final GrowableWriter docToTermOrd = new GrowableWriter(startTermsBPV, maxDoc, acceptableOverheadRatio);\n\n        int termOrd = 0;\n\n        // nocommit use Uninvert?\n\n        if (terms != null) {\n          final TermsEnum termsEnum = terms.iterator(null);\n          DocsEnum docs = null;\n\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            if (termOrd >= termCountHardLimit) {\n              break;\n            }\n\n            if (termOrd == termOrdToBytesOffset.size()) {\n              // NOTE: this code only runs if the incoming\n              // reader impl doesn't implement\n              // size (which should be uncommon)\n              termOrdToBytesOffset = termOrdToBytesOffset.resize(ArrayUtil.oversize(1+termOrd, 1));\n            }\n            termOrdToBytesOffset.set(termOrd, bytes.copyUsingLengthPrefix(term));\n            docs = termsEnum.docs(null, docs, 0);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocIdSetIterator.NO_MORE_DOCS) {\n                break;\n              }\n              // Store 1+ ord into packed bits\n              docToTermOrd.set(docID, 1+termOrd);\n            }\n            termOrd++;\n          }\n\n          if (termOrdToBytesOffset.size() > termOrd) {\n            termOrdToBytesOffset = termOrdToBytesOffset.resize(termOrd);\n          }\n        }\n\n        // maybe an int-only impl?\n        return new DocTermsIndexImpl(bytes.freeze(true), termOrdToBytesOffset.getMutable(), docToTermOrd.getMutable(), termOrd);\n      }\n    }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee":["a4d374b2bebd0d52acaa61038fbf23068620fba7"],"29f7cc7c185412da66c1d0089d9e75da01329a00":["767bfba15cdbe84dd2e3b841e0429a1b4ef8feee"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"9d7e5f3aa5935964617824d1f9b2599ddb334464":["854f97cd3613b9579fba83755c80b697e2f3993f"],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"854f97cd3613b9579fba83755c80b697e2f3993f":["29f7cc7c185412da66c1d0089d9e75da01329a00"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"767bfba15cdbe84dd2e3b841e0429a1b4ef8feee":["29f7cc7c185412da66c1d0089d9e75da01329a00"],"29f7cc7c185412da66c1d0089d9e75da01329a00":["854f97cd3613b9579fba83755c80b697e2f3993f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["a4d374b2bebd0d52acaa61038fbf23068620fba7","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"9d7e5f3aa5935964617824d1f9b2599ddb334464":[],"a4d374b2bebd0d52acaa61038fbf23068620fba7":["767bfba15cdbe84dd2e3b841e0429a1b4ef8feee"],"854f97cd3613b9579fba83755c80b697e2f3993f":["9d7e5f3aa5935964617824d1f9b2599ddb334464"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9d7e5f3aa5935964617824d1f9b2599ddb334464","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}