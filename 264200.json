{"path":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token,int).mjava","commits":[{"id":"5350389bf83287111f7760b9e3db3af8e3648474","date":1216372812,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token,int).mjava","pathOld":"/dev/null","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(Token token, int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(token, p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(token, p);\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223","date":1227051709,"type":5,"author":"Michael Busch","isMerge":false,"pathNew":"src/java/org/apache/lucene/index/TermsHashPerField#add(int).mjava","pathOld":"src/java/org/apache/lucene/index/TermsHashPerField#add(Token,int).mjava","sourceNew":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(p);\n    }\n  }\n\n","sourceOld":"  // Secondary entry point (for 2nd & subsequent TermsHash),\n  // because token text has already been \"interned\" into\n  // textStart, so we hash by textStart\n  public void add(Token token, int textStart) throws IOException {\n\n    int code = textStart;\n\n    int hashPos = code & postingsHashMask;\n\n    assert !postingsCompacted;\n\n    // Locate RawPostingList in hash\n    p = postingsHash[hashPos];\n\n    if (p != null && p.textStart != textStart) {\n      // Conflict: keep searching different locations in\n      // the hash table.\n      final int inc = ((code>>8)+code)|1;\n      do {\n        code += inc;\n        hashPos = code & postingsHashMask;\n        p = postingsHash[hashPos];\n      } while (p != null && p.textStart != textStart);\n    }\n\n    if (p == null) {\n\n      // First time we are seeing this token since we last\n      // flushed the hash.\n\n      // Refill?\n      if (0 == perThread.freePostingsCount)\n        perThread.morePostings();\n\n      // Pull next free RawPostingList from free list\n      p = perThread.freePostings[--perThread.freePostingsCount];\n      assert p != null;\n\n      p.textStart = textStart;\n          \n      assert postingsHash[hashPos] == null;\n      postingsHash[hashPos] = p;\n      numPostings++;\n\n      if (numPostings == postingsHashHalfSize)\n        rehashPostings(2*postingsHashSize);\n\n      // Init stream slices\n      if (numPostingInt + intPool.intUpto > DocumentsWriter.INT_BLOCK_SIZE)\n        intPool.nextBuffer();\n\n      if (DocumentsWriter.BYTE_BLOCK_SIZE - bytePool.byteUpto < numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE)\n        bytePool.nextBuffer();\n\n      intUptos = intPool.buffer;\n      intUptoStart = intPool.intUpto;\n      intPool.intUpto += streamCount;\n\n      p.intStart = intUptoStart + intPool.intOffset;\n\n      for(int i=0;i<streamCount;i++) {\n        final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);\n        intUptos[intUptoStart+i] = upto + bytePool.byteOffset;\n      }\n      p.byteStart = intUptos[intUptoStart];\n\n      consumer.newTerm(token, p);\n\n    } else {\n      intUptos = intPool.buffers[p.intStart >> DocumentsWriter.INT_BLOCK_SHIFT];\n      intUptoStart = p.intStart & DocumentsWriter.INT_BLOCK_MASK;\n      consumer.addTerm(token, p);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["5350389bf83287111f7760b9e3db3af8e3648474"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5350389bf83287111f7760b9e3db3af8e3648474":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"]},"commit2Childs":{"74a5e7f20b4a444da9df3b2c0f331fa7a1f64223":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["5350389bf83287111f7760b9e3db3af8e3648474"],"5350389bf83287111f7760b9e3db3af8e3648474":["74a5e7f20b4a444da9df3b2c0f331fa7a1f64223"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}