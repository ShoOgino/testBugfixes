{"path":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"modules/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n        }\n      }\n      if (curGramSize <= maxGram) {\n        if (! (curGramSize > curTermLength         // if the remaining input is too short, we can't generate any n-grams\n            || curGramSize > maxGram)) {       // if we have hit the end of our n-gram size range, quit\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n        }\n      }\n      if (curGramSize <= maxGram) {\n        if (! (curGramSize > curTermLength         // if the remaining input is too short, we can't generate any n-grams\n            || curGramSize > maxGram)) {       // if we have hit the end of our n-gram size range, quit\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"57212735dccd8b37c96cd84e4b15fc341aa998fa","date":1362672443,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {\n        if (! (curGramSize > curTermLength         // if the remaining input is too short, we can't generate any n-grams\n            || curGramSize > maxGram)) {       // if we have hit the end of our n-gram size range, quit\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n        }\n      }\n      if (curGramSize <= maxGram) {\n        if (! (curGramSize > curTermLength         // if the remaining input is too short, we can't generate any n-grams\n            || curGramSize > maxGram)) {       // if we have hit the end of our n-gram size range, quit\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["0d8ddc12dae0634654a3f363c0f211dccebc3d7b","8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0d8ddc12dae0634654a3f363c0f211dccebc3d7b","date":1366637163,"type":3,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {\n        if (! (curGramSize > curTermLength         // if the remaining input is too short, we can't generate any n-grams\n            || curGramSize > maxGram)) {       // if we have hit the end of our n-gram size range, quit\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":["57212735dccd8b37c96cd84e4b15fc341aa998fa","9b5756469957918cac40a831acec9cf01c8c2bb3"],"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"523c1863d7ec17e9a5067cef7e233c388f8ab263","date":1367931848,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          if (version.onOrAfter(Version.LUCENE_44)) {\n            // Never update offsets\n            updateOffsets = false;\n          } else {\n            // if length by start + end offsets doesn't match the term text then assume\n            // this is a synonym and don't adjust the offsets.\n            updateOffsets = (tokStart + curTermLength) == tokEnd;\n          }\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (updateOffsets) {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          } else {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          // if length by start + end offsets doesn't match the term text then assume\n          // this is a synonym and don't adjust the offsets.\n          hasIllegalOffsets = (tokStart + curTermLength) != tokEnd;\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (hasIllegalOffsets) {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          } else {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7567347acd9579d742a2ffd4feb1a32062fb1bc3","date":1367935406,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          if (version.onOrAfter(Version.LUCENE_44)) {\n            // Never update offsets\n            updateOffsets = false;\n          } else {\n            // if length by start + end offsets doesn't match the term text then assume\n            // this is a synonym and don't adjust the offsets.\n            updateOffsets = (tokStart + curTermLength) == tokEnd;\n          }\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          int start = side == Side.FRONT ? 0 : curTermLength - curGramSize;\n          int end = start + curGramSize;\n          clearAttributes();\n          if (updateOffsets) {\n            offsetAtt.setOffset(tokStart + start, tokStart + end);\n          } else {\n            offsetAtt.setOffset(tokStart, tokEnd);\n          }\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, start, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa","date":1368868906,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr = posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            //  Leave the first token position increment at the cleared-attribute value of 1\n            if ( ! isFirstToken) {\n              posIncrAtt.setPositionIncrement(savePosIncr);\n            }\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e","bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0d93c50fb8ca87a08f2ee7552da827e1efbe9d3","date":1368869273,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          isFirstToken = false;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","date":1371043069,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = charUtils.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curTermLength) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          termAtt.copyBuffer(curTermBuffer, 0, curGramSize);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc","date":1465824262,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = charUtils.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","date":1465913303,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = charUtils.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = charUtils.codePointCount(termAtt);\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = charUtils.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"df0b5820a21fc2ecd43bc3b3bd557259666381bb","date":1484306067,"type":3,"author":"Nathan Gass","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          attributes = input.cloneAttributes();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          attributes.copyTo(this);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"db27aa85cdcabcc064b2be31c3d3f34eecfd7abc","date":1484319247,"type":3,"author":"Nathan Gass","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n\t      restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          attributes = input.cloneAttributes();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          attributes.copyTo(this);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":["8a255765a5625ff80fba75863de5a16ea392015e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"bbbdd19493fa8ae4bdac9205ae34e7387f08f304","date":1484561803,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":["7567347acd9579d742a2ffd4feb1a32062fb1bc3","226f5e862af9059a60fe80d2b27e547bcd95971c","af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa","0c17d12803da6cadc96b3cdf15b0b940eddb28de","360d15dc189fb48153cb62234f7d20819e4e292e"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"507e7decdf00981d09a74632ea30299a4ce6ba72","date":1484600874,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          tokStart = offsetAtt.startOffset();\n          tokEnd = offsetAtt.endOffset();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n          savePosLen = posLenAtt.getPositionLength();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          clearAttributes();\n          offsetAtt.setOffset(tokStart, tokEnd);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          posLenAtt.setPositionLength(savePosLen);\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a255765a5625ff80fba75863de5a16ea392015e","date":1528161860,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        }\n        state = captureState();\n        \n        curTermLength = termAtt.length();\n        curTermCodePointCount = Character.codePointCount(termAtt, 0, curTermLength);\n        curPosIncr += posIncrAtt.getPositionIncrement();\n\n        if (preserveOriginal && curTermCodePointCount < minGram) {\n          // Token is shorter than minGram, but we'd still like to keep it.\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n          return true;\n        }\n        \n        curTermBuffer = termAtt.buffer().clone();\n        curGramSize = minGram;\n      }\n\n      if (curGramSize <= curTermCodePointCount) {\n        if (curGramSize <= maxGram) { // curGramSize is between minGram and maxGram\n          restoreState(state);\n          // first ngram gets increment, others don't\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n        else if (preserveOriginal) {\n          // Token is longer than maxGram, but we'd still like to keep it.\n          restoreState(state);\n          posIncrAtt.setPositionIncrement(0);\n          termAtt.copyBuffer(curTermBuffer, 0, curTermLength);\n          curTermBuffer = null;\n          return true;\n        }\n      }\n      // Done with this input token, get next token on the next iteration.\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":["af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa","0d8ddc12dae0634654a3f363c0f211dccebc3d7b","bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","a7347509fad0711ac30cb15a746e9a3830a38ebd","db27aa85cdcabcc064b2be31c3d3f34eecfd7abc","57212735dccd8b37c96cd84e4b15fc341aa998fa","9b5756469957918cac40a831acec9cf01c8c2bb3","fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        }\n        state = captureState();\n        \n        curTermLength = termAtt.length();\n        curTermCodePointCount = Character.codePointCount(termAtt, 0, curTermLength);\n        curPosIncr += posIncrAtt.getPositionIncrement();\n\n        if (preserveOriginal && curTermCodePointCount < minGram) {\n          // Token is shorter than minGram, but we'd still like to keep it.\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n          return true;\n        }\n        \n        curTermBuffer = termAtt.buffer().clone();\n        curGramSize = minGram;\n      }\n\n      if (curGramSize <= curTermCodePointCount) {\n        if (curGramSize <= maxGram) { // curGramSize is between minGram and maxGram\n          restoreState(state);\n          // first ngram gets increment, others don't\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n        else if (preserveOriginal) {\n          // Token is longer than maxGram, but we'd still like to keep it.\n          restoreState(state);\n          posIncrAtt.setPositionIncrement(0);\n          termAtt.copyBuffer(curTermBuffer, 0, curTermLength);\n          curTermBuffer = null;\n          return true;\n        }\n      }\n      // Done with this input token, get next token on the next iteration.\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","pathOld":"lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter#incrementToken().mjava","sourceNew":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        }\n        state = captureState();\n        \n        curTermLength = termAtt.length();\n        curTermCodePointCount = Character.codePointCount(termAtt, 0, curTermLength);\n        curPosIncr += posIncrAtt.getPositionIncrement();\n\n        if (preserveOriginal && curTermCodePointCount < minGram) {\n          // Token is shorter than minGram, but we'd still like to keep it.\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n          return true;\n        }\n        \n        curTermBuffer = termAtt.buffer().clone();\n        curGramSize = minGram;\n      }\n\n      if (curGramSize <= curTermCodePointCount) {\n        if (curGramSize <= maxGram) { // curGramSize is between minGram and maxGram\n          restoreState(state);\n          // first ngram gets increment, others don't\n          posIncrAtt.setPositionIncrement(curPosIncr);\n          curPosIncr = 0;\n\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n        else if (preserveOriginal) {\n          // Token is longer than maxGram, but we'd still like to keep it.\n          restoreState(state);\n          posIncrAtt.setPositionIncrement(0);\n          termAtt.copyBuffer(curTermBuffer, 0, curTermLength);\n          curTermBuffer = null;\n          return true;\n        }\n      }\n      // Done with this input token, get next token on the next iteration.\n      curTermBuffer = null;\n    }\n  }\n\n","sourceOld":"  @Override\n  public final boolean incrementToken() throws IOException {\n    while (true) {\n      if (curTermBuffer == null) {\n        if (!input.incrementToken()) {\n          return false;\n        } else {\n          curTermBuffer = termAtt.buffer().clone();\n          curTermLength = termAtt.length();\n          curCodePointCount = Character.codePointCount(termAtt, 0, termAtt.length());\n          curGramSize = minGram;\n          state = captureState();\n          savePosIncr += posIncrAtt.getPositionIncrement();\n        }\n      }\n      if (curGramSize <= maxGram) {         // if we have hit the end of our n-gram size range, quit\n        if (curGramSize <= curCodePointCount) { // if the remaining input is too short, we can't generate any n-grams\n          // grab gramSize chars from front or back\n          restoreState(state);\n          // first ngram gets increment, others don't\n          if (curGramSize == minGram) {\n            posIncrAtt.setPositionIncrement(savePosIncr);\n            savePosIncr = 0;\n          } else {\n            posIncrAtt.setPositionIncrement(0);\n          }\n          final int charLength = Character.offsetByCodePoints(curTermBuffer, 0, curTermLength, 0, curGramSize);\n          termAtt.copyBuffer(curTermBuffer, 0, charLength);\n          curGramSize++;\n          return true;\n        }\n      }\n      curTermBuffer = null;\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"523c1863d7ec17e9a5067cef7e233c388f8ab263":["0d8ddc12dae0634654a3f363c0f211dccebc3d7b"],"8a255765a5625ff80fba75863de5a16ea392015e":["bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa":["7567347acd9579d742a2ffd4feb1a32062fb1bc3"],"507e7decdf00981d09a74632ea30299a4ce6ba72":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"7567347acd9579d742a2ffd4feb1a32062fb1bc3":["523c1863d7ec17e9a5067cef7e233c388f8ab263"],"db27aa85cdcabcc064b2be31c3d3f34eecfd7abc":["df0b5820a21fc2ecd43bc3b3bd557259666381bb"],"0d8ddc12dae0634654a3f363c0f211dccebc3d7b":["57212735dccd8b37c96cd84e4b15fc341aa998fa"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["a0d93c50fb8ca87a08f2ee7552da827e1efbe9d3"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["bbbdd19493fa8ae4bdac9205ae34e7387f08f304","8a255765a5625ff80fba75863de5a16ea392015e"],"df0b5820a21fc2ecd43bc3b3bd557259666381bb":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704","fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc"],"f592209545c71895260367152601e9200399776d":["bbbdd19493fa8ae4bdac9205ae34e7387f08f304","8a255765a5625ff80fba75863de5a16ea392015e"],"a0d93c50fb8ca87a08f2ee7552da827e1efbe9d3":["af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa"],"57212735dccd8b37c96cd84e4b15fc341aa998fa":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"bbbdd19493fa8ae4bdac9205ae34e7387f08f304":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79","db27aa85cdcabcc064b2be31c3d3f34eecfd7abc"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["8a255765a5625ff80fba75863de5a16ea392015e"]},"commit2Childs":{"523c1863d7ec17e9a5067cef7e233c388f8ab263":["7567347acd9579d742a2ffd4feb1a32062fb1bc3"],"8a255765a5625ff80fba75863de5a16ea392015e":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc":["57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa":["a0d93c50fb8ca87a08f2ee7552da827e1efbe9d3"],"507e7decdf00981d09a74632ea30299a4ce6ba72":[],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["507e7decdf00981d09a74632ea30299a4ce6ba72"],"7567347acd9579d742a2ffd4feb1a32062fb1bc3":["af2d9657da7a76eceb8bd3a02fb13fbfcfbf83fa"],"db27aa85cdcabcc064b2be31c3d3f34eecfd7abc":["bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"0d8ddc12dae0634654a3f363c0f211dccebc3d7b":["523c1863d7ec17e9a5067cef7e233c388f8ab263"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["57212735dccd8b37c96cd84e4b15fc341aa998fa"],"bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704":["fba4cd6a5aae0bc17cc1cbf1e84b0ae32ee039dc","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","57dc82c7b33cd580e7ab5179019bc78f3d7f8e79"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"df0b5820a21fc2ecd43bc3b3bd557259666381bb":["db27aa85cdcabcc064b2be31c3d3f34eecfd7abc"],"57dc82c7b33cd580e7ab5179019bc78f3d7f8e79":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","df0b5820a21fc2ecd43bc3b3bd557259666381bb","bbbdd19493fa8ae4bdac9205ae34e7387f08f304"],"f592209545c71895260367152601e9200399776d":[],"bbbdd19493fa8ae4bdac9205ae34e7387f08f304":["8a255765a5625ff80fba75863de5a16ea392015e","507e7decdf00981d09a74632ea30299a4ce6ba72","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"57212735dccd8b37c96cd84e4b15fc341aa998fa":["0d8ddc12dae0634654a3f363c0f211dccebc3d7b"],"a0d93c50fb8ca87a08f2ee7552da827e1efbe9d3":["bd095de1c7ac6b6ab3a330b5fbe8cb37e4f34704"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["507e7decdf00981d09a74632ea30299a4ce6ba72","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}