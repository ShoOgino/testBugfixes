{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","commits":[{"id":"505bff044e47a553f461b6f4484d1d08faf4ac85","date":1420728783,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(LeafReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(LeafReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(LeafReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (LeafReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7bf95be1fe06a3535aa13b9fc2ce7ebac0eae6db","date":1420822089,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(mergeDirectory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3c5705cb93fb3daa46c676cad08b916dd57bf1be","date":1422473298,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        try {\n          createCompoundFile(infoStream, mergeDirectory, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98b44240f64a2d6935543ff25faee750b29204eb","date":1424972040,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":["c48871ed951104729f5e17a8ee1091b43fa18980"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2","date":1424979404,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"98a04f56464afdffd4c430d6c47a0c868a38354e","date":1424985833,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":["5faf65b6692f15cca0f87bf8666c87899afc619f","b6f9be74ca7baaef11857ad002cad40419979516","949847c0040cd70a68222d526cb0da7bf6cbb3c2","d6b7c6630218ed9693cdb8643276513f9f0043f4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"79700663e164dece87bed4adfd3e28bab6cb1385","date":1425241849,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"299a2348fa24151d150182211b6208a38e5e3450","date":1425304608,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n    int numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n\n      // Make sure adding the new documents to this index won't\n      // exceed the limit:\n      reserveDocs(numDocs);\n      \n      final IOContext context = new IOContext(new MergeInfo(numDocs, -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, null, StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(IndexReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"618635065f043788c9e034f96ca5cd5cea1b4592","date":1433442044,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directory, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"950882a2bd2a5f9dc16a154871584eaa643d882a","date":1436366563,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          deleter.deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          synchronized(this) {\n            deleter.deleteNewFiles(filesToDelete);\n          }\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          deleter.deleteNewFiles(info.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"aac05884852c2a15a6aa9153063de70dea4fbcae","date":1441829939,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      boolean success = false;\n      try {\n        merger.merge();                // merge 'em\n        success = true;\n      } finally {\n        if (!success) { \n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      success = false;\n      try {\n        codec.segmentInfoFormat().write(trackingDir, info, context);\n        success = true;\n      } finally {\n        if (!success) {\n          synchronized(this) {\n            deleter.refresh(info.name);\n          }\n        }\n      }\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"867e3d9153fb761456b54a9dcce566e1545c5ef6","date":1444903098,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.getCreatedFiles().clear();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c48871ed951104729f5e17a8ee1091b43fa18980","date":1446564542,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (OutOfMemoryError oom) {\n      tragicEvent(oom, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":["01deb9e9fb9dbd5fddce32a5fcd952bbb611fe63","98b44240f64a2d6935543ff25faee750b29204eb"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  // nocommit make sure if you add \"sorted by X\" to \"sorted by Y\" index, we catch it\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5e03940e6e9044943de4b7ac08f8581da37a9534","date":1462870173,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  // nocommit make sure if you add \"sorted by X\" to \"sorted by Y\" index, we catch it\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0f6df47cbfd656ea50ca2996361f7954531ee18b","date":1464133540,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        // no need to increment:\n        return docWriter.deleteQueue.seqNo.get();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          // no need to increment:\n          return docWriter.deleteQueue.seqNo.get();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          // no need to increment:\n          return docWriter.deleteQueue.seqNo.get();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n\n    // no need to increment:\n    return docWriter.deleteQueue.seqNo.get();\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"16ebfabc294f23b88b6a39722a02c9d39b353195","date":1464343867,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        // no need to increment:\n        return docWriter.deleteQueue.seqNo.get();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          // no need to increment:\n          return docWriter.deleteQueue.seqNo.get();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          // no need to increment:\n          return docWriter.deleteQueue.seqNo.get();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n\n    // no need to increment:\n    return docWriter.deleteQueue.seqNo.get();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6483e4260c08168709c02238ae083a51519a28dd","date":1465117546,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"191128ac5b85671b1671e2c857437694283b6ebf","date":1465297861,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public void addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return;\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n          return;\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n    }\n    maybeMerge();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c","date":1477166077,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"80d0e6d59ae23f4a6f30eaf40bfb40742300287f","date":1477598926,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, -1));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1ee9437ba5a8297220428d48a6bb823d1fcd57b","date":1489137809,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this method merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n      \n      rateLimiters.set(new MergeRateLimiter(null));\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        Sort leafIndexSort = leaf.getIndexSort();\n        if (indexSort != null && leafIndexSort != null && indexSort.equals(leafIndexSort) == false) {\n          throw new IllegalArgumentException(\"cannot change index sort from \" + leafIndexSort + \" to \" + indexSort);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"845b760a99e5f369fcd0a5d723a87b8def6a3f56","date":1521117993,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      // dead code but javac disagrees:\n      seqNo = -1;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n\n    Sort indexSort = config.getIndexSort();\n\n    long seqNo;\n\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"68ba24d6f9380e2463dbe5130d27502647f64904","date":1554881362,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), new HashMap<>(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92b74780b4efed2011d2d1a19183689db904519e","date":1586516102,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      merger.merge();                // merge 'em\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b07024a7318c25225dc4d070cf6d047315b73aaf","date":1586885963,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.deleteQueue.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.deleteQueue.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.deleteQueue.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"14654be3f7a82c9a3c52169e365baa55bfe64f66","date":1587212697,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L, StringHelper.randomId());\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L);\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      Codec codec = config.getCodec();\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L, StringHelper.randomId());\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L, StringHelper.randomId());\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a775b68a26e2d19d1b5f16cd18a3bc8df738a302","date":1598253342,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#addIndexes(CodecReader...).mjava","sourceNew":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      Codec codec = config.getCodec();\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert merges.areEnabled();\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L, StringHelper.randomId());\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (merges.areEnabled() == false) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (merges.areEnabled() == false) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","sourceOld":"  /**\n   * Merges the provided indexes into this index.\n   * \n   * <p>\n   * The provided IndexReaders are not closed.\n   * \n   * <p>\n   * See {@link #addIndexes} for details on transactional semantics, temporary\n   * free space required in the Directory, and non-CFS segments on an Exception.\n   * \n   * <p>\n   * <b>NOTE:</b> empty segments are dropped by this method and not added to this\n   * index.\n   * \n   * <p>\n   * <b>NOTE:</b> this merges all given {@link LeafReader}s in one\n   * merge. If you intend to merge a large number of readers, it may be better\n   * to call this method multiple times, each time with a small set of readers.\n   * In principle, if you use a merge policy with a {@code mergeFactor} or\n   * {@code maxMergeAtOnce} parameter, you should pass that many readers in one\n   * call.\n   * \n   * <p>\n   * <b>NOTE:</b> this method does not call or make use of the {@link MergeScheduler},\n   * so any custom bandwidth throttling is at the moment ignored.\n   * \n   * @return The <a href=\"#sequence_number\">sequence number</a>\n   * for this operation\n   *\n   * @throws CorruptIndexException\n   *           if the index is corrupt\n   * @throws IOException\n   *           if there is a low-level IO error\n   * @throws IllegalArgumentException\n   *           if addIndexes would cause the index to exceed {@link #MAX_DOCS}\n   */\n  public long addIndexes(CodecReader... readers) throws IOException {\n    ensureOpen();\n\n    // long so we can detect int overflow:\n    long numDocs = 0;\n    long seqNo;\n    try {\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", \"flush at addIndexes(CodecReader...)\");\n      }\n      flush(false, true);\n\n      String mergedName = newSegmentName();\n      int numSoftDeleted = 0;\n      for (CodecReader leaf : readers) {\n        numDocs += leaf.numDocs();\n        validateMergeReader(leaf);\n        if (softDeletesEnabled) {\n            Bits liveDocs = leaf.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n            DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), leaf), liveDocs);\n        }\n      }\n      \n      // Best-effort up front check:\n      testReserveDocs(numDocs);\n\n      final IOContext context = new IOContext(new MergeInfo(Math.toIntExact(numDocs), -1, false, UNBOUNDED_MAX_MERGE_SEGMENTS));\n\n      // TODO: somehow we should fix this merge so it's\n      // abortable so that IW.close(false) is able to stop it\n      TrackingDirectoryWrapper trackingDir = new TrackingDirectoryWrapper(directory);\n      Codec codec = config.getCodec();\n      // We set the min version to null for now, it will be set later by SegmentMerger\n      SegmentInfo info = new SegmentInfo(directoryOrig, Version.LATEST, null, mergedName, -1,\n                                         false, codec, Collections.emptyMap(), StringHelper.randomId(), Collections.emptyMap(), config.getIndexSort());\n\n      SegmentMerger merger = new SegmentMerger(Arrays.asList(readers), info, infoStream, trackingDir,\n                                               globalFieldNumberMap, \n                                               context);\n\n      if (!merger.shouldMerge()) {\n        return docWriter.getNextSequenceNumber();\n      }\n\n      synchronized (this) {\n        ensureOpen();\n        assert stopMerges == false;\n        runningAddIndexesMerges.add(merger);\n      }\n      try {\n        merger.merge();  // merge 'em\n      } finally {\n        synchronized (this) {\n          runningAddIndexesMerges.remove(merger);\n          notifyAll();\n        }\n      }\n      SegmentCommitInfo infoPerCommit = new SegmentCommitInfo(info, 0, numSoftDeleted, -1L, -1L, -1L, StringHelper.randomId());\n\n      info.setFiles(new HashSet<>(trackingDir.getCreatedFiles()));\n      trackingDir.clearCreatedFiles();\n                                         \n      setDiagnostics(info, SOURCE_ADDINDEXES_READERS);\n\n      final MergePolicy mergePolicy = config.getMergePolicy();\n      boolean useCompoundFile;\n      synchronized(this) { // Guard segmentInfos\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, infoPerCommit, this);\n      }\n\n      // Now create the compound file if needed\n      if (useCompoundFile) {\n        Collection<String> filesToDelete = infoPerCommit.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(directory);\n        // TODO: unlike merge, on exception we arent sniping any trash cfs files here?\n        // createCompoundFile tries to cleanup, but it might not always be able to...\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, info, context, this::deleteNewFiles);\n        } finally {\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToDelete);\n        }\n        info.setUseCompoundFile(true);\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      codec.segmentInfoFormat().write(trackingDir, info, context);\n\n      info.addFiles(trackingDir.getCreatedFiles());\n\n      // Register the new segment\n      synchronized(this) {\n        if (stopMerges) {\n          // Safe: these files must exist\n          deleteNewFiles(infoPerCommit.files());\n\n          return docWriter.getNextSequenceNumber();\n        }\n        ensureOpen();\n\n        // Now reserve the docs, just before we update SIS:\n        reserveDocs(numDocs);\n      \n        segmentInfos.add(infoPerCommit);\n        seqNo = docWriter.getNextSequenceNumber();\n        checkpoint();\n      }\n    } catch (VirtualMachineError tragedy) {\n      tragicEvent(tragedy, \"addIndexes(CodecReader...)\");\n      throw tragedy;\n    }\n    maybeMerge();\n\n    return seqNo;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["7bf95be1fe06a3535aa13b9fc2ce7ebac0eae6db"],"a775b68a26e2d19d1b5f16cd18a3bc8df738a302":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c"],"0ad30c6a479e764150a3316e57263319775f1df2":["c48871ed951104729f5e17a8ee1091b43fa18980","3d33e731a93d4b57e662ff094f64f94a745422d4"],"191128ac5b85671b1671e2c857437694283b6ebf":["d470c8182e92b264680e34081b75e70a9f2b3c89","6483e4260c08168709c02238ae083a51519a28dd"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["c48871ed951104729f5e17a8ee1091b43fa18980","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","79700663e164dece87bed4adfd3e28bab6cb1385"],"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c":["191128ac5b85671b1671e2c857437694283b6ebf"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98b44240f64a2d6935543ff25faee750b29204eb"],"68ba24d6f9380e2463dbe5130d27502647f64904":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b70042a8a492f7054d480ccdd2be9796510d4327":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"79700663e164dece87bed4adfd3e28bab6cb1385":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"98b44240f64a2d6935543ff25faee750b29204eb":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"299a2348fa24151d150182211b6208a38e5e3450":["98a04f56464afdffd4c430d6c47a0c868a38354e","79700663e164dece87bed4adfd3e28bab6cb1385"],"98a04f56464afdffd4c430d6c47a0c868a38354e":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"6483e4260c08168709c02238ae083a51519a28dd":["d470c8182e92b264680e34081b75e70a9f2b3c89","16ebfabc294f23b88b6a39722a02c9d39b353195"],"7bf95be1fe06a3535aa13b9fc2ce7ebac0eae6db":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["0f6df47cbfd656ea50ca2996361f7954531ee18b"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["c48871ed951104729f5e17a8ee1091b43fa18980","191128ac5b85671b1671e2c857437694283b6ebf"],"c48871ed951104729f5e17a8ee1091b43fa18980":["867e3d9153fb761456b54a9dcce566e1545c5ef6"],"618635065f043788c9e034f96ca5cd5cea1b4592":["79700663e164dece87bed4adfd3e28bab6cb1385"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["92b74780b4efed2011d2d1a19183689db904519e"],"aac05884852c2a15a6aa9153063de70dea4fbcae":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["c48871ed951104729f5e17a8ee1091b43fa18980"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["b07024a7318c25225dc4d070cf6d047315b73aaf"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["618635065f043788c9e034f96ca5cd5cea1b4592"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["c48871ed951104729f5e17a8ee1091b43fa18980","5e03940e6e9044943de4b7ac08f8581da37a9534"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"f592209545c71895260367152601e9200399776d":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"92b74780b4efed2011d2d1a19183689db904519e":["68ba24d6f9380e2463dbe5130d27502647f64904"],"867e3d9153fb761456b54a9dcce566e1545c5ef6":["aac05884852c2a15a6aa9153063de70dea4fbcae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["a775b68a26e2d19d1b5f16cd18a3bc8df738a302"]},"commit2Childs":{"845b760a99e5f369fcd0a5d723a87b8def6a3f56":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["b70042a8a492f7054d480ccdd2be9796510d4327","8f2203cb8ae87188877cfbf6ad170c5738a0aad5","f592209545c71895260367152601e9200399776d"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","98b44240f64a2d6935543ff25faee750b29204eb","98a04f56464afdffd4c430d6c47a0c868a38354e"],"a775b68a26e2d19d1b5f16cd18a3bc8df738a302":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"191128ac5b85671b1671e2c857437694283b6ebf":["6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["845b760a99e5f369fcd0a5d723a87b8def6a3f56"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["191128ac5b85671b1671e2c857437694283b6ebf","6483e4260c08168709c02238ae083a51519a28dd","0f6df47cbfd656ea50ca2996361f7954531ee18b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"6b53db710c6b8fb48bb3a2bab4df8d1dfbd7906c":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b","80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2":["98a04f56464afdffd4c430d6c47a0c868a38354e"],"68ba24d6f9380e2463dbe5130d27502647f64904":["92b74780b4efed2011d2d1a19183689db904519e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["505bff044e47a553f461b6f4484d1d08faf4ac85"],"b70042a8a492f7054d480ccdd2be9796510d4327":[],"79700663e164dece87bed4adfd3e28bab6cb1385":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","299a2348fa24151d150182211b6208a38e5e3450","618635065f043788c9e034f96ca5cd5cea1b4592"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"98b44240f64a2d6935543ff25faee750b29204eb":["acf0fc8b8488d15344408e0ed0ab484f4a3e1bf2"],"299a2348fa24151d150182211b6208a38e5e3450":[],"98a04f56464afdffd4c430d6c47a0c868a38354e":["79700663e164dece87bed4adfd3e28bab6cb1385","299a2348fa24151d150182211b6208a38e5e3450"],"6483e4260c08168709c02238ae083a51519a28dd":["191128ac5b85671b1671e2c857437694283b6ebf"],"7bf95be1fe06a3535aa13b9fc2ce7ebac0eae6db":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"16ebfabc294f23b88b6a39722a02c9d39b353195":["6483e4260c08168709c02238ae083a51519a28dd"],"505bff044e47a553f461b6f4484d1d08faf4ac85":["7bf95be1fe06a3535aa13b9fc2ce7ebac0eae6db"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["80d0e6d59ae23f4a6f30eaf40bfb40742300287f"],"c48871ed951104729f5e17a8ee1091b43fa18980":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"618635065f043788c9e034f96ca5cd5cea1b4592":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"b07024a7318c25225dc4d070cf6d047315b73aaf":["14654be3f7a82c9a3c52169e365baa55bfe64f66"],"aac05884852c2a15a6aa9153063de70dea4fbcae":["867e3d9153fb761456b54a9dcce566e1545c5ef6"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["5e03940e6e9044943de4b7ac08f8581da37a9534"],"14654be3f7a82c9a3c52169e365baa55bfe64f66":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"80d0e6d59ae23f4a6f30eaf40bfb40742300287f":[],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["a775b68a26e2d19d1b5f16cd18a3bc8df738a302"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["aac05884852c2a15a6aa9153063de70dea4fbcae"],"5e03940e6e9044943de4b7ac08f8581da37a9534":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["68ba24d6f9380e2463dbe5130d27502647f64904","b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d"],"0f6df47cbfd656ea50ca2996361f7954531ee18b":["16ebfabc294f23b88b6a39722a02c9d39b353195"],"f592209545c71895260367152601e9200399776d":[],"867e3d9153fb761456b54a9dcce566e1545c5ef6":["c48871ed951104729f5e17a8ee1091b43fa18980"],"92b74780b4efed2011d2d1a19183689db904519e":["b07024a7318c25225dc4d070cf6d047315b73aaf"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b70042a8a492f7054d480ccdd2be9796510d4327","92212fd254551a0b1156aafc3a1a6ed1a43932ad","299a2348fa24151d150182211b6208a38e5e3450","80d0e6d59ae23f4a6f30eaf40bfb40742300287f","f592209545c71895260367152601e9200399776d","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}