{"path":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","commits":[{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":1,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","pathOld":"backwards/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17, Field.Store.YES);\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17, Field.Store.YES);\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6","date":1272983566,"type":4,"author":"Uwe Schindler","isMerge":false,"pathNew":"/dev/null","pathOld":"lucene/backwards/src/test/org/apache/lucene/index/TestIndexWriter#testIndexStoreCombos().mjava","sourceNew":null,"sourceOld":"  public void testIndexStoreCombos() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);\n    byte[] b = new byte[50];\n    for(int i=0;i<50;i++)\n      b[i] = (byte) (i+77);\n\n    Document doc = new Document();\n    Field f = new Field(\"binary\", b, 10, 17, Field.Store.YES);\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field1\")));\n    Field f2 = new Field(\"string\", \"value\", Field.Store.YES,Field.Index.ANALYZED);\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc1field2\")));\n    doc.add(f);\n    doc.add(f2);\n    w.addDocument(doc);\n    \n    // add 2 docs to test in-memory merging\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc2field2\")));\n    w.addDocument(doc);\n  \n    // force segment flush so we can force a segment merge with doc3 later.\n    w.commit();\n\n    f.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field1\")));\n    f2.setTokenStream(new WhitespaceTokenizer(new StringReader(\"doc3field2\")));\n\n    w.addDocument(doc);\n    w.commit();\n    w.optimize();   // force segment merge.\n\n    IndexReader ir = IndexReader.open(dir, true);\n    doc = ir.document(0);\n    f = doc.getField(\"binary\");\n    b = f.getBinaryValue();\n    assertTrue(b != null);\n    assertEquals(17, b.length, 17);\n    assertEquals(87, b[0]);\n\n    assertTrue(ir.document(0).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(1).getFieldable(\"binary\").isBinary());\n    assertTrue(ir.document(2).getFieldable(\"binary\").isBinary());\n    \n    assertEquals(\"value\", ir.document(0).get(\"string\"));\n    assertEquals(\"value\", ir.document(1).get(\"string\"));\n    assertEquals(\"value\", ir.document(2).get(\"string\"));\n\n\n    // test that the terms were indexed.\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc1field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc2field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"binary\",\"doc3field1\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc1field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc2field2\")).next());\n    assertTrue(ir.termDocs(new Term(\"string\",\"doc3field2\")).next());\n\n    ir.close();\n    dir.close();\n\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"]},"commit2Childs":{"90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["90eca6fcb6635ca73ea4fdbe2f57d2033b66d3b6"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}