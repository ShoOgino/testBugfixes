{"path":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","commits":[{"id":"888c2d6bca1edd8d9293631d6e1d188b036e0f05","date":1334076894,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["c83d6c4335f31cae14f625a222bc842f20073dcd","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","date":1334174049,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":["888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", new StringReader(input)), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","date":1379858263,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"24a98f5fdd23e04f85819dbc63b47a12f7c44311","date":1482439157,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, offsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":["995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f03e4bed5023ec3ef93a771b8888cae991cf448d","date":1483469262,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, offsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"995993f24c9f6feb42b49b71e1982cda8fa0b37c","date":1522116154,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, graphOffsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), graphOffsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, offsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","24a98f5fdd23e04f85819dbc63b47a12f7c44311","c83d6c4335f31cae14f625a222bc842f20073dcd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","date":1522191940,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, graphOffsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), graphOffsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean offsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, offsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), offsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1c85bcc0cb48e35688c792a172bed271a9836d6b","date":1571776257,"type":3,"author":"David Smiley","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/analysis/BaseTokenStreamTestCase#assertAnalyzesTo(Analyzer,String,String[],int[],int[],String[],int[],int[],boolean).mjava","sourceNew":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean graphOffsetsAreCorrect) throws IOException {\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), graphOffsetsAreCorrect);\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, graphOffsetsAreCorrect);\n  }\n\n","sourceOld":"  public static void assertAnalyzesTo(Analyzer a, String input, String[] output, int startOffsets[], int endOffsets[], String types[], int posIncrements[], int posLengths[], boolean graphOffsetsAreCorrect) throws IOException {\n    checkResetException(a, input);\n    checkAnalysisConsistency(random(), a, true, input, graphOffsetsAreCorrect);\n    assertTokenStreamContents(a.tokenStream(\"dummy\", input), output, startOffsets, endOffsets, types, posIncrements, posLengths, input.length(), graphOffsetsAreCorrect);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","888c2d6bca1edd8d9293631d6e1d188b036e0f05"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["24a98f5fdd23e04f85819dbc63b47a12f7c44311"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","995993f24c9f6feb42b49b71e1982cda8fa0b37c"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0"],"1c85bcc0cb48e35688c792a172bed271a9836d6b":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["1c85bcc0cb48e35688c792a172bed271a9836d6b"],"f03e4bed5023ec3ef93a771b8888cae991cf448d":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","24a98f5fdd23e04f85819dbc63b47a12f7c44311"]},"commit2Childs":{"888c2d6bca1edd8d9293631d6e1d188b036e0f05":["ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"df1b735b811bfe6055a98336ee8dfd1e43cf2dc0":["24a98f5fdd23e04f85819dbc63b47a12f7c44311","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["df1b735b811bfe6055a98336ee8dfd1e43cf2dc0","37a0f60745e53927c4c876cfe5b5a58170f0646c"],"ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd"],"995993f24c9f6feb42b49b71e1982cda8fa0b37c":["d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["888c2d6bca1edd8d9293631d6e1d188b036e0f05","ad9e3deabce40d9849c1b75ef706bfa79f4f0d1e"],"d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7":["1c85bcc0cb48e35688c792a172bed271a9836d6b"],"24a98f5fdd23e04f85819dbc63b47a12f7c44311":["995993f24c9f6feb42b49b71e1982cda8fa0b37c","d61a3e0821ed080b9b21e1328bbaa91dcf79f7d7","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"1c85bcc0cb48e35688c792a172bed271a9836d6b":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"f03e4bed5023ec3ef93a771b8888cae991cf448d":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","cd5edd1f2b162a5cfa08efd17851a07373a96817","f03e4bed5023ec3ef93a771b8888cae991cf448d"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}