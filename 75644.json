{"path":"solr/core/src/java/org/apache/solr/handler/component/PhrasesIdentificationComponent.Phrase#extractPhrases(String,SchemaField,int,int).mjava","commits":[{"id":"0d1411e62d30c460b09c6f3643df82daa10a27cc","date":1536256256,"type":0,"author":"Chris Hostetter","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/handler/component/PhrasesIdentificationComponent.Phrase#extractPhrases(String,SchemaField,int,int).mjava","pathOld":"/dev/null","sourceNew":"    /**\n     * Factory method for constructing a list of Phrases given the specified input and using the analyzer\n     * for the specified field.  The <code>maxIndexedPositionLength</code> and \n     * <code>maxQueryPositionLength</code> provided *must* match the effective values used by \n     * respective analyzers.\n     */\n    public static List<Phrase> extractPhrases(final String input, final SchemaField analysisField,\n                                              final int maxIndexedPositionLength,\n                                              final int maxQueryPositionLength) {\n\n      // TODO: rather then requiring the query analyzer to produce the Phrases for us (assuming Shingles)\n      // we could potentially just require that it produces unigrams compatible with the unigrams in the\n      // indexed fields, and then build our own Phrases at query time -- making the maxQueryPositionLength\n      // a 100% run time configuration option.\n      // But that could be tricky given an arbitrary analyzer -- we'd have pay careful attention\n      // to positions, and we'd have to guess/assume what placeholders/fillers was used in the indexed Phrases\n      // (typically shingles)\n\n      assert maxIndexedPositionLength <= maxQueryPositionLength;\n      \n      final CharsRefBuilder buffer = new CharsRefBuilder();\n      final FieldType ft = analysisField.getType();\n      final Analyzer analyzer = ft.getQueryAnalyzer();\n      final List<Phrase> results = new ArrayList<>(42);\n      try (TokenStream tokenStream = analyzer.tokenStream(analysisField.getName(), input)) {\n        \n        final OffsetAttribute offsetAttr = tokenStream.addAttribute(OffsetAttribute.class);\n        final PositionIncrementAttribute posIncAttr = tokenStream.addAttribute(PositionIncrementAttribute.class);\n        final PositionLengthAttribute posLenAttr = tokenStream.addAttribute(PositionLengthAttribute.class);\n        final TermToBytesRefAttribute termAttr = tokenStream.addAttribute(TermToBytesRefAttribute.class);\n        \n        int position = 0;\n        int lastPosLen = -1;\n        \n        tokenStream.reset();\n        while (tokenStream.incrementToken()) {\n          final Phrase phrase = new Phrase();\n\n          final int posInc = posIncAttr.getPositionIncrement();\n          final int posLen = posLenAttr.getPositionLength();\n\n          if (0 == posInc && posLen <= lastPosLen) {\n            // This requirement of analyzers to return tokens in ascending order of length\n            // is currently neccessary for the \"linking\" logic below to work\n            // if people run into real world sitautions where this is problematic,\n            // we can relax this check if we also make the linking logic more complex\n            // (ie: less optimzied)\n            throw new SolrException\n              (ErrorCode.BAD_REQUEST, \"Phrase identification currently requires that \" +\n               \"the analyzer used must produce tokens that overlap in increasing order of length. \");\n          }\n          \n          position += posInc;\n          lastPosLen = posLen;\n          \n          phrase.position_start = position;\n          phrase.position_end = position + posLen;\n          \n          phrase.is_indexed = (posLen <= maxIndexedPositionLength);\n          \n          phrase.offset_start = offsetAttr.startOffset();\n          phrase.offset_end = offsetAttr.endOffset();\n\n          // populate the subsequence directly from the raw input using the offsets,\n          // (instead of using the TermToBytesRefAttribute) so we preserve the original\n          // casing, whitespace, etc...\n          phrase.subSequence = input.subSequence(phrase.offset_start, phrase.offset_end);\n          \n          if (phrase.is_indexed) {\n            // populate the bytes so we can build term queries\n            phrase.bytes = BytesRef.deepCopyOf(termAttr.getBytesRef());\n          }\n          \n          results.add(phrase);\n        }\n        tokenStream.end();\n      } catch (IOException e) {\n        throw new SolrException(ErrorCode.SERVER_ERROR,\n                                \"Analysis error extracting phrases from: \" + input, e); \n      }\n      \n      // fill in the relationships of each phrase\n      //\n      // NOTE: this logic currently requries that the phrases are sorted by position ascending\n      // (automatic because of how PositionIncrementAttribute works) then by length ascending\n      // (when positions are tied).\n      // We could de-optimize this code if we find that secondary ordering is too restrictive for\n      // some analyzers\n      //\n      // NOTE changes to scoring model may be allow optimize/prune down the relationships tracked,\n      // ...OR.... may require us to add/track more details about sub/parent phrases\n      //\n      for (int p = 0; p < results.size(); p++) {\n        final Phrase current = results.get(p);\n        if (! current.is_indexed) {\n          // we're not an interesting sub phrase of anything\n          continue;\n        }\n        \n        // setup links from the phrase to itself if needed\n        addLinkages(current, current, maxIndexedPositionLength);\n        \n        // scan backwards looking for phrases that might include us...\n        BEFORE: for (int i = p-1; 0 <= i; i--) {\n          final Phrase previous = results.get(i);\n          if (previous.position_start < (current.position_end - maxQueryPositionLength)) {\n            // we've scanned so far back nothing else is viable\n            break BEFORE;\n          }\n          // any 'previous' phrases must start where current starts or earlier,\n          // so only need to check the end...\n          if (current.position_end <= previous.position_end) {\n            addLinkages(previous, current, maxIndexedPositionLength);\n          }\n        }\n        // scan forwards looking for phrases that might include us...\n        AFTER: for (int i = p+1; i < results.size(); i++) {\n          final Phrase next = results.get(i);\n          // the only way a phrase that comes after current can include current is\n          // if they have the same start position...\n          if (current.position_start != next.position_start) {\n            // we've scanned so far forward nothing else is viable\n            break AFTER;\n          }\n          // any 'next' phrases must start where current starts, so only need to check the end...\n          if (current.position_end <= next.position_end) {\n            addLinkages(next, current, maxIndexedPositionLength);\n          }\n        }\n      }\n      \n      return Collections.unmodifiableList(results);\n    }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"0d1411e62d30c460b09c6f3643df82daa10a27cc":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0d1411e62d30c460b09c6f3643df82daa10a27cc"]},"commit2Childs":{"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0d1411e62d30c460b09c6f3643df82daa10a27cc"],"0d1411e62d30c460b09c6f3643df82daa10a27cc":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}