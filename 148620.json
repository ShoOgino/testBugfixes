{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","commits":[{"id":"f9a98541130dbb2dd570f39bd89ced65760cad80","date":1355032328,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  private String getLeader(final CloudDescriptor cloudDesc) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId())\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"407687e67faf6e1f02a211ca078d8e3eed631027","date":1355157407,"type":0,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"/dev/null","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7f70f627ed5d724ec67fa283565a5ab0fdf37488","date":1356803993,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d3fcb70cf561547c7bb1506e0cf32ca7b1287064","date":1357616416,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, 30000);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            30000);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"5b9b43a9dc0834e96a43aa61aae5eea084e0829a","date":1357654236,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4e6354dd7c71fe122926fc53d7d29f715b1283db","date":1357915185,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms);\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d8e7bb86500fd3d522e286ba89fad6089161c687","date":1363307515,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeader = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeader)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeader + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk\", e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a0b5db1c20d7a379233ae3955449a9e42caef007","date":1394048511,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(msInSec);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n        \n        if (tries % 30 == 0) {\n          String warnMsg = String.format(\"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"1656158940b6a27def3c01043df10da4d758e664","date":1394053961,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(msInSec);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n        \n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(msInSec);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n        \n        if (tries % 30 == 0) {\n          String warnMsg = String.format(\"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"96ea64d994d340044e0d57aeb6a5871539d10ca5","date":1394225445,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(msInSec);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n        \n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries == 60) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(1000);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7de2ec0ce65962e078e24b2ca174d1cb269b610d","date":1397799037,"type":3,"author":"Shalin Shekhar Mangar","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        Thread.sleep(msInSec);\n        tries++;\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n        \n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":["f9a98541130dbb2dd570f39bd89ced65760cad80","7f70f627ed5d724ec67fa283565a5ab0fdf37488","6013b4c7388f1627659c8f96c44abd10a294d3a6","d8e7bb86500fd3d522e286ba89fad6089161c687","a0b5db1c20d7a379233ae3955449a9e42caef007"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fbcfc050b9f253136eaa5950b57248b2109eac11","date":1427308993,"type":3,"author":"Noble Paul","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n    \n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n      \n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n                                   // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int)Math.floor(leaderConflictResolveWait/msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n              + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n      \n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    } \n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"bb222a3f9d9421d5c95afce73013fbd8de07ea1f","date":1543514331,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (cc.isShutDown()) throw new AlreadyClosedException();\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (AlreadyClosedException e) { \n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15","date":1554259533,"type":3,"author":"Gus Heck","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#getLeader(CloudDescriptor,int).mjava","sourceNew":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (cc.isShutDown()) throw new AlreadyClosedException();\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (AlreadyClosedException e) {\n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","sourceOld":"  // timeoutms is the timeout for the first call to get the leader - there is then\n  // a longer wait to make sure that leader matches our local state\n  private String getLeader(final CloudDescriptor cloudDesc, int timeoutms) {\n\n    String collection = cloudDesc.getCollectionName();\n    String shardId = cloudDesc.getShardId();\n    // rather than look in the cluster state file, we go straight to the zknodes\n    // here, because on cluster restart there could be stale leader info in the\n    // cluster state node that won't be updated for a moment\n    String leaderUrl;\n    try {\n      leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n          .getCoreUrl();\n\n      // now wait until our currently cloud state contains the latest leader\n      String clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection,\n          shardId, timeoutms * 2); // since we found it in zk, we are willing to\n      // wait a while to find it in state\n      int tries = 0;\n      final long msInSec = 1000L;\n      int maxTries = (int) Math.floor(leaderConflictResolveWait / msInSec);\n      while (!leaderUrl.equals(clusterStateLeaderUrl)) {\n        if (cc.isShutDown()) throw new AlreadyClosedException();\n        if (tries > maxTries) {\n          throw new SolrException(ErrorCode.SERVER_ERROR,\n              \"There is conflicting information about the leader of shard: \"\n                  + cloudDesc.getShardId() + \" our state says:\"\n                  + clusterStateLeaderUrl + \" but zookeeper says:\" + leaderUrl);\n        }\n        tries++;\n        if (tries % 30 == 0) {\n          String warnMsg = String.format(Locale.ENGLISH, \"Still seeing conflicting information about the leader \"\n                  + \"of shard %s for collection %s after %d seconds; our state says %s, but ZooKeeper says %s\",\n              cloudDesc.getShardId(), collection, tries, clusterStateLeaderUrl, leaderUrl);\n          log.warn(warnMsg);\n        }\n        Thread.sleep(msInSec);\n        clusterStateLeaderUrl = zkStateReader.getLeaderUrl(collection, shardId,\n            timeoutms);\n        leaderUrl = getLeaderProps(collection, cloudDesc.getShardId(), timeoutms)\n            .getCoreUrl();\n      }\n\n    } catch (AlreadyClosedException e) { \n      throw e;\n    } catch (Exception e) {\n      log.error(\"Error getting leader from zk\", e);\n      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,\n          \"Error getting leader from zk for shard \" + shardId, e);\n    }\n    return leaderUrl;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1656158940b6a27def3c01043df10da4d758e664":["a0b5db1c20d7a379233ae3955449a9e42caef007"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["407687e67faf6e1f02a211ca078d8e3eed631027","7f70f627ed5d724ec67fa283565a5ab0fdf37488"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":["d8e7bb86500fd3d522e286ba89fad6089161c687","1656158940b6a27def3c01043df10da4d758e664"],"7de2ec0ce65962e078e24b2ca174d1cb269b610d":["1656158940b6a27def3c01043df10da4d758e664"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["fbcfc050b9f253136eaa5950b57248b2109eac11"],"407687e67faf6e1f02a211ca078d8e3eed631027":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","f9a98541130dbb2dd570f39bd89ced65760cad80"],"7f70f627ed5d724ec67fa283565a5ab0fdf37488":["f9a98541130dbb2dd570f39bd89ced65760cad80"],"a0b5db1c20d7a379233ae3955449a9e42caef007":["d8e7bb86500fd3d522e286ba89fad6089161c687"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["7de2ec0ce65962e078e24b2ca174d1cb269b610d","fbcfc050b9f253136eaa5950b57248b2109eac11"],"d8e7bb86500fd3d522e286ba89fad6089161c687":["5b9b43a9dc0834e96a43aa61aae5eea084e0829a"],"f9a98541130dbb2dd570f39bd89ced65760cad80":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"5b9b43a9dc0834e96a43aa61aae5eea084e0829a":["7f70f627ed5d724ec67fa283565a5ab0fdf37488"],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","5b9b43a9dc0834e96a43aa61aae5eea084e0829a"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15"],"fbcfc050b9f253136eaa5950b57248b2109eac11":["7de2ec0ce65962e078e24b2ca174d1cb269b610d"]},"commit2Childs":{"1656158940b6a27def3c01043df10da4d758e664":["96ea64d994d340044e0d57aeb6a5871539d10ca5","7de2ec0ce65962e078e24b2ca174d1cb269b610d"],"d3fcb70cf561547c7bb1506e0cf32ca7b1287064":["4e6354dd7c71fe122926fc53d7d29f715b1283db"],"96ea64d994d340044e0d57aeb6a5871539d10ca5":[],"7de2ec0ce65962e078e24b2ca174d1cb269b610d":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","fbcfc050b9f253136eaa5950b57248b2109eac11"],"bb222a3f9d9421d5c95afce73013fbd8de07ea1f":["0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15"],"407687e67faf6e1f02a211ca078d8e3eed631027":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064"],"7f70f627ed5d724ec67fa283565a5ab0fdf37488":["d3fcb70cf561547c7bb1506e0cf32ca7b1287064","5b9b43a9dc0834e96a43aa61aae5eea084e0829a"],"a0b5db1c20d7a379233ae3955449a9e42caef007":["1656158940b6a27def3c01043df10da4d758e664"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"d8e7bb86500fd3d522e286ba89fad6089161c687":["96ea64d994d340044e0d57aeb6a5871539d10ca5","a0b5db1c20d7a379233ae3955449a9e42caef007"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["407687e67faf6e1f02a211ca078d8e3eed631027","f9a98541130dbb2dd570f39bd89ced65760cad80"],"f9a98541130dbb2dd570f39bd89ced65760cad80":["407687e67faf6e1f02a211ca078d8e3eed631027","7f70f627ed5d724ec67fa283565a5ab0fdf37488"],"5b9b43a9dc0834e96a43aa61aae5eea084e0829a":["d8e7bb86500fd3d522e286ba89fad6089161c687","4e6354dd7c71fe122926fc53d7d29f715b1283db"],"0db83f1bb855a4ac824c9a2a8e1ee9b29a039c15":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4e6354dd7c71fe122926fc53d7d29f715b1283db":[],"fbcfc050b9f253136eaa5950b57248b2109eac11":["bb222a3f9d9421d5c95afce73013fbd8de07ea1f","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["96ea64d994d340044e0d57aeb6a5871539d10ca5","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","4e6354dd7c71fe122926fc53d7d29f715b1283db","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}