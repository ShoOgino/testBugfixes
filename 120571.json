{"path":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/contrib/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4c1b87b155748a27fbed84a0ffb3f8799177451e","date":1346349018,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"05a14b2611ead08655a2b2bdc61632eb31316e57","date":1346366621,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c83d6c4335f31cae14f625a222bc842f20073dcd","date":1373306148,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, text);\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":["69e043c521d4e8db770cc140c63f5ef51f03426a"],"bugIntro":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"37a0f60745e53927c4c876cfe5b5a58170f0646c","date":1373994005,"type":3,"author":"Han Jiang","isMerge":true,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, text);\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, new StringReader(text));\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"782ed6a4b4ba50ec19734fc8db4e570ee193d627","date":1381127065,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {\n      TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      BytesRef bytesRef = termAttribute.getBytesRef();\n\n      tokenStream.reset();\n    \n      while (tokenStream.incrementToken()) {\n        termAttribute.fillBytesRef();\n        bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n      }\n\n      tokenStream.end();\n    }\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    TokenStream tokenStream = analyzer.tokenStream(field, text);\n    TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n    BytesRef bytesRef = termAttribute.getBytesRef();\n\n    tokenStream.reset();\n    \n    while (tokenStream.incrementToken()) {\n      termAttribute.fillBytesRef();\n      bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n    }\n\n    tokenStream.end();\n    tokenStream.close();\n\n    return bytesRefs;\n  }\n\n","bugFix":["e6e919043fa85ee891123768dd655a98edbbf63c","4c1b87b155748a27fbed84a0ffb3f8799177451e","dfd4d352ddf04b37253ad97ce1aad1448253f0f7","c83d6c4335f31cae14f625a222bc842f20073dcd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"634f330c54fd3f9f491d52036dc3f40b4f4d8934","date":1394635157,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<>();\n\n    try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {\n      TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      BytesRef bytesRef = termAttribute.getBytesRef();\n\n      tokenStream.reset();\n    \n      while (tokenStream.incrementToken()) {\n        termAttribute.fillBytesRef();\n        bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n      }\n\n      tokenStream.end();\n    }\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<BytesRef>();\n\n    try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {\n      TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      BytesRef bytesRef = termAttribute.getBytesRef();\n\n      tokenStream.reset();\n    \n      while (tokenStream.incrementToken()) {\n        termAttribute.fillBytesRef();\n        bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n      }\n\n      tokenStream.end();\n    }\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"804b857d1066ab5185b3b9101bde41b0b71426ec","date":1435846169,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","pathOld":"lucene/highlighter/src/test/org/apache/lucene/search/vectorhighlight/AbstractTestCase#analyze(String,String,Analyzer).mjava","sourceNew":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<>();\n\n    try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {\n      TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n      \n      tokenStream.reset();\n    \n      while (tokenStream.incrementToken()) {\n        bytesRefs.add(BytesRef.deepCopyOf(termAttribute.getBytesRef()));\n      }\n\n      tokenStream.end();\n    }\n\n    return bytesRefs;\n  }\n\n","sourceOld":"  protected List<BytesRef> analyze(String text, String field, Analyzer analyzer) throws IOException {\n    List<BytesRef> bytesRefs = new ArrayList<>();\n\n    try (TokenStream tokenStream = analyzer.tokenStream(field, text)) {\n      TermToBytesRefAttribute termAttribute = tokenStream.getAttribute(TermToBytesRefAttribute.class);\n\n      BytesRef bytesRef = termAttribute.getBytesRef();\n\n      tokenStream.reset();\n    \n      while (tokenStream.incrementToken()) {\n        termAttribute.fillBytesRef();\n        bytesRefs.add(BytesRef.deepCopyOf(bytesRef));\n      }\n\n      tokenStream.end();\n    }\n\n    return bytesRefs;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"4c1b87b155748a27fbed84a0ffb3f8799177451e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":["4c1b87b155748a27fbed84a0ffb3f8799177451e","c83d6c4335f31cae14f625a222bc842f20073dcd"],"c83d6c4335f31cae14f625a222bc842f20073dcd":["4c1b87b155748a27fbed84a0ffb3f8799177451e"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["c83d6c4335f31cae14f625a222bc842f20073dcd"],"05a14b2611ead08655a2b2bdc61632eb31316e57":["b89678825b68eccaf09e6ab71675fc0b0af1e099","4c1b87b155748a27fbed84a0ffb3f8799177451e"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["804b857d1066ab5185b3b9101bde41b0b71426ec"]},"commit2Childs":{"804b857d1066ab5185b3b9101bde41b0b71426ec":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"4c1b87b155748a27fbed84a0ffb3f8799177451e":["37a0f60745e53927c4c876cfe5b5a58170f0646c","c83d6c4335f31cae14f625a222bc842f20073dcd","05a14b2611ead08655a2b2bdc61632eb31316e57"],"634f330c54fd3f9f491d52036dc3f40b4f4d8934":["804b857d1066ab5185b3b9101bde41b0b71426ec"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["4c1b87b155748a27fbed84a0ffb3f8799177451e","05a14b2611ead08655a2b2bdc61632eb31316e57"],"37a0f60745e53927c4c876cfe5b5a58170f0646c":[],"c83d6c4335f31cae14f625a222bc842f20073dcd":["37a0f60745e53927c4c876cfe5b5a58170f0646c","782ed6a4b4ba50ec19734fc8db4e570ee193d627"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"782ed6a4b4ba50ec19734fc8db4e570ee193d627":["634f330c54fd3f9f491d52036dc3f40b4f4d8934"],"05a14b2611ead08655a2b2bdc61632eb31316e57":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["37a0f60745e53927c4c876cfe5b5a58170f0646c","05a14b2611ead08655a2b2bdc61632eb31316e57","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}