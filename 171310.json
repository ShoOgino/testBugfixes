{"path":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","commits":[{"id":"b2e70b697d9f5a6130848741a4f0cdc08a1fe258","date":1168661303,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"/dev/null","sourceNew":"  /* (non-Javadoc)\r\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\r\n   */\r\n  protected Query[] prepareQueries() throws Exception {\r\n    // exatract some 100 words from doc text to an array\r\n    String words[];\r\n    ArrayList w = new ArrayList();\r\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\r\n    while (st.hasMoreTokens() && w.size()<100) {\r\n      w.add(st.nextToken());\r\n    }\r\n    words = (String[]) w.toArray(new String[0]);\r\n\r\n    // create queries (that would find stuff) with varying slops\r\n    ArrayList queries = new ArrayList(); \r\n    for (int slop=0; slop<8; slop++) {\r\n      for (int qlen=2; qlen<6; qlen++) {\r\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\r\n          // ordered\r\n          int remainedSlop = slop;\r\n          PhraseQuery q = new PhraseQuery();\r\n          q.setSlop(slop);\r\n          int wind = wd;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(SimpleDocMaker.BODY_FIELD,words[wind++]));\r\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind++;\r\n            }\r\n          }\r\n          queries.add(q);\r\n          // reveresed\r\n          remainedSlop = slop;\r\n          q = new PhraseQuery();\r\n          q.setSlop(slop+2*qlen);\r\n          wind = wd+qlen+remainedSlop-1;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(SimpleDocMaker.BODY_FIELD,words[wind--]));\r\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind--;\r\n            }\r\n          }\r\n          queries.add(q);\r\n        }\r\n      }\r\n    }\r\n    return (Query[]) queries.toArray(new Query[0]);\r\n  }\r\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1ebcfbd9662f54e73fdfe1f4d675ea39531fb9b0","date":1174485154,"type":3,"author":"Grant Ingersoll","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\r\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\r\n   */\r\n  protected Query[] prepareQueries() throws Exception {\r\n    // exatract some 100 words from doc text to an array\r\n    String words[];\r\n    ArrayList w = new ArrayList();\r\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\r\n    while (st.hasMoreTokens() && w.size()<100) {\r\n      w.add(st.nextToken());\r\n    }\r\n    words = (String[]) w.toArray(new String[0]);\r\n\r\n    // create queries (that would find stuff) with varying slops\r\n    ArrayList queries = new ArrayList(); \r\n    for (int slop=0; slop<8; slop++) {\r\n      for (int qlen=2; qlen<6; qlen++) {\r\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\r\n          // ordered\r\n          int remainedSlop = slop;\r\n          PhraseQuery q = new PhraseQuery();\r\n          q.setSlop(slop);\r\n          int wind = wd;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind++;\r\n            }\r\n          }\r\n          queries.add(q);\r\n          // reveresed\r\n          remainedSlop = slop;\r\n          q = new PhraseQuery();\r\n          q.setSlop(slop+2*qlen);\r\n          wind = wd+qlen+remainedSlop-1;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind--;\r\n            }\r\n          }\r\n          queries.add(q);\r\n        }\r\n      }\r\n    }\r\n    return (Query[]) queries.toArray(new Query[0]);\r\n  }\r\n\n","sourceOld":"  /* (non-Javadoc)\r\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\r\n   */\r\n  protected Query[] prepareQueries() throws Exception {\r\n    // exatract some 100 words from doc text to an array\r\n    String words[];\r\n    ArrayList w = new ArrayList();\r\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\r\n    while (st.hasMoreTokens() && w.size()<100) {\r\n      w.add(st.nextToken());\r\n    }\r\n    words = (String[]) w.toArray(new String[0]);\r\n\r\n    // create queries (that would find stuff) with varying slops\r\n    ArrayList queries = new ArrayList(); \r\n    for (int slop=0; slop<8; slop++) {\r\n      for (int qlen=2; qlen<6; qlen++) {\r\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\r\n          // ordered\r\n          int remainedSlop = slop;\r\n          PhraseQuery q = new PhraseQuery();\r\n          q.setSlop(slop);\r\n          int wind = wd;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(SimpleDocMaker.BODY_FIELD,words[wind++]));\r\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind++;\r\n            }\r\n          }\r\n          queries.add(q);\r\n          // reveresed\r\n          remainedSlop = slop;\r\n          q = new PhraseQuery();\r\n          q.setSlop(slop+2*qlen);\r\n          wind = wd+qlen+remainedSlop-1;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(SimpleDocMaker.BODY_FIELD,words[wind--]));\r\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind--;\r\n            }\r\n          }\r\n          queries.add(q);\r\n        }\r\n      }\r\n    }\r\n    return (Query[]) queries.toArray(new Query[0]);\r\n  }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3738fa43eaa87dc7b393fe98b04cde1019e20bac","date":1175557034,"type":3,"author":"Otis Gospodnetic","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // exatract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reveresed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\r\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\r\n   */\r\n  protected Query[] prepareQueries() throws Exception {\r\n    // exatract some 100 words from doc text to an array\r\n    String words[];\r\n    ArrayList w = new ArrayList();\r\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\r\n    while (st.hasMoreTokens() && w.size()<100) {\r\n      w.add(st.nextToken());\r\n    }\r\n    words = (String[]) w.toArray(new String[0]);\r\n\r\n    // create queries (that would find stuff) with varying slops\r\n    ArrayList queries = new ArrayList(); \r\n    for (int slop=0; slop<8; slop++) {\r\n      for (int qlen=2; qlen<6; qlen++) {\r\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\r\n          // ordered\r\n          int remainedSlop = slop;\r\n          PhraseQuery q = new PhraseQuery();\r\n          q.setSlop(slop);\r\n          int wind = wd;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind++;\r\n            }\r\n          }\r\n          queries.add(q);\r\n          // reveresed\r\n          remainedSlop = slop;\r\n          q = new PhraseQuery();\r\n          q.setSlop(slop+2*qlen);\r\n          wind = wd+qlen+remainedSlop-1;\r\n          for (int i=0; i<qlen; i++) {\r\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\r\n              remainedSlop--;\r\n              wind--;\r\n            }\r\n          }\r\n          queries.add(q);\r\n        }\r\n      }\r\n    }\r\n    return (Query[]) queries.toArray(new Query[0]);\r\n  }\r\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b","date":1245355139,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // exatract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reveresed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // exatract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SimpleDocMaker.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reveresed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(BasicDocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"add7d922e63099fbce8f0a1b31216df7ef5067f1","date":1252002701,"type":3,"author":"Chris M. Hostetter","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // exatract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reveresed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ffdf794cee8d43eb612df752c592cef2dc3e75ae","date":1256465578,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList<String> w = new ArrayList<String>();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList<Query> queries = new ArrayList<Query>(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList w = new ArrayList();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = (String[]) w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList queries = new ArrayList(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return (Query[]) queries.toArray(new Query[0]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d57eb7c98c08c03af6e4cd83509df31c81ac16af","date":1257684312,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  @Override\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList<String> w = new ArrayList<String>();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList<Query> queries = new ArrayList<Query>(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList<String> w = new ArrayList<String>();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList<Query> queries = new ArrayList<Query>(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return queries.toArray(new Query[0]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","pathOld":"contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker#prepareQueries().mjava","sourceNew":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  @Override\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList<String> w = new ArrayList<String>();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList<Query> queries = new ArrayList<Query>(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return queries.toArray(new Query[0]);\n  }\n\n","sourceOld":"  /* (non-Javadoc)\n   * @see org.apache.lucene.benchmark.byTask.feeds.SimpleQueryMaker#prepareQueries()\n   */\n  @Override\n  protected Query[] prepareQueries() throws Exception {\n    // extract some 100 words from doc text to an array\n    String words[];\n    ArrayList<String> w = new ArrayList<String>();\n    StringTokenizer st = new StringTokenizer(SingleDocSource.DOC_TEXT);\n    while (st.hasMoreTokens() && w.size()<100) {\n      w.add(st.nextToken());\n    }\n    words = w.toArray(new String[0]);\n\n    // create queries (that would find stuff) with varying slops\n    ArrayList<Query> queries = new ArrayList<Query>(); \n    for (int slop=0; slop<8; slop++) {\n      for (int qlen=2; qlen<6; qlen++) {\n        for (int wd=0; wd<words.length-qlen-slop; wd++) {\n          // ordered\n          int remainedSlop = slop;\n          PhraseQuery q = new PhraseQuery();\n          q.setSlop(slop);\n          int wind = wd;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind++]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind++;\n            }\n          }\n          queries.add(q);\n          // reversed\n          remainedSlop = slop;\n          q = new PhraseQuery();\n          q.setSlop(slop+2*qlen);\n          wind = wd+qlen+remainedSlop-1;\n          for (int i=0; i<qlen; i++) {\n            q.add(new Term(DocMaker.BODY_FIELD,words[wind--]));\n            if (remainedSlop>0) {\n              remainedSlop--;\n              wind--;\n            }\n          }\n          queries.add(q);\n        }\n      }\n    }\n    return queries.toArray(new Query[0]);\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"1ebcfbd9662f54e73fdfe1f4d675ea39531fb9b0":["b2e70b697d9f5a6130848741a4f0cdc08a1fe258"],"ffdf794cee8d43eb612df752c592cef2dc3e75ae":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b":["3738fa43eaa87dc7b393fe98b04cde1019e20bac"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["6944b9fa6d8ef96b83ae2d3a4332d03b3857355b"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"3738fa43eaa87dc7b393fe98b04cde1019e20bac":["1ebcfbd9662f54e73fdfe1f4d675ea39531fb9b0"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["ffdf794cee8d43eb612df752c592cef2dc3e75ae"],"b2e70b697d9f5a6130848741a4f0cdc08a1fe258":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"]},"commit2Childs":{"1ebcfbd9662f54e73fdfe1f4d675ea39531fb9b0":["3738fa43eaa87dc7b393fe98b04cde1019e20bac"],"ffdf794cee8d43eb612df752c592cef2dc3e75ae":["d57eb7c98c08c03af6e4cd83509df31c81ac16af"],"6944b9fa6d8ef96b83ae2d3a4332d03b3857355b":["add7d922e63099fbce8f0a1b31216df7ef5067f1"],"add7d922e63099fbce8f0a1b31216df7ef5067f1":["ffdf794cee8d43eb612df752c592cef2dc3e75ae"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b2e70b697d9f5a6130848741a4f0cdc08a1fe258"],"3738fa43eaa87dc7b393fe98b04cde1019e20bac":["6944b9fa6d8ef96b83ae2d3a4332d03b3857355b"],"b2e70b697d9f5a6130848741a4f0cdc08a1fe258":["1ebcfbd9662f54e73fdfe1f4d675ea39531fb9b0"],"d57eb7c98c08c03af6e4cd83509df31c81ac16af":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}