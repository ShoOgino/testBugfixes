{"path":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","commits":[{"id":"ee59f646cf24586a449cad77391a60a3ac8d8959","date":1408015131,"type":1,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<AtomicReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<AtomicReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<AtomicReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2131047ecceac64b54ba70feec3d26bbd7e483d7","date":1411862069,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0c121ee1dd2daa7753dedf9b86de36146d3e9f27","date":1412083699,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"4cbcba1596953276043d89eca0af2ef0bd115c79","date":1412306275,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"9bb9a29a5e71a90295f175df8919802993142c9a","date":1412517673,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n          context, config.getCheckIntegrityAtMerge());\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      MergeState mergeState;\n      boolean success3 = false;\n      try {\n        if (!merger.shouldMerge()) {\n          // would result in a 0 document segment: nothing to merge!\n          mergeState = new MergeState(new ArrayList<LeafReader>(), merge.info.info, infoStream, checkAbort);\n        } else {\n          mergeState = merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merge.info.info.getDocCount() == 0) {\n          infoStream.message(\"IW\", \"merge away fully deleted segments\");\n        } else {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.fieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.fieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.fieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.fieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        }\n      }\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n              deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_EXTENSION));\n            deleter.deleteFile(IndexFileNames.segmentFileName(mergedName, \"\", IndexFileNames.COMPOUND_FILE_ENTRIES_EXTENSION));\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.fieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null && merge.info.info.getDocCount() != 0) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      // Force READ context because we merge deletes onto\n      // this reader:\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051","date":1412632911,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e2fe60a17a7a0cfd101b1169acf089221bc6c166","date":1412767493,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"7599427f762eb1b4265584fd6e96521e4a1a4f3c","date":1413100083,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"55980207f1977bd1463465de1659b821347e2fa8","date":1413336386,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    final String mergedName = merge.info.info.name;\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().getSegmentInfoWriter().write(directory, merge.info.info, mergeState.mergeFieldInfos, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a485550d032df41f9ff97f4d97d81e2be011d3ca","date":1414053996,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n          merge.info.info, infoStream, dirWrapper,\n          checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8985b5ad07c1b5bb28f504744862fc1c56d4c065","date":1419243010,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      long mergeStartTime = 0;\n      if (infoStream.isEnabled(\"IW\")) {\n        mergeStartTime = System.nanoTime();\n      }\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-mergeStartTime)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%d msec to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           ((t1-mergeStartTime)/1000000),\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                           (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                           (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                           (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\"));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"5faf65b6692f15cca0f87bf8666c87899afc619f","date":1420468108,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, mergeDirectory, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.checkAborted(directory);\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final MergeState.CheckAbort checkAbort = new MergeState.CheckAbort(merge, directory);\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(directory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     checkAbort, globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted(directory);\n\n      long mergeStartTime = 0;\n      if (infoStream.isEnabled(\"IW\")) {\n        mergeStartTime = System.nanoTime();\n      }\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      // Record which codec was used to write the segment\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-mergeStartTime)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%d msec to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           ((t1-mergeStartTime)/1000000),\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, directory, checkAbort, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7b7defbb9b6dd128f30374dce48d25526e60f83","date":1422469255,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, mergeDirectory, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, mergeDirectory, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":["e072d0b1fc19e0533d8ce432eed245196bca6fde"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3c5705cb93fb3daa46c676cad08b916dd57bf1be","date":1422473298,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        String cfsFiles[] = merge.info.info.getCodec().compoundFormat().files(merge.info.info);\n        Collection<String> filesToRemove = merge.info.files();\n\n        try {\n          filesToRemove = createCompoundFile(infoStream, mergeDirectory, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b0267c69e2456a3477a1ad785723f2135da3117e","date":1425317087,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b06445ae1731e049327712db0454e5643ca9b7fe","date":1425329139,"type":3,"author":"Dawid Weiss","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.getDocCount() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.getDocCount(): \"delCount=\" + delCount + \" info.docCount=\" + info.info.getDocCount() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" docCount=\" + merge.info.info.getDocCount() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.getDocCount() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.getDocCount() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.getDocCount();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","date":1434062028,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"950882a2bd2a5f9dc16a154871584eaa643d882a","date":1436366563,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (IOException ioe) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback or close(false)\n              // is called -- fall through to logic below to\n              // remove the partially created CFS:\n            } else {\n              handleMergeException(ioe, merge);\n            }\n          }\n        } catch (Throwable t) {\n          handleMergeException(t, merge);\n        } finally {\n          if (!success) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n\n            synchronized(this) {\n              Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n              for (String cfsFile : cfsFiles) {\n                deleter.deleteFile(cfsFile);\n              }\n              deleter.deleteNewFiles(merge.info.files());\n            }\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleter.deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            Set<String> cfsFiles = new HashSet<>(trackingCFSDir.getCreatedFiles());\n            for (String cfsFile : cfsFiles) {\n              deleter.deleteFile(cfsFile);\n            }\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          synchronized(this) {\n            deleter.deleteNewFiles(merge.info.files());\n          }\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (!success) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aac05884852c2a15a6aa9153063de70dea4fbcae","date":1441829939,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      boolean success3 = false;\n      try {\n        if (merger.shouldMerge()) {\n          merger.merge();\n        }\n        success3 = true;\n      } finally {\n        if (!success3) {\n          synchronized(this) {  \n            deleter.refresh(merge.info.info.name);\n          }\n        }\n      }\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7277addaa5100a3b464703b0a0efb5a993ff5999","date":1447264320,"type":3,"author":"Christine Poerschke","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>();\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"da1c100e0e15178d145d7c3c8f38f3e553b92c12","date":1457461708,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ceaef6cfc68c8ab22a684192e469a8280f9e6e70","date":1462354657,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3d33e731a93d4b57e662ff094f64f94a745422d4","date":1463128289,"type":3,"author":"Mike McCandless","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0ad30c6a479e764150a3316e57263319775f1df2","date":1463395403,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d470c8182e92b264680e34081b75e70a9f2b3c89","date":1463985353,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n      \n      // we pass merge.getMergeReaders() instead of merge.readers to allow the\n      // OneMerge to return a view over the actual segments to merge\n      final SegmentMerger merger = new SegmentMerger(merge.getMergeReaders(),\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c1ee9437ba5a8297220428d48a6bb823d1fcd57b","date":1489137809,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n\n    merge.rateLimiter.checkAbort();\n\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.rateLimiter.checkAbort();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          double stoppedSec = merge.rateLimiter.getTotalStoppedNS()/1000000000.;\n          double throttleSec = merge.rateLimiter.getTotalPausedNS()/1000000000.;\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec (%.1f sec stopped, %.1f sec paused) to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           stoppedSec,\n                                           throttleSec,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.rateLimiter.getAbort()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.rateLimiter.getAbort()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        mergeReaders.add(merge.wrapForMerge(reader));\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f4363cd33f6eff7fb4753574a441e2d18c1022a4","date":1498067235,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7dfa64bc2074fb87d0ca70095a644c1ead107e1","date":1498356339,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"28288370235ed02234a64753cdbf0c6ec096304a","date":1498726817,"type":3,"author":"Karl Wright","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        // Carefully pull the most recent live docs and reader\n        SegmentReader reader;\n        final Bits liveDocs;\n        final int delCount;\n\n        synchronized (this) {\n          // Must sync to ensure BufferedDeletesStream cannot change liveDocs,\n          // pendingDeleteCount and field updates while we pull a copy:\n          reader = rld.getReaderForMerge(context);\n          liveDocs = rld.getReadOnlyLiveDocs();\n          delCount = rld.getPendingDeleteCount() + info.getDelCount();\n\n          assert reader != null;\n          assert rld.verifyDocCounts();\n\n          if (infoStream.isEnabled(\"IW\")) {\n            if (rld.getPendingDeleteCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount() + \" pendingDelCount=\" + rld.getPendingDeleteCount());\n            } else if (info.getDelCount() != 0) {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" delCount=\" + info.getDelCount());\n            } else {\n              infoStream.message(\"IW\", \"seg=\" + segString(info) + \" no deletes\");\n            }\n          }\n        }\n\n        // Deletes might have happened after we pulled the merge reader and\n        // before we got a read-only copy of the segment's actual live docs\n        // (taking pending deletes into account). In that case we need to\n        // make a new reader with updated live docs and del count.\n        if (reader.numDeletedDocs() != delCount) {\n          // fix the reader's live docs and del count\n          assert delCount > reader.numDeletedDocs(); // beware of zombies\n\n          SegmentReader newReader;\n\n          synchronized (this) {\n            // We must also sync on IW here, because another thread could be writing\n            // new DV updates / remove old gen field infos files causing FNFE:\n            newReader = new SegmentReader(info, reader, liveDocs, info.info.maxDoc() - delCount);\n          }\n\n          boolean released = false;\n          try {\n            rld.release(reader);\n            released = true;\n          } finally {\n            if (!released) {\n              newReader.decRef();\n            }\n          }\n\n          reader = newReader;\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n//      System.out.println(\"[\" + Thread.currentThread().getName() + \"] IW.mergeMiddle: merging \" + merge.getMergeReaders());\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      //System.out.println(\"merger set hasProx=\" + merger.hasProx() + \" seg=\" + merge.info.name);\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f0c93fedbecf591a0a34794b87e82ad8d5c754d9","date":1499287985,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e73d8d559120669b47658108d818b637df5456ea","date":1499401413,"type":3,"author":"Shalin Shekhar Mangar","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1926100d9b67becc9701c54266fee3ba7878a5f0","date":1524472150,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = readerPool.get(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (poolReaders && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = readerPool.get(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            readerPool.release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7","date":1524496660,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"36d84416fc00253f9e834f8dba14fa89b298e64e","date":1525428963,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        SegmentReader reader = rld.getReaderForMerge(context);\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":["f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aef2a94da918b657d107b616a643e1759db43b6a","date":1527706131,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n        int delCount = reader.numDeletedDocs();\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        assert delCount <= info.info.maxDoc(): \"delCount=\" + delCount + \" info.maxDoc=\" + info.info.maxDoc() + \" rld.pendingDeleteCount=\" + rld.getPendingDeleteCount() + \" info.getDelCount()=\" + info.getDelCount();\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8f2203cb8ae87188877cfbf6ad170c5738a0aad5","date":1528117512,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"f592209545c71895260367152601e9200399776d","date":1528238935,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b70042a8a492f7054d480ccdd2be9796510d4327","date":1528386658,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c652f4c2c731f462e41a528ed4f97245915206d5","date":1530728194,"type":3,"author":"Nhat Nguyen","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int softDeleteCount = 0;\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n            int hardDeleteCount = 0;\n            DocIdSetIterator softDeletedDocs = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader);\n            if (softDeletedDocs != null) {\n              int docId;\n              while ((docId = softDeletedDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (wrappedLiveDocs == null || wrappedLiveDocs.get(docId)) {\n                  if (hardLiveDocs == null || hardLiveDocs.get(docId)) {\n                    softDeleteCount++;\n                  } else {\n                    hardDeleteCount++;\n                  }\n                }\n              }\n            }\n            // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n            if (hardLiveDocs != null && hardDeleteCount > 0) {\n              Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                @Override\n                public boolean get(int index) {\n                  return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                }\n                @Override\n                public int length() {\n                  return hardLiveDocs.length();\n                }\n              };\n              wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(softDeleteCount);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","date":1531589977,"type":3,"author":"Michael Braun","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int softDeleteCount = 0;\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n            int hardDeleteCount = 0;\n            DocIdSetIterator softDeletedDocs = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader);\n            if (softDeletedDocs != null) {\n              int docId;\n              while ((docId = softDeletedDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (wrappedLiveDocs == null || wrappedLiveDocs.get(docId)) {\n                  if (hardLiveDocs == null || hardLiveDocs.get(docId)) {\n                    softDeleteCount++;\n                  } else {\n                    hardDeleteCount++;\n                  }\n                }\n              }\n            }\n            // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n            if (hardLiveDocs != null && hardDeleteCount > 0) {\n              Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                @Override\n                public boolean get(int index) {\n                  return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                }\n                @Override\n                public int length() {\n                  return hardLiveDocs.length();\n                }\n              };\n              wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(softDeleteCount);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","date":1531905561,"type":3,"author":"Alessandro Benedetti","isMerge":true,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int softDeleteCount = 0;\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n            int hardDeleteCount = 0;\n            DocIdSetIterator softDeletedDocs = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader);\n            if (softDeletedDocs != null) {\n              int docId;\n              while ((docId = softDeletedDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (wrappedLiveDocs == null || wrappedLiveDocs.get(docId)) {\n                  if (hardLiveDocs == null || hardLiveDocs.get(docId)) {\n                    softDeleteCount++;\n                  } else {\n                    hardDeleteCount++;\n                  }\n                }\n              }\n            }\n            // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n            if (hardLiveDocs != null && hardDeleteCount > 0) {\n              Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                @Override\n                public boolean get(int index) {\n                  return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                }\n                @Override\n                public int length() {\n                  return hardLiveDocs.length();\n                }\n              };\n              wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(softDeleteCount);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int numSoftDeleted = 0;\n      for (SegmentReader reader : merge.readers) {\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        mergeReaders.add(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits liveDocs = wrappedReader.getLiveDocs();\n            numSoftDeleted += PendingSoftDeletes.countSoftDeletes(\n                DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader),\n                liveDocs);\n          }\n        }\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(numSoftDeleted);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"2c773cafa43975ea73f6207cffced26cf284c998","date":1542806940,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      int softDeleteCount = 0;\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n            int hardDeleteCount = 0;\n            DocIdSetIterator softDeletedDocs = DocValuesFieldExistsQuery.getDocValuesDocIdSetIterator(config.getSoftDeletesField(), wrappedReader);\n            if (softDeletedDocs != null) {\n              int docId;\n              while ((docId = softDeletedDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                if (wrappedLiveDocs == null || wrappedLiveDocs.get(docId)) {\n                  if (hardLiveDocs == null || hardLiveDocs.get(docId)) {\n                    softDeleteCount++;\n                  } else {\n                    hardDeleteCount++;\n                  }\n                }\n              }\n            }\n            // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n            if (hardLiveDocs != null && hardDeleteCount > 0) {\n              Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                @Override\n                public boolean get(int index) {\n                  return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                }\n                @Override\n                public int length() {\n                  return hardLiveDocs.length();\n                }\n              };\n              wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(softDeleteCount);\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"737e811ac4583c640a0680e784121677f311a8af","date":1587558614,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d2178aefcd4690bd53785e9673e2c918cdb64165","date":1587583605,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = config.getMergeScheduler().wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2","date":1588002560,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n      Codec codec = config.getCodec();\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"102c0838edfd08e8eaf1ec7ad709a8db84f500ed","date":1592251335,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n      Codec codec = config.getCodec();\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        success = true;\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n      Codec codec = config.getCodec();\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"c2a23476693f2bd9a4b44cc3187c429a2e21dac2","date":1593289545,"type":3,"author":"Simon Willnauer","isMerge":false,"pathNew":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","pathOld":"lucene/core/src/java/org/apache/lucene/index/IndexWriter#mergeMiddle(MergePolicy.OneMerge,MergePolicy).mjava","sourceNew":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      merge.initMergeReaders(sci -> {\n        final ReadersAndUpdates rld = getPooledInstance(sci, true);\n        rld.setIsMerging();\n        return rld.getReaderForMerge(context);\n      });\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (MergePolicy.MergeReader mergeReader : merge.getMergeReader()) {\n        SegmentReader reader = mergeReader.reader;\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = mergeReader.hardLiveDocs;\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap,\n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n      Codec codec = config.getCodec();\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\",\n                e.getValue() / 1000000000.,\n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        success = true;\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost...\n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true, false);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","sourceOld":"  /** Does the actual (time-consuming) work of the merge,\n   *  but without holding synchronized lock on IndexWriter\n   *  instance */\n  private int mergeMiddle(MergePolicy.OneMerge merge, MergePolicy mergePolicy) throws IOException {\n    testPoint(\"mergeMiddleStart\");\n    merge.checkAborted();\n\n    Directory mergeDirectory = mergeScheduler.wrapForMerge(merge, directory);\n    List<SegmentCommitInfo> sourceSegments = merge.segments;\n    \n    IOContext context = new IOContext(merge.getStoreMergeInfo());\n\n    final TrackingDirectoryWrapper dirWrapper = new TrackingDirectoryWrapper(mergeDirectory);\n\n    if (infoStream.isEnabled(\"IW\")) {\n      infoStream.message(\"IW\", \"merging \" + segString(merge.segments));\n    }\n\n    merge.readers = new ArrayList<>(sourceSegments.size());\n    merge.hardLiveDocs = new ArrayList<>(sourceSegments.size());\n\n    // This is try/finally to make sure merger's readers are\n    // closed:\n    boolean success = false;\n    try {\n      int segUpto = 0;\n      while(segUpto < sourceSegments.size()) {\n\n        final SegmentCommitInfo info = sourceSegments.get(segUpto);\n\n        // Hold onto the \"live\" reader; we will use this to\n        // commit merged deletes\n        final ReadersAndUpdates rld = getPooledInstance(info, true);\n        rld.setIsMerging();\n\n        ReadersAndUpdates.MergeReader mr = rld.getReaderForMerge(context);\n        SegmentReader reader = mr.reader;\n\n        if (infoStream.isEnabled(\"IW\")) {\n          infoStream.message(\"IW\", \"seg=\" + segString(info) + \" reader=\" + reader);\n        }\n\n        merge.hardLiveDocs.add(mr.hardLiveDocs);\n        merge.readers.add(reader);\n        segUpto++;\n      }\n\n      // Let the merge wrap readers\n      List<CodecReader> mergeReaders = new ArrayList<>();\n      Counter softDeleteCount = Counter.newCounter(false);\n      for (int r = 0; r < merge.readers.size(); r++) {\n        SegmentReader reader = merge.readers.get(r);\n        CodecReader wrappedReader = merge.wrapForMerge(reader);\n        validateMergeReader(wrappedReader);\n        if (softDeletesEnabled) {\n          if (reader != wrappedReader) { // if we don't have a wrapped reader we won't preserve any soft-deletes\n            Bits hardLiveDocs = merge.hardLiveDocs.get(r);\n            if (hardLiveDocs != null) { // we only need to do this accounting if we have mixed deletes\n              Bits wrappedLiveDocs = wrappedReader.getLiveDocs();\n              Counter hardDeleteCounter = Counter.newCounter(false);\n              countSoftDeletes(wrappedReader, wrappedLiveDocs, hardLiveDocs, softDeleteCount, hardDeleteCounter);\n              int hardDeleteCount = Math.toIntExact(hardDeleteCounter.get());\n              // Wrap the wrapped reader again if we have excluded some hard-deleted docs\n              if (hardDeleteCount > 0) {\n                Bits liveDocs = wrappedLiveDocs == null ? hardLiveDocs : new Bits() {\n                  @Override\n                  public boolean get(int index) {\n                    return hardLiveDocs.get(index) && wrappedLiveDocs.get(index);\n                  }\n\n                  @Override\n                  public int length() {\n                    return hardLiveDocs.length();\n                  }\n                };\n                wrappedReader = FilterCodecReader.wrapLiveDocs(wrappedReader, liveDocs, wrappedReader.numDocs() - hardDeleteCount);\n              }\n            } else {\n              final int carryOverSoftDeletes = reader.getSegmentInfo().getSoftDelCount() - wrappedReader.numDeletedDocs();\n              assert carryOverSoftDeletes >= 0 : \"carry-over soft-deletes must be positive\";\n              assert assertSoftDeletesCount(wrappedReader, carryOverSoftDeletes);\n              softDeleteCount.addAndGet(carryOverSoftDeletes);\n            }\n          }\n        }\n        mergeReaders.add(wrappedReader);\n      }\n      final SegmentMerger merger = new SegmentMerger(mergeReaders,\n                                                     merge.info.info, infoStream, dirWrapper,\n                                                     globalFieldNumberMap, \n                                                     context);\n      merge.info.setSoftDelCount(Math.toIntExact(softDeleteCount.get()));\n      merge.checkAborted();\n\n      merge.mergeStartNS = System.nanoTime();\n\n      // This is where all the work happens:\n      if (merger.shouldMerge()) {\n        merger.merge();\n      }\n\n      MergeState mergeState = merger.mergeState;\n      assert mergeState.segmentInfo == merge.info.info;\n      merge.info.info.setFiles(new HashSet<>(dirWrapper.getCreatedFiles()));\n      Codec codec = config.getCodec();\n      if (infoStream.isEnabled(\"IW\")) {\n        if (merger.shouldMerge()) {\n          String pauseInfo = merge.getMergeProgress().getPauseTimes().entrySet()\n            .stream()\n            .filter((e) -> e.getValue() > 0)\n            .map((e) -> String.format(Locale.ROOT, \"%.1f sec %s\", \n                e.getValue() / 1000000000., \n                e.getKey().name().toLowerCase(Locale.ROOT)))\n            .collect(Collectors.joining(\", \"));\n          if (!pauseInfo.isEmpty()) {\n            pauseInfo = \" (\" + pauseInfo + \")\";\n          }\n\n          long t1 = System.nanoTime();\n          double sec = (t1-merge.mergeStartNS)/1000000000.;\n          double segmentMB = (merge.info.sizeInBytes()/1024./1024.);\n          infoStream.message(\"IW\", \"merge codec=\" + codec + \" maxDoc=\" + merge.info.info.maxDoc() + \"; merged segment has \" +\n                             (mergeState.mergeFieldInfos.hasVectors() ? \"vectors\" : \"no vectors\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasNorms() ? \"norms\" : \"no norms\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasDocValues() ? \"docValues\" : \"no docValues\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"prox\" : \"no prox\") + \"; \" + \n                             (mergeState.mergeFieldInfos.hasProx() ? \"freqs\" : \"no freqs\") + \"; \" +\n                             (mergeState.mergeFieldInfos.hasPointValues() ? \"points\" : \"no points\") + \"; \" +\n                             String.format(Locale.ROOT,\n                                           \"%.1f sec%s to merge segment [%.2f MB, %.2f MB/sec]\",\n                                           sec,\n                                           pauseInfo,\n                                           segmentMB,\n                                           segmentMB / sec));\n        } else {\n          infoStream.message(\"IW\", \"skip merging fully deleted segments\");\n        }\n      }\n\n      if (merger.shouldMerge() == false) {\n        // Merge would produce a 0-doc segment, so we do nothing except commit the merge to remove all the 0-doc segments that we \"merged\":\n        assert merge.info.info.maxDoc() == 0;\n        commitMerge(merge, mergeState);\n        success = true;\n        return 0;\n      }\n\n      assert merge.info.info.maxDoc() > 0;\n\n      // Very important to do this before opening the reader\n      // because codec must know if prox was written for\n      // this segment:\n      boolean useCompoundFile;\n      synchronized (this) { // Guard segmentInfos\n        useCompoundFile = mergePolicy.useCompoundFile(segmentInfos, merge.info, this);\n      }\n\n      if (useCompoundFile) {\n        success = false;\n\n        Collection<String> filesToRemove = merge.info.files();\n        TrackingDirectoryWrapper trackingCFSDir = new TrackingDirectoryWrapper(mergeDirectory);\n        try {\n          createCompoundFile(infoStream, trackingCFSDir, merge.info.info, context, this::deleteNewFiles);\n          success = true;\n        } catch (Throwable t) {\n          synchronized(this) {\n            if (merge.isAborted()) {\n              // This can happen if rollback is called while we were building\n              // our CFS -- fall through to logic below to remove the non-CFS\n              // merged files:\n              if (infoStream.isEnabled(\"IW\")) {\n                infoStream.message(\"IW\", \"hit merge abort exception creating compound file during merge\");\n              }\n              return 0;\n            } else {\n              handleMergeException(t, merge);\n            }\n          }\n        } finally {\n          if (success == false) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"hit exception creating compound file during merge\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n          }\n        }\n\n        // So that, if we hit exc in deleteNewFiles (next)\n        // or in commitMerge (later), we close the\n        // per-segment readers in the finally clause below:\n        success = false;\n\n        synchronized(this) {\n\n          // delete new non cfs files directly: they were never\n          // registered with IFD\n          deleteNewFiles(filesToRemove);\n\n          if (merge.isAborted()) {\n            if (infoStream.isEnabled(\"IW\")) {\n              infoStream.message(\"IW\", \"abort merge after building CFS\");\n            }\n            // Safe: these files must exist\n            deleteNewFiles(merge.info.files());\n            return 0;\n          }\n        }\n\n        merge.info.info.setUseCompoundFile(true);\n      } else {\n        // So that, if we hit exc in commitMerge (later),\n        // we close the per-segment readers in the finally\n        // clause below:\n        success = false;\n      }\n\n      // Have codec write SegmentInfo.  Must do this after\n      // creating CFS so that 1) .si isn't slurped into CFS,\n      // and 2) .si reflects useCompoundFile=true change\n      // above:\n      boolean success2 = false;\n      try {\n        codec.segmentInfoFormat().write(directory, merge.info.info, context);\n        success2 = true;\n      } finally {\n        if (!success2) {\n          // Safe: these files must exist\n          deleteNewFiles(merge.info.files());\n        }\n      }\n\n      // TODO: ideally we would freeze merge.info here!!\n      // because any changes after writing the .si will be\n      // lost... \n\n      if (infoStream.isEnabled(\"IW\")) {\n        infoStream.message(\"IW\", String.format(Locale.ROOT, \"merged segment size=%.3f MB vs estimate=%.3f MB\", merge.info.sizeInBytes()/1024./1024., merge.estimatedMergeBytes/1024/1024.));\n      }\n\n      final IndexReaderWarmer mergedSegmentWarmer = config.getMergedSegmentWarmer();\n      if (readerPool.isReaderPoolingEnabled() && mergedSegmentWarmer != null) {\n        final ReadersAndUpdates rld = getPooledInstance(merge.info, true);\n        final SegmentReader sr = rld.getReader(IOContext.READ);\n        try {\n          mergedSegmentWarmer.warm(sr);\n        } finally {\n          synchronized(this) {\n            rld.release(sr);\n            release(rld);\n          }\n        }\n      }\n\n      if (!commitMerge(merge, mergeState)) {\n        // commitMerge will return false if this merge was\n        // aborted\n        return 0;\n      }\n\n      success = true;\n\n    } finally {\n      // Readers are already closed in commitMerge if we didn't hit\n      // an exc:\n      if (success == false) {\n        closeMergeReaders(merge, true);\n      }\n    }\n\n    return merge.info.info.maxDoc();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"7599427f762eb1b4265584fd6e96521e4a1a4f3c":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"36d84416fc00253f9e834f8dba14fa89b298e64e":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"7277addaa5100a3b464703b0a0efb5a993ff5999":["aac05884852c2a15a6aa9153063de70dea4fbcae"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["1926100d9b67becc9701c54266fee3ba7878a5f0"],"da1c100e0e15178d145d7c3c8f38f3e553b92c12":["7277addaa5100a3b464703b0a0efb5a993ff5999"],"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa":["b0267c69e2456a3477a1ad785723f2135da3117e"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":["b70042a8a492f7054d480ccdd2be9796510d4327","c652f4c2c731f462e41a528ed4f97245915206d5"],"102c0838edfd08e8eaf1ec7ad709a8db84f500ed":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"b70042a8a492f7054d480ccdd2be9796510d4327":["aef2a94da918b657d107b616a643e1759db43b6a","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"c652f4c2c731f462e41a528ed4f97245915206d5":["8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"0c121ee1dd2daa7753dedf9b86de36146d3e9f27":["2131047ecceac64b54ba70feec3d26bbd7e483d7"],"b0267c69e2456a3477a1ad785723f2135da3117e":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"4cbcba1596953276043d89eca0af2ef0bd115c79":["0c121ee1dd2daa7753dedf9b86de36146d3e9f27"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["92212fd254551a0b1156aafc3a1a6ed1a43932ad","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"aef2a94da918b657d107b616a643e1759db43b6a":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"f592209545c71895260367152601e9200399776d":["aef2a94da918b657d107b616a643e1759db43b6a","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["e7b7defbb9b6dd128f30374dce48d25526e60f83"],"ee59f646cf24586a449cad77391a60a3ac8d8959":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"8985b5ad07c1b5bb28f504744862fc1c56d4c065":["a485550d032df41f9ff97f4d97d81e2be011d3ca"],"0ad30c6a479e764150a3316e57263319775f1df2":["da1c100e0e15178d145d7c3c8f38f3e553b92c12","3d33e731a93d4b57e662ff094f64f94a745422d4"],"737e811ac4583c640a0680e784121677f311a8af":["2c773cafa43975ea73f6207cffced26cf284c998"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["ee59f646cf24586a449cad77391a60a3ac8d8959"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["da1c100e0e15178d145d7c3c8f38f3e553b92c12","0ad30c6a479e764150a3316e57263319775f1df2"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","b0267c69e2456a3477a1ad785723f2135da3117e"],"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051":["9bb9a29a5e71a90295f175df8919802993142c9a"],"b06445ae1731e049327712db0454e5643ca9b7fe":["3c5705cb93fb3daa46c676cad08b916dd57bf1be","b0267c69e2456a3477a1ad785723f2135da3117e"],"9bb9a29a5e71a90295f175df8919802993142c9a":["c9fb5f46e264daf5ba3860defe623a89d202dd87","4cbcba1596953276043d89eca0af2ef0bd115c79"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b"],"2c773cafa43975ea73f6207cffced26cf284c998":["c652f4c2c731f462e41a528ed4f97245915206d5"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":["f592209545c71895260367152601e9200399776d","c652f4c2c731f462e41a528ed4f97245915206d5"],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["2c3c58609ce8cbaa9116c281d30aa3cdc6a87051"],"55980207f1977bd1463465de1659b821347e2fa8":["4cbcba1596953276043d89eca0af2ef0bd115c79","7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"1926100d9b67becc9701c54266fee3ba7878a5f0":["f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"28288370235ed02234a64753cdbf0c6ec096304a":["31741cf1390044e38a2ec3127cf302ba841bfd75","f4363cd33f6eff7fb4753574a441e2d18c1022a4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["da1c100e0e15178d145d7c3c8f38f3e553b92c12","d470c8182e92b264680e34081b75e70a9f2b3c89"],"a485550d032df41f9ff97f4d97d81e2be011d3ca":["7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"d2178aefcd4690bd53785e9673e2c918cdb64165":["737e811ac4583c640a0680e784121677f311a8af"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["102c0838edfd08e8eaf1ec7ad709a8db84f500ed"],"aac05884852c2a15a6aa9153063de70dea4fbcae":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["da1c100e0e15178d145d7c3c8f38f3e553b92c12"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["d2178aefcd4690bd53785e9673e2c918cdb64165"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"f0c93fedbecf591a0a34794b87e82ad8d5c754d9":["28288370235ed02234a64753cdbf0c6ec096304a"],"e73d8d559120669b47658108d818b637df5456ea":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"5faf65b6692f15cca0f87bf8666c87899afc619f":["8985b5ad07c1b5bb28f504744862fc1c56d4c065"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["aef2a94da918b657d107b616a643e1759db43b6a"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["da1c100e0e15178d145d7c3c8f38f3e553b92c12","ceaef6cfc68c8ab22a684192e469a8280f9e6e70"],"e7b7defbb9b6dd128f30374dce48d25526e60f83":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"]},"commit2Childs":{"7599427f762eb1b4265584fd6e96521e4a1a4f3c":["55980207f1977bd1463465de1659b821347e2fa8","a485550d032df41f9ff97f4d97d81e2be011d3ca"],"36d84416fc00253f9e834f8dba14fa89b298e64e":["aef2a94da918b657d107b616a643e1759db43b6a"],"7277addaa5100a3b464703b0a0efb5a993ff5999":["da1c100e0e15178d145d7c3c8f38f3e553b92c12"],"86a2e8a56b368d37ef3ba7180541fa317d6fd6c7":["36d84416fc00253f9e834f8dba14fa89b298e64e"],"da1c100e0e15178d145d7c3c8f38f3e553b92c12":["0ad30c6a479e764150a3316e57263319775f1df2","d470c8182e92b264680e34081b75e70a9f2b3c89","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","ceaef6cfc68c8ab22a684192e469a8280f9e6e70","3d33e731a93d4b57e662ff094f64f94a745422d4"],"d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa":["950882a2bd2a5f9dc16a154871584eaa643d882a"],"0efc9f2cae117418f13ba9035f5e1d516ea7a2b5":[],"102c0838edfd08e8eaf1ec7ad709a8db84f500ed":["c2a23476693f2bd9a4b44cc3187c429a2e21dac2"],"b70042a8a492f7054d480ccdd2be9796510d4327":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5"],"c652f4c2c731f462e41a528ed4f97245915206d5":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","2c773cafa43975ea73f6207cffced26cf284c998","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"0c121ee1dd2daa7753dedf9b86de36146d3e9f27":["4cbcba1596953276043d89eca0af2ef0bd115c79"],"b0267c69e2456a3477a1ad785723f2135da3117e":["d0f1e17ca60794d40e2f396dbf36ae3a83dde3aa","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"4cbcba1596953276043d89eca0af2ef0bd115c79":["9bb9a29a5e71a90295f175df8919802993142c9a","55980207f1977bd1463465de1659b821347e2fa8"],"b7dfa64bc2074fb87d0ca70095a644c1ead107e1":["e73d8d559120669b47658108d818b637df5456ea"],"aef2a94da918b657d107b616a643e1759db43b6a":["b70042a8a492f7054d480ccdd2be9796510d4327","f592209545c71895260367152601e9200399776d","8f2203cb8ae87188877cfbf6ad170c5738a0aad5"],"f592209545c71895260367152601e9200399776d":["7eeaaea0106c7d6a2de50acfc8d357121ef8bd26"],"3c5705cb93fb3daa46c676cad08b916dd57bf1be":["b0267c69e2456a3477a1ad785723f2135da3117e","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe"],"f4363cd33f6eff7fb4753574a441e2d18c1022a4":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1","28288370235ed02234a64753cdbf0c6ec096304a"],"ee59f646cf24586a449cad77391a60a3ac8d8959":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"c1ee9437ba5a8297220428d48a6bb823d1fcd57b":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"8985b5ad07c1b5bb28f504744862fc1c56d4c065":["5faf65b6692f15cca0f87bf8666c87899afc619f"],"0ad30c6a479e764150a3316e57263319775f1df2":["d470c8182e92b264680e34081b75e70a9f2b3c89"],"737e811ac4583c640a0680e784121677f311a8af":["d2178aefcd4690bd53785e9673e2c918cdb64165"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["9bb9a29a5e71a90295f175df8919802993142c9a","2131047ecceac64b54ba70feec3d26bbd7e483d7"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["f4363cd33f6eff7fb4753574a441e2d18c1022a4","28288370235ed02234a64753cdbf0c6ec096304a"],"d470c8182e92b264680e34081b75e70a9f2b3c89":["c1ee9437ba5a8297220428d48a6bb823d1fcd57b","4cce5816ef15a48a0bc11e5d400497ee4301dd3b"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"2c3c58609ce8cbaa9116c281d30aa3cdc6a87051":["e2fe60a17a7a0cfd101b1169acf089221bc6c166"],"b06445ae1731e049327712db0454e5643ca9b7fe":[],"9bb9a29a5e71a90295f175df8919802993142c9a":["2c3c58609ce8cbaa9116c281d30aa3cdc6a87051"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["ee59f646cf24586a449cad77391a60a3ac8d8959"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["b7dfa64bc2074fb87d0ca70095a644c1ead107e1"],"2c773cafa43975ea73f6207cffced26cf284c998":["737e811ac4583c640a0680e784121677f311a8af"],"7eeaaea0106c7d6a2de50acfc8d357121ef8bd26":[],"e2fe60a17a7a0cfd101b1169acf089221bc6c166":["7599427f762eb1b4265584fd6e96521e4a1a4f3c"],"55980207f1977bd1463465de1659b821347e2fa8":[],"1926100d9b67becc9701c54266fee3ba7878a5f0":["86a2e8a56b368d37ef3ba7180541fa317d6fd6c7"],"28288370235ed02234a64753cdbf0c6ec096304a":["f0c93fedbecf591a0a34794b87e82ad8d5c754d9"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"a485550d032df41f9ff97f4d97d81e2be011d3ca":["8985b5ad07c1b5bb28f504744862fc1c56d4c065"],"d2178aefcd4690bd53785e9673e2c918cdb64165":["8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2"],"c2a23476693f2bd9a4b44cc3187c429a2e21dac2":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"aac05884852c2a15a6aa9153063de70dea4fbcae":["7277addaa5100a3b464703b0a0efb5a993ff5999"],"ceaef6cfc68c8ab22a684192e469a8280f9e6e70":["3d33e731a93d4b57e662ff094f64f94a745422d4"],"8a428f5314daaabf8eab7c50bdc3bc14e6cd1aa2":["102c0838edfd08e8eaf1ec7ad709a8db84f500ed"],"2131047ecceac64b54ba70feec3d26bbd7e483d7":["0c121ee1dd2daa7753dedf9b86de36146d3e9f27"],"950882a2bd2a5f9dc16a154871584eaa643d882a":["aac05884852c2a15a6aa9153063de70dea4fbcae"],"f0c93fedbecf591a0a34794b87e82ad8d5c754d9":["1926100d9b67becc9701c54266fee3ba7878a5f0","e73d8d559120669b47658108d818b637df5456ea"],"e73d8d559120669b47658108d818b637df5456ea":[],"5faf65b6692f15cca0f87bf8666c87899afc619f":["e7b7defbb9b6dd128f30374dce48d25526e60f83"],"8f2203cb8ae87188877cfbf6ad170c5738a0aad5":["b70042a8a492f7054d480ccdd2be9796510d4327","c652f4c2c731f462e41a528ed4f97245915206d5","f592209545c71895260367152601e9200399776d"],"3d33e731a93d4b57e662ff094f64f94a745422d4":["0ad30c6a479e764150a3316e57263319775f1df2"],"e7b7defbb9b6dd128f30374dce48d25526e60f83":["3c5705cb93fb3daa46c676cad08b916dd57bf1be"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["0efc9f2cae117418f13ba9035f5e1d516ea7a2b5","a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","b06445ae1731e049327712db0454e5643ca9b7fe","7eeaaea0106c7d6a2de50acfc8d357121ef8bd26","55980207f1977bd1463465de1659b821347e2fa8","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","e73d8d559120669b47658108d818b637df5456ea","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}