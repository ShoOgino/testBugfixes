{"path":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","commits":[{"id":"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb","date":1185160645,"type":0,"author":"Grant Ingersoll","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"/dev/null","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0018e7a0579df5d3de71d0bd878322a7abef04d9","date":1202242049,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","date":1221082732,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e8d1458a2543cbd30cbfe7929be4dcb5c5251659","date":1254582241,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"0a046c0c310bc77931fc8441bd920053b607dd14","date":1254584734,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9","date":1256127131,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e450c7d50c2fc84c963d0d7ade9d3217d868064d","date":1259932067,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set set = (Set) map.get(\"termvector\");\n    for (Iterator iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry = (TermVectorEntry) iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6","date":1265808957,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"1cedb00d2dd44640194401179358a2e3ba6051bf","date":1268243626,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(TEST_VERSION_CURRENT));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e52fea2c4081a1e552b98506691990be59503168","date":1268250331,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(TEST_VERSION_CURRENT));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8","date":1268494368,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new StandardAnalyzer(TEST_VERSION_CURRENT), true, IndexWriter.MaxFieldLength.LIMITED);\n    // want to get some more segments here\n    // new termvector fields\n    for (int i = 0; i < 5 * writer.getMergeFactor(); i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n\n\n\n\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"42607aa380c892dc1ec0ab26e86a575c28e13618","date":1268641604,"type":3,"author":"Uwe Schindler","isMerge":false,"pathNew":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      if (VERBOSE) System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9454a6510e2db155fb01faa5c049b06ece95fab9","date":1453508333,"type":5,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","pathOld":"src/test/org/apache/lucene/index/TestIndexReader#testTermVectors().mjava","sourceNew":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      if (VERBOSE) System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","sourceOld":"  public void testTermVectors() throws Exception {\n    RAMDirectory d = new MockRAMDirectory();\n    // set up writer\n    IndexWriter writer = new IndexWriter(d, new IndexWriterConfig(\n        TEST_VERSION_CURRENT, new StandardAnalyzer(TEST_VERSION_CURRENT)));\n    // want to get some more segments here\n    // new termvector fields\n    int mergeFactor = ((LogMergePolicy) writer.getMergePolicy()).getMergeFactor();\n    for (int i = 0; i < 5 * mergeFactor; i++) {\n      Document doc = new Document();\n        doc.add(new Field(\"tvnot\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.NO));\n        doc.add(new Field(\"termvector\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.YES));\n        doc.add(new Field(\"tvoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_OFFSETS));\n        doc.add(new Field(\"tvposition\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS));\n        doc.add(new Field(\"tvpositionoffset\",\"one two two three three three\", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n\n        writer.addDocument(doc);\n    }\n    writer.close();\n    IndexReader reader = IndexReader.open(d, false);\n    FieldSortedTermVectorMapper mapper = new FieldSortedTermVectorMapper(new TermVectorEntryFreqSortedComparator());\n    reader.getTermFreqVector(0, mapper);\n    Map<String,SortedSet<TermVectorEntry>> map = mapper.getFieldToTerms();\n    assertTrue(\"map is null and it shouldn't be\", map != null);\n    assertTrue(\"map Size: \" + map.size() + \" is not: \" + 4, map.size() == 4);\n    Set<TermVectorEntry> set = map.get(\"termvector\");\n    for (Iterator<TermVectorEntry> iterator = set.iterator(); iterator.hasNext();) {\n      TermVectorEntry entry =  iterator.next();\n      assertTrue(\"entry is null and it shouldn't be\", entry != null);\n      if (VERBOSE) System.out.println(\"Entry: \" + entry);\n    }\n    \n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["e450c7d50c2fc84c963d0d7ade9d3217d868064d"],"0a046c0c310bc77931fc8441bd920053b607dd14":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"e52fea2c4081a1e552b98506691990be59503168":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"e450c7d50c2fc84c963d0d7ade9d3217d868064d":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["e52fea2c4081a1e552b98506691990be59503168"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["0a046c0c310bc77931fc8441bd920053b607dd14"],"42607aa380c892dc1ec0ab26e86a575c28e13618":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"9454a6510e2db155fb01faa5c049b06ece95fab9":["42607aa380c892dc1ec0ab26e86a575c28e13618"]},"commit2Childs":{"55f083e91bb056b57de136da1dfc3b9b6ecc4ef6":["1cedb00d2dd44640194401179358a2e3ba6051bf"],"0a046c0c310bc77931fc8441bd920053b607dd14":["4b41b991de69ba7b72d5e90cfcee25699a1a7fc9"],"1cedb00d2dd44640194401179358a2e3ba6051bf":["e52fea2c4081a1e552b98506691990be59503168"],"b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a":["0a046c0c310bc77931fc8441bd920053b607dd14","e8d1458a2543cbd30cbfe7929be4dcb5c5251659"],"0018e7a0579df5d3de71d0bd878322a7abef04d9":["b5015bd4c211c4f399ae66ee20fe6841ba5b0b6a"],"e52fea2c4081a1e552b98506691990be59503168":["84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8"],"e450c7d50c2fc84c963d0d7ade9d3217d868064d":["55f083e91bb056b57de136da1dfc3b9b6ecc4ef6"],"84080a7d7dbdaa3e6a4a1c9f1bb6221be40f47e8":["42607aa380c892dc1ec0ab26e86a575c28e13618"],"e8d1458a2543cbd30cbfe7929be4dcb5c5251659":["0a046c0c310bc77931fc8441bd920053b607dd14"],"352bfe1fae83b92d1562f01c057bfbe6f5af3ddb":["0018e7a0579df5d3de71d0bd878322a7abef04d9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["352bfe1fae83b92d1562f01c057bfbe6f5af3ddb"],"4b41b991de69ba7b72d5e90cfcee25699a1a7fc9":["e450c7d50c2fc84c963d0d7ade9d3217d868064d"],"42607aa380c892dc1ec0ab26e86a575c28e13618":["9454a6510e2db155fb01faa5c049b06ece95fab9"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"9454a6510e2db155fb01faa5c049b06ece95fab9":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}