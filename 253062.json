{"path":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","commits":[{"id":"b89678825b68eccaf09e6ab71675fc0b0af1e099","date":1334669779,"type":1,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"modules/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e9409b6a55f3d296d49614bc49405b834457f93e","date":1359015345,"type":3,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b2d5244a676b83c2d551c3746e8181588ba619e1","date":1359031414,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    while (it.next()) {\n      pq.insertWithReuse((int)(it.getDocID() * PHI_32) & 0x7FFFFFFF);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((IntPriorityQueue.MI)(heap[si+1])).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"607428da722dcb3e86bbd11c63de8986e6275c36","date":1360334150,"type":5,"author":"Shai Erera","isMerge":false,"pathNew":"lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","pathOld":"lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler#sample2(ScoredDocIDs,int,int[],long[]).mjava","sourceNew":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","sourceOld":"  /**\n   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>\n   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements\n   * are returned in <code>times</code>, which must be an array of at least three longs. The first\n   * will be set when the algorithm starts; the second, when a hash key has been calculated and\n   * inserted into the priority queue for every element in the collection; and the third when the\n   * original elements associated with the keys remaining in the PQ have been stored in the sample\n   * array for return.\n   * <P>\n   * This algorithm slows as the sample size becomes a significant fraction of the collection\n   * size, because the PQ is as large as the sample set, and will not do early rejection of values\n   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,\n   * resulting in less early rejection and more logN insertions.\n   * \n   * @param collection The set to be sampled.\n   * @param collectionSize The number of values to use (starting from first).\n   * @param sample The array in which to return the sample.\n   * @param times The times of three events, for measuring performance.\n   */\n  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) \n  throws IOException {\n    if (returnTimings) {\n      times[0] = System.currentTimeMillis();\n    }\n    int sampleSize = sample.length;\n    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);\n    /*\n     * Convert every value in the collection to a hashed \"weight\" value, and insert\n     * into a bounded PQ (retains only sampleSize highest weights).\n     */\n    ScoredDocIDsIterator it = collection.iterator();\n    MI mi = null;\n    while (it.next()) {\n      if (mi == null) {\n        mi = new MI();\n      }\n      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;\n      mi = pq.insertWithOverflow(mi);\n    }\n    if (returnTimings) {\n      times[1] = System.currentTimeMillis();\n    }\n    /*\n     * Extract heap, convert weights back to original values, and return as integers.\n     */\n    Object[] heap = pq.getHeap();\n    for (int si = 0; si < sampleSize; si++) {\n      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;\n    }\n    if (returnTimings) {\n      times[2] = System.currentTimeMillis();\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"607428da722dcb3e86bbd11c63de8986e6275c36":["e9409b6a55f3d296d49614bc49405b834457f93e"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"e9409b6a55f3d296d49614bc49405b834457f93e":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["607428da722dcb3e86bbd11c63de8986e6275c36"],"b2d5244a676b83c2d551c3746e8181588ba619e1":["b89678825b68eccaf09e6ab71675fc0b0af1e099","e9409b6a55f3d296d49614bc49405b834457f93e"]},"commit2Childs":{"607428da722dcb3e86bbd11c63de8986e6275c36":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"b89678825b68eccaf09e6ab71675fc0b0af1e099":["e9409b6a55f3d296d49614bc49405b834457f93e","b2d5244a676b83c2d551c3746e8181588ba619e1"],"e9409b6a55f3d296d49614bc49405b834457f93e":["607428da722dcb3e86bbd11c63de8986e6275c36","b2d5244a676b83c2d551c3746e8181588ba619e1"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["b89678825b68eccaf09e6ab71675fc0b0af1e099"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"b2d5244a676b83c2d551c3746e8181588ba619e1":[]},"heads":["cd5edd1f2b162a5cfa08efd17851a07373a96817","b2d5244a676b83c2d551c3746e8181588ba619e1"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}