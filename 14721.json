{"path":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","commits":[{"id":"0ce6c1f997c135ab2e3d211580d089de539d7e20","date":1421161966,"type":0,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"/dev/null","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    if (safeModeTesting) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197","c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c6d82c04c0bc088fae82f28ef47cb25a164f47fd","date":1422552161,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      FileSystem fs = FileSystem.newInstance(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = fs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n\n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    if (safeModeTesting) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"bugIntro":["24eba487e7dfe78969eb62b1667949fccc4a90b0"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197","date":1427044180,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      FileSystem fs = FileSystem.newInstance(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = fs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      FileSystem fs = FileSystem.newInstance(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = fs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"24eba487e7dfe78969eb62b1667949fccc4a90b0","date":1427044327,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      FileSystem fs = FileSystem.newInstance(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = fs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","date":1427779360,"type":3,"author":"Ryan Ernst","isMerge":true,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    \n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      FileSystem fs = FileSystem.newInstance(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = fs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a","date":1429888091,"type":3,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","pathOld":"solr/core/src/test/org/apache/solr/cloud/hdfs/HdfsTestUtil#setupClass(String,boolean).mjava","sourceNew":"  public static MiniDFSCluster setupClass(String dir, boolean haTesting) throws Exception {\n    return setupClass(dir, haTesting, true);\n  }\n\n","sourceOld":"  public static MiniDFSCluster setupClass(String dir, boolean safeModeTesting) throws Exception {\n    LuceneTestCase.assumeFalse(\"HDFS tests were disabled by -Dtests.disableHdfs\",\n      Boolean.parseBoolean(System.getProperty(\"tests.disableHdfs\", \"false\")));\n\n    savedLocale = Locale.getDefault();\n    // TODO: we HACK around HADOOP-9643\n    Locale.setDefault(Locale.ENGLISH);\n    \n    int dataNodes = 2;\n    \n    Configuration conf = new Configuration();\n    conf.set(\"dfs.block.access.token.enable\", \"false\");\n    conf.set(\"dfs.permissions.enabled\", \"false\");\n    conf.set(\"hadoop.security.authentication\", \"simple\");\n    conf.set(\"hdfs.minidfs.basedir\", dir + File.separator + \"hdfsBaseDir\");\n    conf.set(\"dfs.namenode.name.dir\", dir + File.separator + \"nameNodeNameDir\");\n    conf.setBoolean(\"fs.hdfs.impl.disable.cache\", true);\n    \n    System.setProperty(\"test.build.data\", dir + File.separator + \"hdfs\" + File.separator + \"build\");\n    System.setProperty(\"test.cache.data\", dir + File.separator + \"hdfs\" + File.separator + \"cache\");\n    System.setProperty(\"solr.lock.type\", \"hdfs\");\n    \n    \n    System.setProperty(\"solr.hdfs.blockcache.global\", Boolean.toString(LuceneTestCase.random().nextBoolean()));\n    \n    final MiniDFSCluster dfsCluster = new MiniDFSCluster(conf, dataNodes, true, null);\n    dfsCluster.waitActive();\n    \n    System.setProperty(\"solr.hdfs.home\", getDataDir(dfsCluster, \"solr_hdfs_home\"));\n    \n    int rndMode = LuceneTestCase.random().nextInt(10);\n    if (safeModeTesting && rndMode > 4) {\n      NameNodeAdapter.enterSafeMode(dfsCluster.getNameNode(), false);\n      \n      int rnd = LuceneTestCase.random().nextInt(10000);\n      Timer timer = new Timer();\n      timer.schedule(new TimerTask() {\n        \n        @Override\n        public void run() {\n          NameNodeAdapter.leaveSafeMode(dfsCluster.getNameNode());\n        }\n      }, rnd);\n      \n      timers.put(dfsCluster, timer);\n    } else {\n      // TODO: we could do much better at testing this\n      // force a lease recovery by creating a tlog file and not closing it\n      URI uri = dfsCluster.getURI();\n      Path hdfsDirPath = new Path(uri.toString() + \"/solr/collection1/core_node1/data/tlog/tlog.0000000000000000000\");\n      // tran log already being created testing\n      badTlogOutStreamFs = FileSystem.get(hdfsDirPath.toUri(), conf);\n      badTlogOutStream = badTlogOutStreamFs.create(hdfsDirPath);\n    }\n    \n    SolrTestCaseJ4.useFactory(\"org.apache.solr.core.HdfsDirectoryFactory\");\n    \n    return dfsCluster;\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"24eba487e7dfe78969eb62b1667949fccc4a90b0":["a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd","24eba487e7dfe78969eb62b1667949fccc4a90b0"],"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"c6d82c04c0bc088fae82f28ef47cb25a164f47fd":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["24eba487e7dfe78969eb62b1667949fccc4a90b0"]},"commit2Childs":{"24eba487e7dfe78969eb62b1667949fccc4a90b0":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","3def6e0e7b7566dd7f04a3514e77ee97a40fc78a"],"a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae":[],"a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197":["24eba487e7dfe78969eb62b1667949fccc4a90b0"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["0ce6c1f997c135ab2e3d211580d089de539d7e20"],"c6d82c04c0bc088fae82f28ef47cb25a164f47fd":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","a3bf63a4bdcd7c37d2bef84364a56ffdb8a51197"],"0ce6c1f997c135ab2e3d211580d089de539d7e20":["c6d82c04c0bc088fae82f28ef47cb25a164f47fd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[],"3def6e0e7b7566dd7f04a3514e77ee97a40fc78a":["cd5edd1f2b162a5cfa08efd17851a07373a96817"]},"heads":["a0d1e2aaf870d9d4f740ed0aaaf5824ccd9394ae","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}