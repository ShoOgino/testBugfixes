{"path":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","commits":[{"id":"3e0300d1df37d7e9662d491269e91b6f66dca8bd","date":1443011762,"type":1,"author":"Mark Robert Miller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,String,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).leaderZkNodeParentVersion;\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, String leaderCoreNodeName,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    String leaderSeqPath = getLeaderSeqPath(collection, leaderCoreNodeName);\n    if (leaderSeqPath == null) {\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR,\n          \"Failed to update data to 'down' for znode: \" + znodePath +\n              \" because the zookeeper leader sequence for leader: \" + leaderCoreNodeName + \" is null\");\n    }\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(leaderSeqPath, -1)); // version doesn't matter, the seq path is unique\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"7e29cfec8d8ad7e37d8e7a4d954e786b3249c857","date":1456507932,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).leaderZkNodeParentVersion;\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"af2638813028b254a88b418ebeafb541afb49653","date":1456804822,"type":3,"author":"Noble Paul","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).leaderZkNodeParentVersion;\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"f291d2d430e8149d24fdd06b0bcdab0941ec9144","date":1481216635,"type":3,"author":"markrmiller","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        // make sure we don't create /collections/{collection} if they do not exist with 2 param\n        zkClient.makePath(parentZNodePath, (byte[]) null, CreateMode.PERSISTENT, (Watcher) null, true, retryOnConnLoss, 2);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"9856095f7afb5a607bf5e65077615ed91273508c","date":1481837697,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        // make sure we don't create /collections/{collection} if they do not exist with 2 param\n        zkClient.makePath(parentZNodePath, (byte[]) null, CreateMode.PERSISTENT, (Watcher) null, true, retryOnConnLoss, 2);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        zkClient.makePath(parentZNodePath, retryOnConnLoss);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"84f20f331d8001864545c7021812d8c6509c7593","date":1517216128,"type":3,"author":"Cao Manh Dat","isMerge":false,"pathNew":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n\n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n\n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n\n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n\n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n\n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n\n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        // make sure we don't create /collections/{collection} if they do not exist with 2 param\n        zkClient.makePath(parentZNodePath, (byte[]) null, CreateMode.PERSISTENT, (Watcher) null, true, retryOnConnLoss, 2);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n\n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n    \n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n    \n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n    \n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n    \n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n    \n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        // make sure we don't create /collections/{collection} if they do not exist with 2 param\n        zkClient.makePath(parentZNodePath, (byte[]) null, CreateMode.PERSISTENT, (Watcher) null, true, retryOnConnLoss, 2);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n      \n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180","date":1539076849,"type":4,"author":"Cao Manh Dat","isMerge":false,"pathNew":"/dev/null","pathOld":"solr/core/src/java/org/apache/solr/cloud/ZkController#markShardAsDownIfLeader(String,String,CoreDescriptor,String,byte[],boolean).mjava","sourceNew":null,"sourceOld":"  /**\n   * we use ZK's multi-transactional semantics to ensure that we are able to\n   * publish a replica as 'down' only if our leader election node still exists\n   * in ZK. This ensures that a long running network partition caused by GC etc\n   * doesn't let us mark a node as down *after* we've already lost our session\n   */\n  private void markShardAsDownIfLeader(String collection, String shardId, CoreDescriptor leaderCd,\n                                       String znodePath, byte[] znodeData,\n                                       boolean retryOnConnLoss) throws KeeperException, InterruptedException {\n\n\n    if (!leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n\n    ContextKey key = new ContextKey(collection, leaderCd.getCloudDescriptor().getCoreNodeName());\n    ElectionContext context = electionContexts.get(key);\n\n    // we make sure we locally think we are the leader before and after getting the context - then\n    // we only try zk if we still think we are the leader and have our leader context\n    if (context == null || !leaderCd.getCloudDescriptor().isLeader()) {\n      log.info(\"No longer leader, aborting attempt to mark shard down as part of LIR\");\n      throw new NotLeaderException(ErrorCode.SERVER_ERROR, \"Locally, we do not think we are the leader.\");\n    }\n\n    // we think we are the leader - get the expected shard leader version\n    // we use this version and multi to ensure *only* the current zk registered leader\n    // for a shard can put a replica into LIR\n\n    Integer leaderZkNodeParentVersion = ((ShardLeaderElectionContextBase)context).getLeaderZkNodeParentVersion();\n\n    // TODO: should we do this optimistically to avoid races?\n    if (zkClient.exists(znodePath, retryOnConnLoss)) {\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.setData(znodePath, znodeData, -1));\n      zkClient.multi(ops, retryOnConnLoss);\n    } else {\n      String parentZNodePath = getLeaderInitiatedRecoveryZnodePath(collection, shardId);\n      try {\n        // make sure we don't create /collections/{collection} if they do not exist with 2 param\n        zkClient.makePath(parentZNodePath, (byte[]) null, CreateMode.PERSISTENT, (Watcher) null, true, retryOnConnLoss, 2);\n      } catch (KeeperException.NodeExistsException nee) {\n        // if it exists, that's great!\n      }\n\n      // we only create the entry if the context we are using is registered as the current leader in ZK\n      List<Op> ops = new ArrayList<>(2);\n      ops.add(Op.check(new org.apache.hadoop.fs.Path(((ShardLeaderElectionContextBase)context).leaderPath).getParent().toString(), leaderZkNodeParentVersion));\n      ops.add(Op.create(znodePath, znodeData, zkClient.getZkACLProvider().getACLsToAdd(znodePath),\n          CreateMode.PERSISTENT));\n      zkClient.multi(ops, retryOnConnLoss);\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"af2638813028b254a88b418ebeafb541afb49653":["3e0300d1df37d7e9662d491269e91b6f66dca8bd","7e29cfec8d8ad7e37d8e7a4d954e786b3249c857"],"7e29cfec8d8ad7e37d8e7a4d954e786b3249c857":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"f291d2d430e8149d24fdd06b0bcdab0941ec9144":["af2638813028b254a88b418ebeafb541afb49653"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"84f20f331d8001864545c7021812d8c6509c7593":["f291d2d430e8149d24fdd06b0bcdab0941ec9144"],"9856095f7afb5a607bf5e65077615ed91273508c":["af2638813028b254a88b418ebeafb541afb49653","f291d2d430e8149d24fdd06b0bcdab0941ec9144"],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["84f20f331d8001864545c7021812d8c6509c7593"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"]},"commit2Childs":{"af2638813028b254a88b418ebeafb541afb49653":["f291d2d430e8149d24fdd06b0bcdab0941ec9144","9856095f7afb5a607bf5e65077615ed91273508c"],"7e29cfec8d8ad7e37d8e7a4d954e786b3249c857":["af2638813028b254a88b418ebeafb541afb49653"],"f291d2d430e8149d24fdd06b0bcdab0941ec9144":["84f20f331d8001864545c7021812d8c6509c7593","9856095f7afb5a607bf5e65077615ed91273508c"],"3e0300d1df37d7e9662d491269e91b6f66dca8bd":["af2638813028b254a88b418ebeafb541afb49653","7e29cfec8d8ad7e37d8e7a4d954e786b3249c857"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3e0300d1df37d7e9662d491269e91b6f66dca8bd"],"84f20f331d8001864545c7021812d8c6509c7593":["b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180"],"9856095f7afb5a607bf5e65077615ed91273508c":[],"b6d72c72ee67b4aa8bc8bdd91bae9069b04fc180":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["9856095f7afb5a607bf5e65077615ed91273508c","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}