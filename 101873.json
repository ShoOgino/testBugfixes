{"path":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","commits":[{"id":"d943163030bbd7a9caf93cb5fea92257390a2a99","date":1403094254,"type":0,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"/dev/null","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    for (AtomicReaderContext context : reader.leaves()) {\n      final AtomicReader r = context.reader();\n      // beware of lazy-loaded stuff\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(r);\n      final long actualBytes = RamUsageTester.sizeOf(r, new Accumulator(r));\n      final long expectedBytes = ((SegmentReader) r).ramBytesUsed();\n      final long absoluteError = actualBytes - expectedBytes;\n      final double relativeError = (double) absoluteError / actualBytes;\n      final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + relativeError + \"% error\";\n      assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"e57c73924f3b8c19defa62e96bfa34a4922d49c2","date":1403106358,"type":0,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"/dev/null","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    for (AtomicReaderContext context : reader.leaves()) {\n      final AtomicReader r = context.reader();\n      // beware of lazy-loaded stuff\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(r);\n      final long actualBytes = RamUsageTester.sizeOf(r, new Accumulator(r));\n      final long expectedBytes = ((SegmentReader) r).ramBytesUsed();\n      final long absoluteError = actualBytes - expectedBytes;\n      final double relativeError = (double) absoluteError / actualBytes;\n      final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + relativeError + \"% error\";\n      assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"67074e5e2c6dcb1baf22fb8a66841d1e2b900072","date":1403118934,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    for (AtomicReaderContext context : reader.leaves()) {\n      final AtomicReader r = context.reader();\n      // beware of lazy-loaded stuff\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(r);\n      final long actualBytes = RamUsageTester.sizeOf(r, new Accumulator(r));\n      final long expectedBytes = ((SegmentReader) r).ramBytesUsed();\n      final long absoluteError = actualBytes - expectedBytes;\n      final double relativeError = (double) absoluteError / actualBytes;\n      final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n      assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    for (AtomicReaderContext context : reader.leaves()) {\n      final AtomicReader r = context.reader();\n      // beware of lazy-loaded stuff\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(r);\n      final long actualBytes = RamUsageTester.sizeOf(r, new Accumulator(r));\n      final long expectedBytes = ((SegmentReader) r).ramBytesUsed();\n      final long absoluteError = actualBytes - expectedBytes;\n      final double relativeError = (double) absoluteError / actualBytes;\n      final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + relativeError + \"% error\";\n      assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"282412d91b33b1a7f2efd2610399fe03045d7d2c","date":1403134999,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    AtomicReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    AtomicReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (AtomicReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    IndexReader reader = DirectoryReader.open(dir);\n\n    for (AtomicReaderContext context : reader.leaves()) {\n      final AtomicReader r = context.reader();\n      // beware of lazy-loaded stuff\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(r);\n      final long actualBytes = RamUsageTester.sizeOf(r, new Accumulator(r));\n      final long expectedBytes = ((SegmentReader) r).ramBytesUsed();\n      final long absoluteError = actualBytes - expectedBytes;\n      final double relativeError = (double) absoluteError / actualBytes;\n      final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n      assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n    }\n\n    reader.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","8913cb9a44846cb65e41bee682ba0372f1636056"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    AtomicReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    AtomicReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (AtomicReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    AtomicReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    AtomicReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (AtomicReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"c9fb5f46e264daf5ba3860defe623a89d202dd87","date":1411516315,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    AtomicReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    AtomicReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (AtomicReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"3a135438c33df9bffb763f5e15d4ac63251107fd","date":1417049132,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1","date":1457644139,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlySegmentReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlySegmentReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":["c9fb5f46e264daf5ba3860defe623a89d202dd87","282412d91b33b1a7f2efd2610399fe03045d7d2c"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"31741cf1390044e38a2ec3127cf302ba841bfd75","date":1491292636,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"92212fd254551a0b1156aafc3a1a6ed1a43932ad","date":1491296431,"type":3,"author":"Adrien Grand","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = newDirectory();\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8913cb9a44846cb65e41bee682ba0372f1636056","date":1547808953,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    long act1 = RamUsageTester.sizeOf(reader2, new Accumulator(reader2));\n    long act2 = RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long measuredBytes = act1 - act2;\n\n    long reported1 = ((SegmentReader) reader2).ramBytesUsed();\n    long reported2 = ((SegmentReader) reader1).ramBytesUsed();\n    final long reportedBytes = reported1 - reported2;\n\n    final long absoluteError = Math.abs(measuredBytes - reportedBytes);\n    final double relativeError = (double) absoluteError / measuredBytes;\n    final String message = String.format(Locale.ROOT,\n        \"RamUsageTester reports %d bytes but ramBytesUsed() returned %d (%.1f error). \" +\n        \" [Measured: %d, %d. Reported: %d, %d]\",\n        measuredBytes,\n        reportedBytes,\n        (100 * relativeError),\n        act1, act2,\n        reported1, reported2);\n\n    assertTrue(message, relativeError < 0.20d || absoluteError < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    final long actualBytes = RamUsageTester.sizeOf(reader2, new Accumulator(reader2)) - RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long expectedBytes = ((SegmentReader) reader2).ramBytesUsed() - ((SegmentReader) reader1).ramBytesUsed();\n    final long absoluteError = actualBytes - expectedBytes;\n    final double relativeError = (double) absoluteError / actualBytes;\n    final String message = \"Actual RAM usage \" + actualBytes + \", but got \" + expectedBytes + \", \" + 100*relativeError + \"% error\";\n    assertTrue(message, Math.abs(relativeError) < 0.20d || Math.abs(absoluteError) < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":["282412d91b33b1a7f2efd2610399fe03045d7d2c"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f","date":1579652839,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","pathOld":"lucene/test-framework/src/java/org/apache/lucene/index/BaseIndexFileFormatTestCase#testRamBytesUsed().mjava","sourceNew":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Nightly\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    long act1 = RamUsageTester.sizeOf(reader2, new Accumulator(reader2));\n    long act2 = RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long measuredBytes = act1 - act2;\n\n    long reported1 = ((SegmentReader) reader2).ramBytesUsed();\n    long reported2 = ((SegmentReader) reader1).ramBytesUsed();\n    final long reportedBytes = reported1 - reported2;\n\n    final long absoluteError = Math.abs(measuredBytes - reportedBytes);\n    final double relativeError = (double) absoluteError / measuredBytes;\n    final String message = String.format(Locale.ROOT,\n        \"RamUsageTester reports %d bytes but ramBytesUsed() returned %d (%.1f error). \" +\n        \" [Measured: %d, %d. Reported: %d, %d]\",\n        measuredBytes,\n        reportedBytes,\n        (100 * relativeError),\n        act1, act2,\n        reported1, reported2);\n\n    assertTrue(message, relativeError < 0.20d || absoluteError < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","sourceOld":"  /** Test the accuracy of the ramBytesUsed estimations. */\n  @Slow\n  public void testRamBytesUsed() throws IOException {\n    if (Codec.getDefault() instanceof RandomCodec) {\n      // this test relies on the fact that two segments will be written with\n      // the same codec so we need to disable MockRandomPF\n      final Set<String> avoidCodecs = new HashSet<>(((RandomCodec) Codec.getDefault()).avoidCodecs);\n      avoidCodecs.add(new MockRandomPostingsFormat().getName());\n      Codec.setDefault(new RandomCodec(random(), avoidCodecs));\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    IndexWriterConfig cfg = newIndexWriterConfig(new MockAnalyzer(random()));\n    IndexWriter w = new IndexWriter(dir, cfg);\n    // we need to index enough documents so that constant overhead doesn't dominate\n    final int numDocs = atLeast(10000);\n    LeafReader reader1 = null;\n    for (int i = 0; i < numDocs; ++i) {\n      Document d = new Document();\n      addRandomFields(d);\n      w.addDocument(d);\n      if (i == 100) {\n        w.forceMerge(1);\n        w.commit();\n        reader1 = getOnlyLeafReader(DirectoryReader.open(dir));\n      }\n    }\n    w.forceMerge(1);\n    w.commit();\n    w.close();\n\n    LeafReader reader2 = getOnlyLeafReader(DirectoryReader.open(dir));\n\n    for (LeafReader reader : Arrays.asList(reader1, reader2)) {\n      new SimpleMergedSegmentWarmer(InfoStream.NO_OUTPUT).warm(reader);\n    }\n\n    long act1 = RamUsageTester.sizeOf(reader2, new Accumulator(reader2));\n    long act2 = RamUsageTester.sizeOf(reader1, new Accumulator(reader1));\n    final long measuredBytes = act1 - act2;\n\n    long reported1 = ((SegmentReader) reader2).ramBytesUsed();\n    long reported2 = ((SegmentReader) reader1).ramBytesUsed();\n    final long reportedBytes = reported1 - reported2;\n\n    final long absoluteError = Math.abs(measuredBytes - reportedBytes);\n    final double relativeError = (double) absoluteError / measuredBytes;\n    final String message = String.format(Locale.ROOT,\n        \"RamUsageTester reports %d bytes but ramBytesUsed() returned %d (%.1f error). \" +\n        \" [Measured: %d, %d. Reported: %d, %d]\",\n        measuredBytes,\n        reportedBytes,\n        (100 * relativeError),\n        act1, act2,\n        reported1, reported2);\n\n    assertTrue(message, relativeError < 0.20d || absoluteError < 1000);\n\n    reader1.close();\n    reader2.close();\n    dir.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["8913cb9a44846cb65e41bee682ba0372f1636056"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["282412d91b33b1a7f2efd2610399fe03045d7d2c"],"e57c73924f3b8c19defa62e96bfa34a4922d49c2":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","d943163030bbd7a9caf93cb5fea92257390a2a99"],"d943163030bbd7a9caf93cb5fea92257390a2a99":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"67074e5e2c6dcb1baf22fb8a66841d1e2b900072":["d943163030bbd7a9caf93cb5fea92257390a2a99"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["3a135438c33df9bffb763f5e15d4ac63251107fd"],"282412d91b33b1a7f2efd2610399fe03045d7d2c":["67074e5e2c6dcb1baf22fb8a66841d1e2b900072"],"8913cb9a44846cb65e41bee682ba0372f1636056":["31741cf1390044e38a2ec3127cf302ba841bfd75"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"3a135438c33df9bffb763f5e15d4ac63251107fd":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"]},"commit2Childs":{"fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["c9fb5f46e264daf5ba3860defe623a89d202dd87"],"e57c73924f3b8c19defa62e96bfa34a4922d49c2":[],"d943163030bbd7a9caf93cb5fea92257390a2a99":["e57c73924f3b8c19defa62e96bfa34a4922d49c2","67074e5e2c6dcb1baf22fb8a66841d1e2b900072"],"c9fb5f46e264daf5ba3860defe623a89d202dd87":["3a135438c33df9bffb763f5e15d4ac63251107fd"],"67074e5e2c6dcb1baf22fb8a66841d1e2b900072":["282412d91b33b1a7f2efd2610399fe03045d7d2c"],"31741cf1390044e38a2ec3127cf302ba841bfd75":["8913cb9a44846cb65e41bee682ba0372f1636056"],"0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1":["31741cf1390044e38a2ec3127cf302ba841bfd75","92212fd254551a0b1156aafc3a1a6ed1a43932ad"],"282412d91b33b1a7f2efd2610399fe03045d7d2c":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"8913cb9a44846cb65e41bee682ba0372f1636056":["fc1e9ddca40a3ddf8b097f2cf1fe2547fe8e384f"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["e57c73924f3b8c19defa62e96bfa34a4922d49c2","d943163030bbd7a9caf93cb5fea92257390a2a99"],"92212fd254551a0b1156aafc3a1a6ed1a43932ad":[],"3a135438c33df9bffb763f5e15d4ac63251107fd":["0bdb67d0b49ddf963c3bfc4975fce171ad3aacb1"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["e57c73924f3b8c19defa62e96bfa34a4922d49c2","92212fd254551a0b1156aafc3a1a6ed1a43932ad","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}