{"path":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","commits":[{"id":"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1","date":1342716838,"type":0,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // nocommit\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // nocommit not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // nocommit: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d","date":1343058759,"type":0,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":["d4d69c535930b5cce125cff868d40f6373dc27d4"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"094f0d273d15943ff2daa367b891b16c672f66f1","date":1343063629,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // nocommit\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // nocommit not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // nocommit: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":0,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"/dev/null","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":null,"bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"cd44770dc3c8ff9605dcfc09f5704aefffdaa5cb","date":1343700944,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        // TODO: reduce the ram usage of this test so we can safely do this\n        // numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"8354bffbd37db6037b531bbf6eafa728e19f0b7a","date":1343733434,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        // TODO: reduce the ram usage of this test so we can safely do this\n        // numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","date":1343768312,"type":3,"author":"Michael McCandless","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d6f074e73200c07d54f242d3880a8da5a35ff97b","date":1344507653,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 3) {\n          // 10% of the time make a highish freq term:\n          numDocs = _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 10) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = atLeast(3000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else {\n          // Low freq term:\n          numDocs = _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        }\n\n        numDocs *= RANDOM_MULTIPLIER;\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"8649cef98eb6b4eda0113896873934b1dcd745bb","date":1346856421,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"95323da8eca89d45766013f5b300a865a5ac7dfb","date":1348933777,"type":5,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/test-framework/src/java/org/apache/lucene/index/BasePostingsFormatTestCase#createPostings().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestPostingsFormat#createPostings().mjava","sourceNew":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","sourceOld":"  @BeforeClass\n  public static void createPostings() throws IOException {\n\n    final int numFields = _TestUtil.nextInt(random(), 1, 5);\n    if (VERBOSE) {\n      System.out.println(\"TEST: \" + numFields + \" fields\");\n    }\n\n    FieldInfo[] fieldInfoArray = new FieldInfo[numFields];\n    int fieldUpto = 0;\n    int numMediumTerms = 0;\n    int numBigTerms = 0;\n    int numManyPositions = 0;\n    totalPostings = 0;\n    totalPayloadBytes = 0;\n    while (fieldUpto < numFields) {\n      String field = _TestUtil.randomSimpleString(random());\n      if (fields.containsKey(field)) {\n        continue;\n      }\n\n      boolean fieldHasPayloads = random().nextBoolean();\n\n      fieldInfoArray[fieldUpto] = new FieldInfo(field, true, fieldUpto, false, false, fieldHasPayloads,\n                                                IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,\n                                                null, DocValues.Type.FIXED_INTS_8, null);\n      fieldUpto++;\n\n      Map<BytesRef,List<Posting>> postings = new TreeMap<BytesRef,List<Posting>>();\n      fields.put(field, postings);\n      Set<String> seenTerms = new HashSet<String>();\n\n      // TODO\n      //final int numTerms = atLeast(10);\n      final int numTerms = 4;\n      for(int termUpto=0;termUpto<numTerms;termUpto++) {\n        String term = _TestUtil.randomSimpleString(random());\n        if (seenTerms.contains(term)) {\n          continue;\n        }\n        seenTerms.add(term);\n\n        int numDocs;\n        if (random().nextInt(10) == 3 && numBigTerms < 2) {\n          // 10% of the time make a highish freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 50000, 70000);\n          numBigTerms++;\n          term = \"big_\" + term;\n        } else if (random().nextInt(10) == 3 && numMediumTerms < 5) {\n          // 10% of the time make a medium freq term:\n          // TODO not high enough to test level 1 skipping:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 3000, 6000);\n          numMediumTerms++;\n          term = \"medium_\" + term;\n        } else if (random().nextBoolean()) {\n          // Low freq term:\n          numDocs = RANDOM_MULTIPLIER * _TestUtil.nextInt(random(), 1, 40);\n          term = \"low_\" + term;\n        } else {\n          // Very low freq term (don't multiply by RANDOM_MULTIPLIER):\n          numDocs = _TestUtil.nextInt(random(), 1, 3);\n          term = \"verylow_\" + term;\n        }\n\n        List<Posting> termPostings = new ArrayList<Posting>();\n        postings.put(new BytesRef(term), termPostings);\n\n        int docID = 0;\n\n        // TODO: more realistic to inversely tie this to numDocs:\n        int maxDocSpacing = _TestUtil.nextInt(random(), 1, 100);\n\n        // 10% of the time create big payloads:\n        int payloadSize;\n        if (!fieldHasPayloads) {\n          payloadSize = 0;\n        } else if (random().nextInt(10) == 7) {\n          payloadSize = random().nextInt(50);\n        } else {\n          payloadSize = random().nextInt(10);\n        }\n\n        boolean fixedPayloads = random().nextBoolean();\n\n        for(int docUpto=0;docUpto<numDocs;docUpto++) {\n          if (docUpto == 0 && random().nextBoolean()) {\n            // Sometimes index docID = 0\n          } else if (maxDocSpacing == 1) {\n            docID++;\n          } else {\n            // TODO: sometimes have a biggish gap here!\n            docID += _TestUtil.nextInt(random(), 1, maxDocSpacing);\n          }\n\n          Posting posting = new Posting();\n          posting.docID = docID;\n          maxDocID = Math.max(docID, maxDocID);\n          posting.positions = new ArrayList<Position>();\n          termPostings.add(posting);\n\n          int freq;\n          if (random().nextInt(30) == 17 && numManyPositions < 10) {\n            freq = _TestUtil.nextInt(random(), 1, 1000);\n            numManyPositions++;\n          } else {\n            freq = _TestUtil.nextInt(random(), 1, 20);\n          }\n          int pos = 0;\n          int offset = 0;\n          int posSpacing = _TestUtil.nextInt(random(), 1, 100);\n          totalPostings += freq;\n          for(int posUpto=0;posUpto<freq;posUpto++) {\n            if (posUpto == 0 && random().nextBoolean()) {\n              // Sometimes index pos = 0\n            } else if (posSpacing == 1) {\n              pos++;\n            } else {\n              pos += _TestUtil.nextInt(random(), 1, posSpacing);\n            }\n\n            Position position = new Position();\n            posting.positions.add(position);\n            position.position = pos;\n            if (payloadSize != 0) {\n              if (fixedPayloads) {\n                position.payload = new byte[payloadSize];\n              } else {\n                int thisPayloadSize = random().nextInt(payloadSize);\n                if (thisPayloadSize != 0) {\n                  position.payload = new byte[thisPayloadSize];\n                }\n              }\n            }\n\n            if (position.payload != null) {\n              random().nextBytes(position.payload); \n              totalPayloadBytes += position.payload.length;\n            }\n\n            position.startOffset = offset + random().nextInt(5);\n            position.endOffset = position.startOffset + random().nextInt(10);\n            offset = position.endOffset;\n          }\n        }\n      }\n    }\n\n    fieldInfos = new FieldInfos(fieldInfoArray);\n\n    globalLiveDocs = new FixedBitSet(1+maxDocID);\n    double liveRatio = random().nextDouble();\n    for(int i=0;i<1+maxDocID;i++) {\n      if (random().nextDouble() <= liveRatio) {\n        globalLiveDocs.set(i);\n      }\n    }\n\n    // Pre-filter postings by globalLiveDocs:\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      Map<BytesRef,List<Posting>> postingsLive = new TreeMap<BytesRef,List<Posting>>();\n      fieldsLive.put(fieldEnt.getKey(), postingsLive);\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        List<Posting> termPostingsLive = new ArrayList<Posting>();\n        postingsLive.put(termEnt.getKey(), termPostingsLive);\n        for(Posting posting : termEnt.getValue()) {\n          if (globalLiveDocs.get(posting.docID)) {\n            termPostingsLive.add(posting);\n          }\n        }\n      }\n    }\n\n    allTerms = new ArrayList<FieldAndTerm>();\n    for(Map.Entry<String,Map<BytesRef,List<Posting>>> fieldEnt : fields.entrySet()) {\n      String field = fieldEnt.getKey();\n      for(Map.Entry<BytesRef,List<Posting>> termEnt : fieldEnt.getValue().entrySet()) {\n        allTerms.add(new FieldAndTerm(field, termEnt.getKey()));\n      }\n    }\n\n    if (VERBOSE) {\n      System.out.println(\"TEST: done init postings; maxDocID=\" + maxDocID + \"; \" + allTerms.size() + \" total terms, across \" + fieldInfos.size() + \" fields\");\n    }\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null}],"commit2Parents":{"094f0d273d15943ff2daa367b891b16c672f66f1":["e885d2b1e112b1d9db6a2dae82b3b493dfba1df1","ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"aba371508186796cc6151d8223a5b4e16d02e26e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85","ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["8649cef98eb6b4eda0113896873934b1dcd745bb"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":["094f0d273d15943ff2daa367b891b16c672f66f1","8354bffbd37db6037b531bbf6eafa728e19f0b7a"],"cd44770dc3c8ff9605dcfc09f5704aefffdaa5cb":["ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":["aba371508186796cc6151d8223a5b4e16d02e26e","8354bffbd37db6037b531bbf6eafa728e19f0b7a"],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"8649cef98eb6b4eda0113896873934b1dcd745bb":["8354bffbd37db6037b531bbf6eafa728e19f0b7a"],"8354bffbd37db6037b531bbf6eafa728e19f0b7a":["cd44770dc3c8ff9605dcfc09f5704aefffdaa5cb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["95323da8eca89d45766013f5b300a865a5ac7dfb"]},"commit2Childs":{"094f0d273d15943ff2daa367b891b16c672f66f1":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f"],"aba371508186796cc6151d8223a5b4e16d02e26e":["d6f074e73200c07d54f242d3880a8da5a35ff97b"],"ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d":["094f0d273d15943ff2daa367b891b16c672f66f1","aba371508186796cc6151d8223a5b4e16d02e26e","cd44770dc3c8ff9605dcfc09f5704aefffdaa5cb"],"95323da8eca89d45766013f5b300a865a5ac7dfb":["cd5edd1f2b162a5cfa08efd17851a07373a96817"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["aba371508186796cc6151d8223a5b4e16d02e26e","ddf662c415c0d0ad543e5314fcdf8396cd2f1b8d","e885d2b1e112b1d9db6a2dae82b3b493dfba1df1"],"b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f":[],"cd44770dc3c8ff9605dcfc09f5704aefffdaa5cb":["8354bffbd37db6037b531bbf6eafa728e19f0b7a"],"d6f074e73200c07d54f242d3880a8da5a35ff97b":[],"e885d2b1e112b1d9db6a2dae82b3b493dfba1df1":["094f0d273d15943ff2daa367b891b16c672f66f1"],"8649cef98eb6b4eda0113896873934b1dcd745bb":["95323da8eca89d45766013f5b300a865a5ac7dfb"],"8354bffbd37db6037b531bbf6eafa728e19f0b7a":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","8649cef98eb6b4eda0113896873934b1dcd745bb"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["b7cbfe9a112ef62d75f3289e4c79bbe274cb2a4f","d6f074e73200c07d54f242d3880a8da5a35ff97b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}