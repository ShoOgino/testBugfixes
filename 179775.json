{"path":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","commits":[{"id":"3a119bbc8703c10faa329ec201c654b3a35a1e3e","date":1328644745,"type":1,"author":"Steven Rowe","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"a78a90fc9701e511308346ea29f4f5e548bb39fe","date":1329489995,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":["87c966e9308847938a7c905c2e46a56d8df788b8"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"629c38c4ae4e303d0617e05fbfe508140b32f0a3","date":1334500904,"type":3,"author":"Dawid Weiss","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random)).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f","date":1338430031,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = IndexReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"04f07771a2a7dd3a395700665ed839c3dae2def2","date":1339350139,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newField(\"id\", \"\", StringField.TYPE_STORED);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":["1509f151d7692d84fae414b2b799ac06ba60fcb4"],"bugIntro":[],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"d19974432be9aed28ee7dca73bdf01d139e763a9","date":1342822166,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":["1f653cfcf159baeaafe5d01682a911e95bba4012"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","date":1343059585,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"aba371508186796cc6151d8223a5b4e16d02e26e","date":1343474871,"type":3,"author":"Uwe Schindler","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    MockDirectoryWrapper directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","date":1379624229,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100))\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"ae14298f4eec6d5faee6a149f88ba57d14a6f21a","date":1396971290,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.shutdown(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.shutdown();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.close(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e","date":1406737224,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.shutdown(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.shutdown();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.shutdown(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.shutdown();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"d0ef034a4f10871667ae75181537775ddcf8ade4","date":1407610475,"type":3,"author":"Ryan Ernst","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100))\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      writer.shutdown(false);\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.shutdown();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"431452b6326a9c17ba5bb1e1a6d89e23a8932e73","date":1417113370,"type":3,"author":"Robert Muir","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    for(int iter=0;iter<10;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"32e605e499439692cdaf766a6b4255a4c97bb9dd","date":1427792768,"type":3,"author":"Michael McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    if (directory instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper) directory).setPreventDoubleWrite(false);\n    }\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"isBuggy":true,"nexts":[],"revCommit":null},{"id":"fab172655716b96f7e42376116235017a922de3a","date":1427850611,"type":3,"author":"Robert Muir","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    if (directory instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper) directory).setPreventDoubleWrite(false);\n    }\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n      writer.commit();\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"11c6df42fb3eba174c3ca0d9a5194eaecd893b77","date":1465931757,"type":3,"author":"Mike McCandless","isMerge":false,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    if (directory instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper) directory).setPreventDoubleWrite(false);\n    }\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":["32e605e499439692cdaf766a6b4255a4c97bb9dd"],"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null},{"id":"4cce5816ef15a48a0bc11e5d400497ee4301dd3b","date":1476991456,"type":3,"author":"Kevin Risden","isMerge":true,"pathNew":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","pathOld":"lucene/core/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler#testNoWaitClose().mjava","sourceNew":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","sourceOld":"  public void testNoWaitClose() throws IOException {\n    Directory directory = newDirectory();\n    if (directory instanceof MockDirectoryWrapper) {\n      ((MockDirectoryWrapper) directory).setPreventDoubleWrite(false);\n    }\n    Document doc = new Document();\n    Field idField = newStringField(\"id\", \"\", Field.Store.YES);\n    doc.add(idField);\n\n    IndexWriter writer = new IndexWriter(\n        directory,\n        newIndexWriterConfig(new MockAnalyzer(random())).\n            // Force excessive merging:\n            setMaxBufferedDocs(2).\n            setMergePolicy(newLogMergePolicy(100)).\n            setCommitOnClose(false)\n    );\n\n    int numIters = TEST_NIGHTLY ? 10 : 3;\n    for(int iter=0;iter<numIters;iter++) {\n\n      for(int j=0;j<201;j++) {\n        idField.setStringValue(Integer.toString(iter*201+j));\n        writer.addDocument(doc);\n      }\n\n      int delID = iter*201;\n      for(int j=0;j<20;j++) {\n        writer.deleteDocuments(new Term(\"id\", Integer.toString(delID)));\n        delID += 5;\n      }\n\n      // Force a bunch of merge threads to kick off so we\n      // stress out aborting them on close:\n      ((LogMergePolicy) writer.getConfig().getMergePolicy()).setMergeFactor(3);\n      writer.addDocument(doc);\n\n      try {\n        writer.commit();\n      } finally {\n        writer.close();\n      }\n\n      IndexReader reader = DirectoryReader.open(directory);\n      assertEquals((1+iter)*182, reader.numDocs());\n      reader.close();\n\n      // Reopen\n      writer = new IndexWriter(\n          directory,\n          newIndexWriterConfig(new MockAnalyzer(random())).\n              setOpenMode(OpenMode.APPEND).\n              setMergePolicy(newLogMergePolicy(100)).\n              // Force excessive merging:\n              setMaxBufferedDocs(2).\n              setCommitOnClose(false)\n      );\n    }\n    writer.close();\n\n    directory.close();\n  }\n\n","bugFix":null,"bugIntro":[],"isBuggy":false,"nexts":[],"revCommit":null}],"commit2Parents":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"32e605e499439692cdaf766a6b4255a4c97bb9dd":["431452b6326a9c17ba5bb1e1a6d89e23a8932e73"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["32e605e499439692cdaf766a6b4255a4c97bb9dd"],"431452b6326a9c17ba5bb1e1a6d89e23a8932e73":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":["32e605e499439692cdaf766a6b4255a4c97bb9dd","11c6df42fb3eba174c3ca0d9a5194eaecd893b77"],"04f07771a2a7dd3a395700665ed839c3dae2def2":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"aba371508186796cc6151d8223a5b4e16d02e26e":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["d19974432be9aed28ee7dca73bdf01d139e763a9"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":[],"d0ef034a4f10871667ae75181537775ddcf8ade4":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":["04f07771a2a7dd3a395700665ed839c3dae2def2","d19974432be9aed28ee7dca73bdf01d139e763a9"],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"fab172655716b96f7e42376116235017a922de3a":["431452b6326a9c17ba5bb1e1a6d89e23a8932e73","32e605e499439692cdaf766a6b4255a4c97bb9dd"],"cd5edd1f2b162a5cfa08efd17851a07373a96817":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77"]},"commit2Childs":{"54a6bea0b991120a99ad0e2f72ae853fd5ecae0e":["d0ef034a4f10871667ae75181537775ddcf8ade4"],"32e605e499439692cdaf766a6b4255a4c97bb9dd":["11c6df42fb3eba174c3ca0d9a5194eaecd893b77","4cce5816ef15a48a0bc11e5d400497ee4301dd3b","fab172655716b96f7e42376116235017a922de3a"],"3a119bbc8703c10faa329ec201c654b3a35a1e3e":["a78a90fc9701e511308346ea29f4f5e548bb39fe"],"11c6df42fb3eba174c3ca0d9a5194eaecd893b77":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"431452b6326a9c17ba5bb1e1a6d89e23a8932e73":["32e605e499439692cdaf766a6b4255a4c97bb9dd","fab172655716b96f7e42376116235017a922de3a"],"4cce5816ef15a48a0bc11e5d400497ee4301dd3b":[],"04f07771a2a7dd3a395700665ed839c3dae2def2":["aba371508186796cc6151d8223a5b4e16d02e26e","d19974432be9aed28ee7dca73bdf01d139e763a9","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a78a90fc9701e511308346ea29f4f5e548bb39fe":["629c38c4ae4e303d0617e05fbfe508140b32f0a3"],"aba371508186796cc6151d8223a5b4e16d02e26e":[],"e7e8d6f15900ee22ac3cb0a503f15dc952a3580f":["04f07771a2a7dd3a395700665ed839c3dae2def2"],"d19974432be9aed28ee7dca73bdf01d139e763a9":["aba371508186796cc6151d8223a5b4e16d02e26e","519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7"],"a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85":["3a119bbc8703c10faa329ec201c654b3a35a1e3e"],"519bac5f6a2dc1779d2fe8e51d9e1762ec94b4a5":["ae14298f4eec6d5faee6a149f88ba57d14a6f21a"],"d0ef034a4f10871667ae75181537775ddcf8ade4":["431452b6326a9c17ba5bb1e1a6d89e23a8932e73"],"ae14298f4eec6d5faee6a149f88ba57d14a6f21a":["54a6bea0b991120a99ad0e2f72ae853fd5ecae0e"],"4b51f65902cc2d20ddeb7a5b949aaddf990f31a7":[],"629c38c4ae4e303d0617e05fbfe508140b32f0a3":["e7e8d6f15900ee22ac3cb0a503f15dc952a3580f"],"fab172655716b96f7e42376116235017a922de3a":[],"cd5edd1f2b162a5cfa08efd17851a07373a96817":[]},"heads":["4cce5816ef15a48a0bc11e5d400497ee4301dd3b","aba371508186796cc6151d8223a5b4e16d02e26e","4b51f65902cc2d20ddeb7a5b949aaddf990f31a7","fab172655716b96f7e42376116235017a922de3a","cd5edd1f2b162a5cfa08efd17851a07373a96817"],"roots":["a0e7ee9d0d12370e8d2b5ae0a23b6e687e018d85"],"pathCommit":null}